{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.util import hash_pandas_object\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Uncomment next line when u first use nltk and press download when all in selected on the windows of nltk downloads\n",
    "#nltk.download()\n",
    "\n",
    "#To detect language for stemming and lemmatization\n",
    "# https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann Schilling\\Anaconda3\\envs\\ada\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,24,25,26,28,44,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/en.openfoodfacts.org.products.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2drop = [\"creator\",\"code\",\"url\",\"created_t\",\"created_datetime\",\"last_modified_t\",\"last_modified_datetime\",\n",
    "               \"brands\", \"brands_tags\",\"origins\", \"origins_tags\", \"manufacturing_places\",\"manufacturing_places_tags\",\n",
    "                \"labels_tags\",\"emb_codes\",\"emb_codes_tags\",\"first_packaging_code_geo\",\"cities\", \"cities_tags\", \n",
    "                \"purchase_places\", \"stores\", \"countries\",\"countries_tags\", \"countries_en\",\"states\",\"states_tags\", \n",
    "                \"states_en\",\"image_url\", \"image_small_url\", \"image_ingredients_url\",\"image_ingredients_small_url\",\n",
    "                \"image_nutrition_url\",\"image_nutrition_small_url\",\"additives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns2drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(\"unknown\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2keep = data.columns[data.count()/len(data.index)*100 > 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[columns2keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We export to a CSV file the completeness of each column along with the most frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>completeness</th>\n",
       "      <th>most_frequent_1</th>\n",
       "      <th>most_frequent_2</th>\n",
       "      <th>most_frequent_3</th>\n",
       "      <th>most_frequent_4</th>\n",
       "      <th>most_frequent_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>product_name</td>\n",
       "      <td>95.019717</td>\n",
       "      <td>Comté</td>\n",
       "      <td>Filet de poulet</td>\n",
       "      <td>Emmental</td>\n",
       "      <td>Miel</td>\n",
       "      <td>Aceite de oliva virgen extra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>generic_name</td>\n",
       "      <td>8.866342</td>\n",
       "      <td>Pâtes alimentaires au blé dur de qualité supér...</td>\n",
       "      <td>Beignets fourrés à la purée de framboise</td>\n",
       "      <td>Pâtes alimentaires de qualité supérieure</td>\n",
       "      <td>Beignets fourrés à la purée de pomme</td>\n",
       "      <td>Bière</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>quantity</td>\n",
       "      <td>31.055727</td>\n",
       "      <td>500 g</td>\n",
       "      <td>250 g</td>\n",
       "      <td>200 g</td>\n",
       "      <td>100 g</td>\n",
       "      <td>400 g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>packaging</td>\n",
       "      <td>17.836446</td>\n",
       "      <td>sachet,plastique</td>\n",
       "      <td>Sachet,Plastique</td>\n",
       "      <td>Plastique</td>\n",
       "      <td>Kunststoff</td>\n",
       "      <td>Bouteille,Verre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>packaging_tags</td>\n",
       "      <td>17.835778</td>\n",
       "      <td>sachet,plastique</td>\n",
       "      <td>plastique</td>\n",
       "      <td>carton</td>\n",
       "      <td>bouteille,verre</td>\n",
       "      <td>sachet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>categories</td>\n",
       "      <td>37.001845</td>\n",
       "      <td>Boissons</td>\n",
       "      <td>Viandes, Volailles, Poulets, Filets de poulet</td>\n",
       "      <td>Boissons, Boissons avec sucre ajouté</td>\n",
       "      <td>Matières grasses</td>\n",
       "      <td>Snacks, Snacks sucrés, Chocolats, Chocolats noirs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>categories_tags</td>\n",
       "      <td>37.001463</td>\n",
       "      <td>en:plant-based-foods-and-beverages,en:plant-ba...</td>\n",
       "      <td>en:beverages,en:non-alcoholic-beverages,en:uns...</td>\n",
       "      <td>en:snacks,en:sweet-snacks,en:biscuits-and-cake...</td>\n",
       "      <td>en:beverages,en:non-alcoholic-beverages</td>\n",
       "      <td>en:snacks,en:sweet-snacks,en:chocolates,en:dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>categories_en</td>\n",
       "      <td>37.001463</td>\n",
       "      <td>Plant-based foods and beverages,Plant-based fo...</td>\n",
       "      <td>Beverages,Non-Alcoholic beverages,Unsweetened ...</td>\n",
       "      <td>Snacks,Sweet snacks,Biscuits and cakes,Biscuits</td>\n",
       "      <td>Beverages,Non-Alcoholic beverages</td>\n",
       "      <td>Snacks,Sweet snacks,Chocolates,Dark chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>labels</td>\n",
       "      <td>19.676954</td>\n",
       "      <td>en:gluten-free</td>\n",
       "      <td>Bio</td>\n",
       "      <td>en:organic</td>\n",
       "      <td>Point Vert</td>\n",
       "      <td>Bio, Bio européen, AB Agriculture Biologique  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>labels_en</td>\n",
       "      <td>19.679245</td>\n",
       "      <td>Organic</td>\n",
       "      <td>Gluten-free</td>\n",
       "      <td>Organic,EU Organic,fr:ab-agriculture-biologique</td>\n",
       "      <td>Green Dot</td>\n",
       "      <td>Vegetarian,Vegan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 completeness  \\\n",
       "product_name        95.019717   \n",
       "generic_name         8.866342   \n",
       "quantity            31.055727   \n",
       "packaging           17.836446   \n",
       "packaging_tags      17.835778   \n",
       "categories          37.001845   \n",
       "categories_tags     37.001463   \n",
       "categories_en       37.001463   \n",
       "labels              19.676954   \n",
       "labels_en           19.679245   \n",
       "\n",
       "                                                   most_frequent_1  \\\n",
       "product_name                                                 Comté   \n",
       "generic_name     Pâtes alimentaires au blé dur de qualité supér...   \n",
       "quantity                                                     500 g   \n",
       "packaging                                         sachet,plastique   \n",
       "packaging_tags                                    sachet,plastique   \n",
       "categories                                                Boissons   \n",
       "categories_tags  en:plant-based-foods-and-beverages,en:plant-ba...   \n",
       "categories_en    Plant-based foods and beverages,Plant-based fo...   \n",
       "labels                                              en:gluten-free   \n",
       "labels_en                                                  Organic   \n",
       "\n",
       "                                                   most_frequent_2  \\\n",
       "product_name                                       Filet de poulet   \n",
       "generic_name              Beignets fourrés à la purée de framboise   \n",
       "quantity                                                     250 g   \n",
       "packaging                                         Sachet,Plastique   \n",
       "packaging_tags                                           plastique   \n",
       "categories           Viandes, Volailles, Poulets, Filets de poulet   \n",
       "categories_tags  en:beverages,en:non-alcoholic-beverages,en:uns...   \n",
       "categories_en    Beverages,Non-Alcoholic beverages,Unsweetened ...   \n",
       "labels                                                       Bio     \n",
       "labels_en                                              Gluten-free   \n",
       "\n",
       "                                                   most_frequent_3  \\\n",
       "product_name                                              Emmental   \n",
       "generic_name              Pâtes alimentaires de qualité supérieure   \n",
       "quantity                                                     200 g   \n",
       "packaging                                                Plastique   \n",
       "packaging_tags                                              carton   \n",
       "categories                    Boissons, Boissons avec sucre ajouté   \n",
       "categories_tags  en:snacks,en:sweet-snacks,en:biscuits-and-cake...   \n",
       "categories_en      Snacks,Sweet snacks,Biscuits and cakes,Biscuits   \n",
       "labels                                                  en:organic   \n",
       "labels_en          Organic,EU Organic,fr:ab-agriculture-biologique   \n",
       "\n",
       "                                         most_frequent_4  \\\n",
       "product_name                                        Miel   \n",
       "generic_name        Beignets fourrés à la purée de pomme   \n",
       "quantity                                           100 g   \n",
       "packaging                                     Kunststoff   \n",
       "packaging_tags                           bouteille,verre   \n",
       "categories                              Matières grasses   \n",
       "categories_tags  en:beverages,en:non-alcoholic-beverages   \n",
       "categories_en          Beverages,Non-Alcoholic beverages   \n",
       "labels                                        Point Vert   \n",
       "labels_en                                      Green Dot   \n",
       "\n",
       "                                                   most_frequent_5  \n",
       "product_name                          Aceite de oliva virgen extra  \n",
       "generic_name                                                 Bière  \n",
       "quantity                                                     400 g  \n",
       "packaging                                          Bouteille,Verre  \n",
       "packaging_tags                                              sachet  \n",
       "categories       Snacks, Snacks sucrés, Chocolats, Chocolats noirs  \n",
       "categories_tags  en:snacks,en:sweet-snacks,en:chocolates,en:dar...  \n",
       "categories_en       Snacks,Sweet snacks,Chocolates,Dark chocolates  \n",
       "labels           Bio, Bio européen, AB Agriculture Biologique  ...  \n",
       "labels_en                                         Vegetarian,Vegan  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nRows = len(data.index)\n",
    "odf = pd.DataFrame(columns = [\"most_frequent_1\",\"most_frequent_2\",\"most_frequent_3\",\"most_frequent_4\",\"most_frequent_5\"])\n",
    "\n",
    "for col in data.columns:\n",
    "    freq_list = data[col].value_counts().head(5).index.tolist()\n",
    "    while (len(freq_list) < 5) :\n",
    "        freq_list.append(\"\")\n",
    "    odf.loc[col] = freq_list\n",
    "\n",
    "odf.insert(0,\"completeness\",(data.count()/len(data.index)*100).values)\n",
    "odf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf.to_csv('odf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the CSV file, no values among the most frequent ones caracterize missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column preprocessing helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to use them to clean the wanted column_to_clean, please look how I clean \"categories\" field in FOOD DIET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = False, langdetec = False, tokenize = False, stemming = False, lemmatizing = False, onlyEngStemmer = False, applyNounFilter = False):\n",
    "    \"\"\"function that appeals all cleaning methods depending of the booleans\"\"\"\n",
    "    cleaned_data = pd.DataFrame()    \n",
    "    cleaned_data[column_to_clean] = data[column_to_clean].copy()\n",
    "    \n",
    "    \n",
    "    if langdetec:\n",
    "        cleaned_data[\"languages\"] = data[data[column_to_clean].notnull()][column_to_clean].apply(language_detection)\n",
    "        #I found all the languages disponibles for stemming, and i map the corresponding name to the ISO_code found by langdetect.detect\n",
    "        cleaned_data[\"languages\"] = cleaned_data[\"languages\"].map({\"ar\": \"arabic\", \"da\": \"danish\", \"nl\" : \"dutch\", \"en\": \"english\", \"fi\": \"finnish\", \"fr\": \"french\", \"de\": \"german\", \\\n",
    "                                  \"hu\": \"hungarian\", \"it\": \"italian\", \"no\": \"norwegian\", \"ro\": \"romanian\", \"ru\" : \"russian\", \"es\": \"spanish\", \\\n",
    "                                  \"sv\" :\"swedish\"})\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_langdetect.pkl\")\n",
    "       \n",
    "    #tokenize column_to_clean : --> stemming + lemmatization need list of tokens\n",
    "    if tokenize:\n",
    "        cleaned_data[column_to_clean] = tokenize_data(cleaned_data[column_to_clean], stop_words, tokenizer, applyNounFilter)\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_token.pkl\")\n",
    "    else: \n",
    "        cleaned_data = pd.read_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_token.pkl\") \n",
    "    \n",
    "    #stemm column_to_clean column according to the language used\n",
    "    if stemming:\n",
    "        #for stemming, langdetection as to be made previously\n",
    "        if not(langdetec):\n",
    "            lang_data = pd.read_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_langdetect.pkl\")\n",
    "            cleaned_data[\"languages\"] = lang_data[\"languages\"].copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        cleaned_data = stemming_data(cleaned_data, column_to_clean, onlyEngStemmer)\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_stem.pkl\")  \n",
    "            \n",
    "    elif lemmatizing: \n",
    "        #lemmatizing only works well for english words\n",
    "        cleaned_data = lemmatizing_data(cleaned_data, column_to_clean, wordnet_lemmatizer)\n",
    "        if saving: \n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_lem\")    \n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nountag(tokenlist_cell):\n",
    "    \"\"\"keep only NN : nouns, singular or mass and NNS : noun,plural and NNP : proper noun\"\"\"\n",
    "    \n",
    "    postag_cell = pos_tag(tokenlist_cell)\n",
    "    postag_cell_filtered = [tag for tag in postag_cell if ((tag[1]=='NNS') or (tag[1] == 'NN') or (tag[1] == \"NNP\"))]\n",
    "    tokenlist_cell = [tag[0] for tag in postag_cell_filtered]\n",
    "    \n",
    "    return tokenlist_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data_column, stop_words, tokenizer, applyNounFilter = False):\n",
    "    \"\"\"Clean data, i.e. \n",
    "        - lower each words in the cells of data_column\n",
    "        - tokenize cells of data_column, i.e. from float type create list of string (token)\n",
    "        - remove stopwords for the list of tokens for each cells of data_column\n",
    "        - keep only tokens with tag = NN (noun), or NNS (noun, plural)\n",
    "    \"\"\"  \n",
    "    data_column = data_column[data_column.notnull()].str.lower() \\\n",
    "        .apply(str) \\\n",
    "        .apply(tokenizer.tokenize) \\\n",
    "        .apply(lambda cell : [item for item in cell if item not in stop_words]) \n",
    "    if applyNounFilter:\n",
    "        data_column = data_column[data_column.notnull()].apply(filter_nountag)\n",
    "    \n",
    "    return data_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detection(category_cell): \n",
    "    \"\"\"take a cell containing a the string from unprocessed dataframe and detect the language\"\"\"\n",
    "    tmp_cell = str()\n",
    "    tmp_cell = category_cell\n",
    "    \n",
    "    #supress numbers in string\n",
    "    tmp_cell = tmp_cell.replace('\\d+', '')\n",
    "    #supress punctuations\n",
    "    tmp_cell = re.sub(r'[^\\w\\s]','', tmp_cell)\n",
    "    #remove spaces in string\n",
    "    tmp_cell = ''.join(tmp_cell.split())\n",
    "    \n",
    "    #check if the string contain only letters --> f**king 🍩 \n",
    "    if tmp_cell.isalpha():\n",
    "        language = detect(category_cell)\n",
    "        return detect(category_cell)\n",
    "    else: \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_data(df_lang, serie_tokenlist, onlyEngStemmer = False):\n",
    "    \"\"\"Take as argument : \n",
    "        - df_lang = dataframe containing a serie of tokenlist, and a serie \"languages\" of corresponding language\n",
    "        - serie_tokenlist = string name of the serie of tokenlist to stem\n",
    "    \"\"\"\n",
    "    for i in [1, 2]:\n",
    "        for index, row in df_lang[df_lang[serie_tokenlist].notnull()].iterrows():\n",
    "            \n",
    "            if onlyEngStemmer: \n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "            else: \n",
    "                stemmer = SnowballStemmer(row[\"languages\"])\n",
    "                df_lang.iloc[index][serie_tokenlist] = [stemmer.stem(token) for token in row[serie_tokenlist]]\n",
    " \n",
    "        \n",
    "    return df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing_data(df, serie_tokenlist, wordnet_lemmatizer):\n",
    "    \"\"\"lemmatize serie containing token_list\n",
    "        - serie_tokenlist = string name of the serie of tokenlist to lemmatize\n",
    "        - df = dataframe containing \"serie_tokenlist\" column\n",
    "        \"\"\"\n",
    "    for index, row in df[df[serie_tokenlist].notnull()].iterrows():\n",
    "        df.iloc[index][serie_tokenlist] =  [wordnet_lemmatizer.lemmatize(token, pos=\"n\") for token in row[serie_tokenlist]]\n",
    "        #if u lemmatize verbs --> pos = \"-v\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOOD DIET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step = cleaning wanted column field by \n",
    "* Tokenization + Stemming / Lemmatazing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning \"categories\" field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> If column field has to be cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below aims to clean the \"categories\" field and save outputs of each steps of the cleaning in pickle format. \n",
    "* create a repository in processed_pickle which has the name of the column to clean, i.e. processed_pickle/categories/ is the repository in which pickles will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving = True\n",
    "# note : Language detection is only needed for stemming, \n",
    "# and it is a very long run (~1h)\n",
    "\n",
    "#StopWords and Tokenizer Object initialization\n",
    "stop_words = set(stopwords.words(\"french\")).union(set(stopwords.words(\"english\"))) #will remove only english and french stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_clean = \"categories\"\n",
    "#language detection\n",
    "langdetect_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, langdetec = True)\n",
    "#tokenization\n",
    "tok_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, tokenize = True, applyNounFilter = True)\n",
    "#stemming --> good since it consider language\n",
    "stem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, stemming = True)\n",
    "#lemmatize --> not very good since it does not consider language (only good for english)\n",
    "lem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, lemmatizing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> If column field already be cleaned and save in pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"categories\"\n",
    "langdetect_data = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_langdetect.pkl\")\n",
    "tok_data = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_token.pkl\") \n",
    "stem_data = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_stem.pkl\") \n",
    "lem_data = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_lem.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliment\n",
      "boisson\n",
      "produit\n",
      "snack\n",
      "fruit\n",
      "lait\n",
      "fromag\n",
      "plat\n",
      "céréal\n",
      "biscuit\n",
      "bas\n",
      "plant\n",
      "food\n",
      "sauc\n",
      "viand\n",
      "pomm\n",
      "légum\n",
      "dessert\n",
      "végétal\n",
      "pât\n",
      "confitur\n",
      "beverag\n",
      "poisson\n",
      "chocolat\n",
      "product\n",
      "yaourt\n",
      "déjeun\n",
      "lebensmittel\n",
      "nectar\n",
      "jus\n",
      "conserv\n",
      "pet\n",
      "cereal\n",
      "charcut\n",
      "getrank\n",
      "mat\n",
      "pflanzlich\n",
      "snacks\n",
      "chip\n",
      "fr\n",
      "pain\n",
      "orig\n",
      "grain\n",
      "huil\n",
      "tartin\n",
      "epic\n",
      "grass\n",
      "glac\n",
      "franc\n",
      "poulet\n",
      "vin\n",
      "confis\n",
      "coqu\n",
      "miel\n",
      "ajout\n",
      "milk\n",
      "crem\n",
      "soup\n",
      "surgel\n",
      "légumin\n",
      "bonbon\n",
      "volaill\n",
      "san\n",
      "bi\n",
      "prépar\n",
      "compot\n",
      "filet\n",
      "marmelad\n",
      "tart\n",
      "foi\n",
      "jambons\n",
      "salad\n",
      "rillet\n",
      "frut\n",
      "sirop\n",
      "oliv\n",
      "sardin\n",
      "thon\n",
      "alimentair\n",
      "cuit\n",
      "jambon\n",
      "roug\n",
      "chocolats\n",
      "sorbet\n",
      "juic\n",
      "élevag\n",
      "steak\n",
      "beb\n",
      "cond\n",
      "veget\n",
      "terrin\n",
      "pickl\n",
      "spread\n",
      "flocon\n",
      "édulcor\n",
      "tomat\n",
      "barr\n",
      "beurr\n",
      "sauces\n",
      "saumon\n",
      "frais\n",
      "meal\n",
      "cream\n",
      "vert\n",
      "sus\n",
      "meat\n",
      "vegetal\n",
      "beverages\n",
      "chocol\n",
      "pâtiss\n",
      "viennois\n",
      "riz\n",
      "milch\n",
      "yogurt\n",
      "orang\n",
      "sech\n",
      "champignon\n",
      "sandwich\n",
      "chaud\n",
      "brioch\n",
      "dulc\n",
      "caf\n",
      "fraîch\n",
      "charcuteri\n",
      "dairi\n",
      "galet\n",
      "noiset\n",
      "milchprodukt\n",
      "quich\n",
      "bread\n",
      "noir\n",
      "tea\n",
      "mélang\n",
      "gazeux\n",
      "onde\n",
      "crêp\n",
      "gâteau\n",
      "chee\n",
      "lech\n",
      "natur\n",
      "viandes\n",
      "vinaigres\n",
      "multifruit\n",
      "pur\n",
      "moutard\n",
      "emmental\n",
      "enti\n",
      "biscuits\n",
      "fleur\n",
      "aliments\n",
      "verdur\n",
      "amand\n",
      "infus\n",
      "groceri\n",
      "ques\n",
      "haricot\n",
      "drink\n",
      "fermentiert\n",
      "croût\n",
      "farc\n",
      "bar\n",
      "pan\n",
      "bot\n",
      "pizz\n",
      "cake\n",
      "chips\n",
      "milks\n",
      "comt\n",
      "sucr\n",
      "kas\n",
      "madelein\n",
      "bean\n",
      "escalop\n",
      "confiseri\n",
      "getreid\n",
      "fat\n",
      "potato\n",
      "bouillon\n",
      "dind\n",
      "jam\n",
      "oil\n",
      "congel\n",
      "brotaufstrich\n",
      "pasta\n",
      "aromat\n",
      "breakfast\n",
      "semoul\n",
      "farin\n",
      "ice\n",
      "charcuteries\n",
      "gras\n",
      "chair\n",
      "candi\n",
      "lentill\n",
      "sel\n",
      "carott\n",
      "reconstitu\n",
      "nouill\n",
      "foods\n",
      "mouss\n",
      "vierg\n",
      "hortaliz\n",
      "past\n",
      "com\n",
      "epicer\n",
      "nut\n",
      "rice\n",
      "seed\n",
      "untabl\n",
      "butter\n",
      "base\n",
      "compl\n",
      "boudin\n",
      "sal\n",
      "concentr\n",
      "prep\n",
      "frit\n",
      "oils\n",
      "epiceri\n",
      "corn\n",
      "thé\n",
      "animal\n",
      "lact\n",
      "yogur\n",
      "virg\n",
      "con\n",
      "yogurts\n",
      "из\n",
      "gratin\n",
      "legum\n",
      "gallet\n",
      "canard\n",
      "frucht\n",
      "dur\n",
      "aid\n",
      "tournesol\n",
      "substitut\n",
      "cook\n",
      "blanc\n",
      "campagn\n",
      "deriv\n",
      "cru\n",
      "и\n",
      "desayun\n",
      "bebidas\n",
      "farines\n",
      "oignon\n",
      "e\n",
      "minéral\n",
      "pastel\n",
      "épic\n",
      "mayon\n",
      "almond\n",
      "cacahuet\n",
      "casc\n",
      "wine\n",
      "carn\n",
      "legumin\n",
      "butt\n",
      "vanill\n",
      "poul\n",
      "doset\n",
      "fri\n",
      "fresc\n",
      "tomato\n",
      "porc\n",
      "condimentair\n",
      "beignet\n",
      "camembert\n",
      "margarin\n",
      "lasagn\n",
      "cuiss\n",
      "condiment\n",
      "blanch\n",
      "raisin\n",
      "raviol\n",
      "sod\n",
      "confectioneri\n",
      "citron\n",
      "réfrig\n",
      "salti\n",
      "mois\n",
      "complet\n",
      "blé\n",
      "céréales\n",
      "quenel\n",
      "culinair\n",
      "honey\n",
      "muesl\n",
      "сыр\n",
      "feuill\n",
      "bloc\n",
      "fish\n",
      "crevet\n",
      "drinks\n",
      "peanut\n",
      "caramel\n",
      "syrup\n",
      "sourc\n",
      "frambois\n",
      "col\n",
      "hach\n",
      "wheat\n",
      "fleisch\n",
      "cidr\n",
      "croiss\n",
      "flake\n",
      "produits\n",
      "turron\n",
      "vach\n",
      "ber\n",
      "taboul\n",
      "blond\n",
      "растительн\n",
      "tapenad\n",
      "cibi\n",
      "appet\n",
      "ketchup\n",
      "pies\n",
      "cheddar\n",
      "coff\n",
      "sels\n",
      "comest\n",
      "mustard\n",
      "dériv\n",
      "water\n",
      "gum\n",
      "italien\n",
      "herb\n",
      "egg\n",
      "pâqu\n",
      "naturel\n",
      "products\n",
      "extra\n",
      "mayonnaises\n",
      "flak\n",
      "salmon\n",
      "truit\n",
      "barbecu\n",
      "chipolat\n",
      "maï\n",
      "appl\n",
      "coco\n",
      "pizzas\n",
      "fisch\n",
      "deshidrat\n",
      "lardon\n",
      "conf\n",
      "balsamiques\n",
      "joghurt\n",
      "di\n",
      "condiments\n",
      "kuch\n",
      "écrem\n",
      "muffin\n",
      "alcool\n",
      "und\n",
      "fruhstuck\n",
      "gaufr\n",
      "par\n",
      "dair\n",
      "mit\n",
      "vinaigr\n",
      "confiseries\n",
      "tortill\n",
      "industriel\n",
      "zum\n",
      "pistach\n",
      "напитк\n",
      "sop\n",
      "nect\n",
      "asperg\n",
      "desserts\n",
      "biscott\n",
      "aguas\n",
      "plats\n",
      "poudr\n",
      "postr\n",
      "assaison\n",
      "gaufret\n",
      "brass\n",
      "épinard\n",
      "pesc\n",
      "œuf\n",
      "goud\n",
      "poir\n",
      "patat\n",
      "cooki\n",
      "boissons\n",
      "boir\n",
      "fum\n",
      "win\n",
      "cib\n",
      "alkohol\n",
      "saint\n",
      "moulag\n",
      "produkt\n",
      "pl\n",
      "fruits\n",
      "poivr\n",
      "pêch\n",
      "semill\n",
      "bacon\n",
      "moutardes\n",
      "noix\n",
      "hareng\n",
      "édulcorants\n",
      "dairies\n",
      "mar\n",
      "seafood\n",
      "chicken\n",
      "beef\n",
      "myrtill\n",
      "tranch\n",
      "vinaigret\n",
      "cornichon\n",
      "brot\n",
      "minerales\n",
      "breb\n",
      "virgen\n",
      "figu\n",
      "pudding\n",
      "cer\n",
      "bovin\n"
     ]
    }
   ],
   "source": [
    "categories_dict = stem_data[stem_data[\"categories\"].notnull()][\"categories\"].explode().value_counts()\n",
    "for word in categories_dict[categories_dict>500].index:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for categories field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes = ['Vegetarian', 'Vegan', 'Glutenfree', 'Lactosefree', 'Ketogenic', 'Organic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['categories_Vegetarian_NOlist',\n",
       " 'categories_Vegan_NOlist',\n",
       " 'categories_Glutenfree_NOlist',\n",
       " 'categories_Lactosefree_NOlist',\n",
       " 'categories_Ketogenic_NOlist',\n",
       " 'categories_Organic_NOlist']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization needed to create 2 series containing authorized items and unauthorized items respectively for each of the diets\n",
    "unauthorized_categories = pd.Series(index = regimes).rename('unauthorized_categories')\n",
    "\n",
    "unauthorized_listnames_categories = ['categories_' + str(x) + '_NOlist' for x in regimes]\n",
    "unauthorized_listnames_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organic = regimes(5)\n",
    "categories_Organic_NOlist = []\n",
    "\n",
    "#Ketogenic = regimes(4)\n",
    "categories_Ketogenic_NOlist = ['sucr', 'pomm', 'confitur', 'confiseries', 'confiseri', 'confis' ,'orang', 'nouill', 'semoul', 'sorbet', 'legumin', 'légumin', 'multifruit', 'potato', 'lait', 'bean', 'haricot', 'lentill', 'lentill', 'riz', 'pasta', \\\n",
    "                               'pasta', 'milk', 'milks', 'sirop', 'candi', 'jus', 'juic', 'tart', 'marmelad', 'snack', 'snacks', 'biscuit', 'pât', 'confitur', 'chocolat', 'chocolats', 'chocol', \\\n",
    "                               'nectar', 'nect', 'bread', 'chip', 'chips', 'pain', 'miel', 'bonbon', 'compot', 'pâtiss']\n",
    "\n",
    "#Lactosefree = regime(3)\n",
    "categories_Lactosefree_NOlist = ['fromag', 'lait', 'yaourt', 'bi', 'écrem', 'cream', 'crem', 'glac', 'milk', 'milks', 'dairi', 'dair', 'dairies', 'beurr', 'milch', 'milchprodukt', 'chee', \\\n",
    "                                'cake', 'crêp', 'bread', 'gâteau' , 'lait', 'butter', 'butt', 'cooki', 'emmental', 'comt', 'camembert', 'margarin', 'kas', 'cheddar', 'yogurt', 'yogur', 'yogurts']\n",
    "\n",
    "#Glutenfree = regime(2)\n",
    "categories_Glutenfree_NOlist = ['pât', 'pasta', 'pâtiss', 'past', 'nouill', 'viennois', 'biscuit', 'biscuits', 'sauc', 'sauces', 'charcut', 'charcuteri', 'charcuteries', 'bread', 'crêp', 'gâteau', 'farin', 'farines', 'pizz', 'pizzas', 'cake', 'madelein', \\\n",
    "                                'brotaufstrich', 'brot', 'beignet', 'lasagn', 'raviol', 'blé', 'wheat', 'bi', 'pies', 'kuch', 'muffin', 'gaufr', 'gaufret', 'biscott', 'cooki', 'cook', 'brot']\n",
    "\n",
    "#Vegan_NOlist = regimes(1)\n",
    "categories_Vegan_NOlist = categories_Lactosefree_NOlist + \\\n",
    "                        ['chee', 'viand', 'viandes', 'meat', 'beef', 'poisson', 'charcut', 'charcuteries', 'charcuteri', 'poulet', 'poul', 'volaill', 'jambons', 'jambon', 'jam', 'rillet', 'sardin', 'thon', 'steak', 'saumon', 'salmon', 'escalop', 'dind', 'jam', \\\n",
    "                         'boudin', 'animal', 'cannard', 'mayon', 'mayonnaises', 'mayonnaises', 'moutardes', 'mustard', 'porc', 'lasagn', 'cuiss', 'fish', 'crevet', 'fleisch', 'vach', 'egg', 'chipolat', 'fisch', 'lardon', 'œuf', 'bacon', 'hareng', 'seafood', 'chicken', \\\n",
    "                         'breb', 'bovin']\n",
    "\n",
    "#Vegetarian_NOlist = regimes(0)\n",
    "categories_Vegetarian_NOlist = ['viand', 'viandes', 'meat', 'beef', 'poisson', 'charcut', 'charcuteries', 'charcuteri', 'poulet', 'poul', 'volaill', 'jambons', 'jambon', 'jam', 'rillet', 'sardin', 'thon', 'steak', 'saumon', 'salmon', 'escalop', 'dind', 'jam', \\\n",
    "                             'boudin', 'animal', 'cannard', 'porc', 'lasagn', 'cuiss', 'fish', 'crevet', 'fleisch', 'vach', 'chipolat', 'fisch', 'lardon', 'bacon', 'hareng', 'seafood', 'chicken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for labels_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving = True\n",
    "# note : Language detection is only needed for stemming, \n",
    "# and it is a very long run (~1h)\n",
    "\n",
    "#StopWords and Tokenizer Object initialization\n",
    "stop_words = set(stopwords.words(\"french\")).union(set(stopwords.words(\"english\"))) #will remove only english and french stopwords\n",
    "tokenizer_withdash = RegexpTokenizer(r'\\w+([-]\\w+)*|\\S+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-d5bf3c0adc73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#tokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtok_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_to_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_withdash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet_lemmatizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#stemming --> good since it consider language\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mstem_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_to_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet_lemmatizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monlyEngStemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-5ad4479fa9d8>\u001b[0m in \u001b[0;36mclean_data\u001b[1;34m(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving, langdetec, tokenize, stemming, lemmatizing, onlyEngStemmer)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#tokenize column_to_clean : --> stemming + lemmatization need list of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mcleaned_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_to_clean\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_to_clean\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msaving\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mcleaned_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"processed_pickle/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_to_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/out_token.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-42a56d7bc32d>\u001b[0m in \u001b[0;36mtokenize_data\u001b[1;34m(data_column, stop_words, tokenizer)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_nountag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-e0f63e4f8214>\u001b[0m in \u001b[0;36mfilter_nountag\u001b[1;34m(tokenlist_cell)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"keep only NN : nouns, singular or mass and NNS : noun,plural and NNP : proper noun\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpostag_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenlist_cell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpostag_cell_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpostag_cell\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'NNS'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'NN'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NNP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtokenlist_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpostag_cell_filtered\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Maps to the specified tagset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_tagdict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_tagdict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!YEAR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!DIGITS'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "column_to_clean = \"labels_en\"\n",
    "\n",
    "#tokenization\n",
    "tok_data = clean_data(data, column_to_clean, stop_words, tokenizer_withdash, wordnet_lemmatizer, saving=True, tokenize=True)\n",
    "#stemming --> good since it consider language\n",
    "stem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving=True, stemming=True, onlyEngStemmer=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOOD CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first step, we aim to categorize products according to their PNNS Category.\n",
    "\n",
    "https://www.cerin.org/rapports/groupes-groupes-daliments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = initial_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 category from PNNS norm\n",
    "pnns_category = [\"Dairies\",\"Composite\",\"Fish Meat Eggs\",\"Beverages\",\"Fat Sauces\",\"Fruits Vegetables\",\"Starchy\",\"Snacks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pnns_groups_1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sugary snacks              58626\n",
       "Milk and dairy products    43945\n",
       "Fish Meat Eggs             43040\n",
       "Cereals and potatoes       34562\n",
       "Beverages                  29319\n",
       "Fat and sauces             28375\n",
       "Composite foods            25712\n",
       "Fruits and vegetables      24710\n",
       "Salty snacks               19978\n",
       "sugary-snacks               3498\n",
       "fruits-and-vegetables       2924\n",
       "cereals-and-potatoes          44\n",
       "salty-snacks                   5\n",
       "Name: pnns_groups_1, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data[\"pnns_groups_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary to normalize category names into PNNS category\n",
    "pnns1_category = {\n",
    " 'Sugary snacks' : 'Snacks', \n",
    " 'Milk and dairy products' : 'Dairies',\n",
    " 'Composite foods' : 'Composite', \n",
    " 'Cereals and potatoes' : 'Starchy', \n",
    " 'Fish Meat Eggs' : 'Fish Meat Eggs',\n",
    " 'Beverages' : 'Beverages',\n",
    " 'Fat and sauces' : 'Fat Sauces',\n",
    " 'Fruits and vegetables' : 'Fruits Vegetables',\n",
    " 'Salty snacks' : 'Snacks',\n",
    " 'fruits-and-vegetables' : 'Fruits Vegetables',\n",
    " 'sugary-snacks' : 'Snacks',\n",
    " 'cereals-and-potatoes' : 'Starchy',\n",
    " 'salty-snacks' : 'Snacks'\n",
    "}\n",
    "data.loc[:,'pnns_groups_1'] = data['pnns_groups_1'].map(pnns1_category)\n",
    "data = data.rename(columns={\"pnns_groups_1\": \"pnns_category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Snacks               82107\n",
       "Dairies              43945\n",
       "Fish Meat Eggs       43040\n",
       "Starchy              34606\n",
       "Beverages            29319\n",
       "Fat Sauces           28375\n",
       "Fruits Vegetables    27634\n",
       "Composite            25712\n",
       "Name: pnns_category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New pnns categories\n",
    "data[\"pnns_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pnns_groups_2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alcoholic beverages       10460\n",
       "Pizza pies and quiches      402\n",
       "Name: pnns_groups_2, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values of pnns_groups_2 which haven't been classified in pnns_category\n",
    "data.loc[data.pnns_category.isna()][\"pnns_groups_2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary to extend pnns_categories from pnns_groups_2\n",
    "pnns2_category = {\n",
    " 'Pizza pies and quiches' : 'Composite', \n",
    " 'Alcoholic beverages' : 'Beverages',\n",
    "}\n",
    "data.loc[data.pnns_category.isna(),'pnns_category'] = data.loc[data.pnns_category.isna()]['pnns_groups_2'].map(pnns2_category)\n",
    "data = data.drop('pnns_groups_2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Snacks               82107\n",
       "Dairies              43945\n",
       "Fish Meat Eggs       43040\n",
       "Beverages            39779\n",
       "Starchy              34606\n",
       "Fat Sauces           28375\n",
       "Fruits Vegetables    27634\n",
       "Composite            26114\n",
       "Name: pnns_category, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"pnns_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From main_category_en column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beverages                          9485\n",
       "Groceries                          6052\n",
       "Plant-based foods and beverages    5362\n",
       "Snacks                             3535\n",
       "Dietary supplements                2032\n",
       "Dairies                            1578\n",
       "Desserts                           1300\n",
       "Sweeteners                         1248\n",
       "Baby foods                         1242\n",
       "Cooking helpers                     743\n",
       "Crêpes and galettes                 689\n",
       "Food additives                      595\n",
       "Non food products                   521\n",
       "Canned foods                        480\n",
       "Frozen foods                        452\n",
       "Spreads                             398\n",
       "Cocoa and chocolate powders         332\n",
       "Licensed products                   306\n",
       "Syrups                              288\n",
       "Chips and fries                     267\n",
       "es:bolleria-industrial              237\n",
       "fr:bloc-de-foie-gras-de-canard      229\n",
       "Pizza dough                         204\n",
       "Breakfasts                          198\n",
       "Banana-crisps                       197\n",
       "Fish eggs                           172\n",
       "Terrines                            168\n",
       "fr:escalopes                        165\n",
       "Salads                              164\n",
       "fr:pilons-de-poulet                 136\n",
       "Name: main_category_en, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.pnns_category.isna()][\"main_category_en\"].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After inspecting the food type for the 30 most frequent values in main_category_en, we create the following dictionary\n",
    "maincategoryen_category = {\n",
    "'Beverages' : 'Beverages',\n",
    "'Snacks' : 'Snacks',\n",
    "'Dairies' : 'Dairies',\n",
    "'Desserts' : 'Dairies',\n",
    "'Crêpes and galettes' : 'Snacks',\n",
    "'Cocoa and chocolate powders': 'Snacks',\n",
    "'Syrups':'Fat Sauces',\n",
    "'Chips and fries': 'Snacks',\n",
    "'es:bolleria-industrial': 'Snacks',\n",
    "'fr:bloc-de-foie-gras-de-canard': 'Fish Meat Eggs' ,\n",
    "'Pizza dough' : 'Starchy',\n",
    "'Breakfast' : 'Starchy',\n",
    "'Banana-crisps' : \"Snacks\",\n",
    "'Fish eggs' : 'Fish Meat Eggs',\n",
    "'Terrines' : 'Fish Meat Eggs',\n",
    "'fr:escalopes' : 'Fish Meat Eggs',\n",
    "'Salads' : 'Fruits Vegetables',\n",
    "'fr:pilons-de-poulet' : 'Fish Meat Eggs'\n",
    "}\n",
    "\n",
    "data.loc[data.pnns_category.isna(),'pnns_category'] =  data.loc[data.pnns_category.isna(),'main_category_en'].map(maincategoryen_category)\n",
    "data = data.drop('main_category_en',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3290845377633065"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"pnns_category\"].value_counts())/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After these steps, we managed to categorize 33% of our date among PNNS groups.**\n",
    "\n",
    "For the unclassified data, the other columns don't bring any strong evidence for the categorization of the product. For instance, a lot of Dietary Supplements or Non Food Products are unclassified. Also, the table beneath shows that below 6% of the unclassified data have values in columns representing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_name                               0.931412\n",
       "energy_100g                                0.767232\n",
       "proteins_100g                              0.764975\n",
       "carbohydrates_100g                         0.763191\n",
       "fat_100g                                   0.762912\n",
       "sugars_100g                                0.749857\n",
       "salt_100g                                  0.745506\n",
       "sodium_100g                                0.745460\n",
       "energy-kcal_100g                           0.741572\n",
       "saturated-fat_100g                         0.730360\n",
       "ingredients_text                           0.333944\n",
       "additives_n                                0.333791\n",
       "ingredients_from_palm_oil_n                0.333791\n",
       "ingredients_that_may_be_from_palm_oil_n    0.333791\n",
       "fiber_100g                                 0.253628\n",
       "serving_quantity                           0.249664\n",
       "serving_size                               0.249524\n",
       "nova_group                                 0.193878\n",
       "additives_en                               0.186772\n",
       "additives_tags                             0.186772\n",
       "quantity                                   0.175353\n",
       "cholesterol_100g                           0.172855\n",
       "trans-fat_100g                             0.172052\n",
       "iron_100g                                  0.169706\n",
       "vitamin-c_100g                             0.169349\n",
       "calcium_100g                               0.168746\n",
       "vitamin-a_100g                             0.166091\n",
       "labels                                     0.119601\n",
       "labels_en                                  0.119595\n",
       "categories                                 0.061012\n",
       "categories_tags                            0.061006\n",
       "categories_en                              0.061006\n",
       "main_category                              0.060927\n",
       "packaging                                  0.047274\n",
       "packaging_tags                             0.047265\n",
       "nutrition-score-uk_100g                    0.036315\n",
       "nutrition-score-fr_100g                    0.036315\n",
       "nutrition_grade_fr                         0.036315\n",
       "traces_tags                                0.030537\n",
       "traces_en                                  0.030537\n",
       "energy-kj_100g                             0.025540\n",
       "allergens                                  0.018101\n",
       "generic_name                               0.015380\n",
       "traces                                     0.015047\n",
       "pnns_category                              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.pnns_category.isnull()].count().sort_values(ascending=False)/len(data.loc[data.pnns_category.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_data = data.loc[~data.pnns_category.isnull()]\n",
    "unclassified_data = data.loc[data.pnns_category.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6275750842649371"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_data.loc[~classified_data.ingredients_text.isnull(),\"ingredients_text\"].count()/len(classified_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33% of the remaining data has a list of ingredients, and 62% of our classified data also has a list of ingredients. We will use Jaccard similarities on the words in ingredient_text to associate unclassified products to categories if the similarity is big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ingredients_text column (IN THE WORKINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_data.loc[:,\"ingredients_text\"] = classified_data.loc[:,\"ingredients_text\"].str.lower() \\\n",
    "                                            .replace('%','') \\\n",
    "                                            .replace('-','') \\\n",
    "                                            .replace(':','') \\\n",
    "                                            .replace(',','') \\\n",
    "                                            .replace(\"  \",'') \\\n",
    "                                            .replace(\"(\",'') \\\n",
    "                                            .replace(\")\",'') \\\n",
    "                                            .replace(\";\",'')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "stopWords = stopwords.words('french')\n",
    "\n",
    "classified_data.loc[:,\"ingredients_text\"] = classified_data.loc[:,\"ingredients_text\"].str.lower()\n",
    "\n",
    "def createWordSet(df):\n",
    "    \n",
    "    count = Counter([ word for sentence in df.loc[~df[\"ingredients_text\"].isnull(),\"ingredients_text\"].tolist() for word in sentence.split(\" \")])\n",
    "    return [word[0] for word in count.most_common(100)]\n",
    "\n",
    "res = classified_data.groupby(\"pnns_category\").apply(createWordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"Fat Sauces\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PACKAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebauche of dictionnary to class type of packaging Plastique/Recylable(nonplastik)/nonrecyclable\n",
    "pack_mapping = dict.fromkeys(['plastic', 'plastique,frais','plastique', 'barquette,plastique', 'sachet,plastique',\n",
    "                              'frais,plastique','carton,plastique','carton,plastique,surgele','flacon,plastique'\n",
    "                 'sachet-plastique','carton,boite,plastique', 'boite,plastique', 'plastique,carton','sachet-plastique',\n",
    "                'sachet,plastique,surgele','barquette,plastique,frais','pot,plastique','plastique,sachet',\n",
    "                             'plastico','boite,carton,plastique','frais,plastique,carton','frais,carton,plastique',\n",
    "                              'carton,sachet,plastique','kunststoff','barquette,film,plastique,sous-atmosphere-protectrice',\n",
    "                             'carton,surgele,plastique','plastique,barquette','sachet,plastique,frais',\n",
    "                             'barquette,plastique,sous-atmosphere-protectrice','frais,barquette,plastique,opercule,film-plastique,sous-atmosphere-protectrice',\n",
    "                             'sachet,plastique,carton','film,plastique','sachet,plastique,sous-atmosphere-protectrice',\n",
    "                             'bolsa-de-plastico,ultracongelado','barquette-plastique','plastique,sous-vide','plastic,bag',\n",
    "                             'barquette,film,plastique','plastik','plastica','pot-plastique','plastic-bag','boite-plastique',\n",
    "                             'frais,barquette,plastique','05-pp','pp','sachet,plastique,etui,carton','bolsa-de-plastico',\n",
    "                             'boite,carton,sachet,plastique','pot,plastique,frais','botella,plastico','sachet,plastique,doypack',\n",
    "                             'sachet,plastique,sachet','plastique,boite','plastique,surgele','sac-plastique'],'Plastic')\n",
    "\n",
    "pack_mapping.update(dict.fromkeys(['bouteille,verre','carton','boite,carton','carton,surgele','karton','carton,boite','karton,kunststoff',\n",
    "                                  'sachet,papier','papier','bouteille,plastique','conserve','bocal,verre','glas',\n",
    "                                  'bouteille','conserve,metal','bocal,verre,couvercle,metal','bocal,verre,metal',\n",
    "                                  'verre','verre,bouteille','pot,verre','bocal','verre,bocal',\n",
    "                                  'flacon,plastique','bouteille-plastique','bottle','canned','pot-en-verre','bolsa,plastico',\n",
    "                                   'bouteille-verre','bocal-en-verre','verre,bocal,metal','plastique,bouteille','bouteille-en-verre','canette','pot,verre,couvercle,metal',\n",
    "                                  'glas,mehrwegpfand','carton,aluminium','boite,metal','botella-de-plastico','boite-carton',\n",
    "                                  'plastic-bottle','sous-vide','plastic,bottle','can','pot-en-plastique','glass','caja,carton','bote-de-vidrio',\n",
    "                                  'boite,conserve,metal','pet','becher','lata','bocal-verre','aluminium','caja-de-carton','bouteille,verre,capsule,metal',\n",
    "                                  'bokaal,glas','konserve','paper','glass-bottle','bocal,verre,couvercle,metal,conserve','glass-jar',\n",
    "                                  'glasflasche','flacon-verre','glas,flasche','kunststoff,karton','papier,aluminium','boite-de-conserve',\n",
    "                                  'pot-verre','canette,metal','carton,brique','flacon,verre','caja,carton,lata,en-conserva','bocal-verre,couvercle-metal','glass,bottle',\n",
    "                                  'conserve,boite,metal','carton,sachet','canette,aluminium','brique,carton','boite-en-carton','conserve,conserve'],'Recyclable'))\n",
    "\n",
    "pack_mapping.update(dict.fromkeys(['frais','surgele','surgele,carton,plastique','tetra-pak','tetra-brik','tetrapak',\n",
    "                                  'brique','tetrapack','surgele,carton','barquette,film,plastique,frais,sous-atmosphere-protectrice','sous-atmosphere-protectrice',\n",
    "                                  'carton,plastique,frais','frais,pot,plastique'],'Non-Recyclable'))\n",
    "\n",
    "\n",
    "data['emballage'] = data['packaging_tags'].replace(pack_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTRISCORE\n",
    "https://quoidansmonassiette.fr/comment-est-calcule-le-nutri-score-logo-nutritionnel/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import just enough field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/envs/ada/lib/python3.7/site-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "# replace NàN of 'fruits-vegetables-nuts_100g' by 100 if it is 'Fruit juices'\n",
    "data[(data['main_category_en'] == 'Fruit juices')]['fruits-vegetables-nuts_100g'].fillna(100, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_enough_field =[\n",
    "    'Category_Food',\n",
    "    'energy_100g','sugars_100g','saturated-fat_100g','sodium_100g',\n",
    "    'fruits-vegetables-nuts_100g','fiber_100g','proteins_100g',\n",
    "    'nutrition-score-uk_100g','nutrition_grade_fr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataframe into 3 groups to simplify the calculation of nutriscrore\n",
    "data_beverages = data[data['Category_Food']=='Beverages'][selected_enough_field]\n",
    "data_fatsauces = data[data['Category_Food']=='Fat Sauces'][selected_enough_field]\n",
    "data_without_beverage_fat = data[(data['Category_Food']!='Fat Sauces') & (data['Category_Food']!='Beverages') ][selected_enough_field]\n",
    "                                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of negative points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENERGY\n",
    "# energy bins\n",
    "energy_bins_without_beverage_fat = [data_without_beverage_fat['energy_100g'].min() - 1, 335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350, data_without_beverage_fat['energy_100g'].max()]\n",
    "energy_bins_fatsauces = [data_fatsauces['energy_100g'].min() - 1, 335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350, data_fatsauces['energy_100g'].max()]\n",
    "energy_bins_beverages = [data_beverages['energy_100g'].min() - 1, 0, 30, 60, 90, 120, 150, 180, 210, 240, 270, data_beverages['energy_100g'].max()]\n",
    "# energy point\n",
    "data_without_beverage_fat['energy_points'] = pd.cut(data_without_beverage_fat['energy_100g'], energy_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_fatsauces['energy_points'] = pd.cut(data_fatsauces['energy_100g'], energy_bins_fatsauces, labels=range(11)).astype(float)\n",
    "data_beverages['energy_points'] = pd.cut(data_beverages['energy_100g'], energy_bins_beverages, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SUGAR\n",
    "# sugar bins\n",
    "sugar_bins_without_beverage_fat = [data_without_beverage_fat['sugars_100g'].min() - 1, 0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5, 12, 13.5, data_without_beverage_fat['sugars_100g'].max()]\n",
    "sugar_bins_fatsauces = [data_fatsauces['sugars_100g'].min() - 1, 0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5, 12, 13.5, data_fatsauces['sugars_100g'].max()]\n",
    "sugar_bins_beverages = [data_beverages['sugars_100g'].min() - 1, 4.5, 9, 13.5, 18, 22.5, 27, 31, 36, 40, 45, data_beverages['sugars_100g'].max()]\n",
    "# sugar point (CHANGE SUGAR_BINS)\n",
    "data_without_beverage_fat['sugar_points'] = pd.cut(data_without_beverage_fat['sugars_100g'], sugar_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_fatsauces['sugar_points'] = pd.cut(data_fatsauces['sugars_100g'], sugar_bins_fatsauces, labels=range(11)).astype(float)\n",
    "data_beverages['sugar_points'] = pd.cut(data_beverages['sugars_100g'], sugar_bins_beverages, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SATURATED FAT\n",
    "# s-fat bins\n",
    "fat_bins_without_beverage_fat = [data_without_beverage_fat['saturated-fat_100g'].min() - 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, data_without_beverage_fat['saturated-fat_100g'].max()]\n",
    "fat_bins_beverages = [data_beverages['saturated-fat_100g'].min() - 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, data_beverages['saturated-fat_100g'].max()]\n",
    "fat_bins_fatsauces = [data_fatsauces['saturated-fat_100g'].min() - 1, 10, 16, 22, 28, 34, 40, 46, 52, 58, 64, data_fatsauces['saturated-fat_100g'].max()]\n",
    "# s-FAT point (CHANGE FAT_BINS)\n",
    "data_without_beverage_fat['saturated-fat_points'] = pd.cut(data_without_beverage_fat['saturated-fat_100g'], fat_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_beverages['saturated-fat_points'] = pd.cut(data_beverages['saturated-fat_100g'], fat_bins_beverages, labels=range(11)).astype(float)\n",
    "data_fatsauces['saturated-fat_points'] = pd.cut(data_fatsauces['saturated-fat_100g'], fat_bins_fatsauces, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SODIUM\n",
    "# sodium bins\n",
    "sodium_bins = [data['sodium_100g'].min() - 1, 90, 180, 270, 360, 450, 540, 630, 720, 810, 900, data['sodium_100g'].max()]\n",
    "# sodium points\n",
    "data_without_beverage_fat['sodium_points'] = pd.cut(data_without_beverage_fat['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "data_beverages['sodium_points'] = pd.cut(data_beverages['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "data_fatsauces['sodium_points'] = pd.cut(data_fatsauces['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation positive points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRUITS\n",
    "# fruits bins\n",
    "fruits_bins_without_beverage_fat = [data_without_beverage_fat['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_without_beverage_fat['fruits-vegetables-nuts_100g'].max()]\n",
    "fruits_bins_fatsauces = [data_fatsauces['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_fatsauces['fruits-vegetables-nuts_100g'].max()]\n",
    "fruits_bins_beverages = [data_beverages['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_beverages['fruits-vegetables-nuts_100g'].max()]\n",
    "# fruits points\n",
    "data_without_beverage_fat['fruits_points'] = pd.cut(data_without_beverage_fat['fruits-vegetables-nuts_100g'], fruits_bins_without_beverage_fat, labels=[0,1,2,5]).astype(float)\n",
    "data_beverages['fruits_points'] = pd.cut(data_beverages['fruits-vegetables-nuts_100g'], fruits_bins_beverages, labels=[0,2,4,10]).astype(float)\n",
    "data_fatsauces['fruits_points'] = pd.cut(data_fatsauces['fruits-vegetables-nuts_100g'], fruits_bins_fatsauces, labels=[0,1,2,5]).astype(float)\n",
    "\n",
    "\n",
    "# FIBRES\n",
    "# fibers bins\n",
    "fibers_bins = [data['fiber_100g'].min() - 1, 0.7, 1.4, 2.1, 2.8, 3.5, data['fiber_100g'].max()]\n",
    "# fibers points\n",
    "data_without_beverage_fat['fiber_points'] = pd.cut(data_without_beverage_fat['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "data_beverages['fiber_points'] = pd.cut(data_beverages['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "data_fatsauces['fiber_points'] = pd.cut(data_fatsauces['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "\n",
    "# PROTEINS\n",
    "# proteins bins\n",
    "proteins_bins = [data['proteins_100g'].min() - 1, 1.6, 3.2, 4.8, 6.4, 8.0, data['proteins_100g'].max()]\n",
    "# proteins points\n",
    "data_without_beverage_fat['proteins_points'] = pd.cut(data_without_beverage_fat['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n",
    "data_beverages['proteins_points'] = pd.cut(data_beverages['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n",
    "data_fatsauces['proteins_points'] = pd.cut(data_fatsauces['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin last separated dataframe\n",
    "frames = [data_without_beverage_fat, data_beverages, data_fatsauces]\n",
    "nutridata = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concession made to put 0 inplace of Nan of 'fruits_points','fiber_points','proteins_points' \n",
    "# this will not be a problem we do not substract Positive point\n",
    "nutridata['fruits_points'].fillna(0, inplace=True)\n",
    "nutridata['fiber_points'].fillna(0, inplace=True)\n",
    "nutridata['proteins_points'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of P x N\n",
    "nutridata['points_N']= nutridata['energy_points'] + nutridata['saturated-fat_points'] + nutridata['sugar_points'] + nutridata['sodium_points']\n",
    "nutridata['points_P'] = nutridata['fruits_points'] + nutridata['fiber_points'] + nutridata['proteins_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(row):\n",
    "    N = row['points_N']\n",
    "    P = row['points_P']\n",
    "    fruit = row['fruits_points']\n",
    "    fiber = row['fiber_points']\n",
    "    if N < 11 or fruit == 5:\n",
    "        return N - P\n",
    "    else:\n",
    "        return N - (fiber + fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutridata['nutri-score_calculated'] = nutridata.apply(compute_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#petite visualization\n",
    "nutridata['nutri-score_calculated'].hist(bins=int(nutridata['nutri-score_calculated'].max() - nutridata['nutri-score_calculated'].min() + 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re divided dataframe to allow a simpler transformation of score into letter\n",
    "nutridata_beverages = nutridata[nutridata['Category_Food']=='Beverages']\n",
    "nutridata_not_beverages = nutridata[nutridata['Category_Food']!='Beverages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/envs/ada/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/Matt/anaconda3/envs/ada/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# NUTRILETTER\n",
    "\n",
    "#letter bins\n",
    "letter_bins_aliments = [nutridata['nutri-score_calculated'].min() - 1, 0, 2.9, 10.9, 18.9, nutridata['nutri-score_calculated'].max()]\n",
    "letter_bins_beverages = [nutridata['nutri-score_calculated'].min() - 1, 2.5, 5.5, 9.5, nutridata['nutri-score_calculated'].max()]\n",
    "\n",
    "#letter\n",
    "nutridata_not_beverages['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_not_beverages['nutri-score_calculated'], letter_bins_aliments, labels=['a','b','c','d','e'])\n",
    "nutridata_beverages['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_beverages['nutri-score_calculated'], letter_bins_beverages, labels=['b','c','d','e'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutriframes = [nutridata_not_beverages, nutridata_beverages]\n",
    "nutridata = pd.concat(nutriframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d    247561\n",
       "a    160302\n",
       "c    156517\n",
       "e    118151\n",
       "b     87801\n",
       "Name: nutri-score_letter_CALCULATED, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutridata['nutri-score_letter_CALCULATED'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
