{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.util import hash_pandas_object\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Uncomment next line when u first use nltk and press download when all in selected on the windows of nltk downloads\n",
    "#nltk.download()\n",
    "\n",
    "#To detect language for stemming and lemmatization\n",
    "# https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "from langdetect import detect\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann Schilling\\Anaconda3\\envs\\ada\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,24,25,26,28,44,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/en.openfoodfacts.org.products.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns2drop = [\"creator\",\"code\",\"url\",\"created_t\",\"created_datetime\",\"last_modified_t\",\"last_modified_datetime\",\n",
    "               \"brands\", \"brands_tags\",\"origins\", \"origins_tags\", \"manufacturing_places\",\"manufacturing_places_tags\",\n",
    "                \"labels_tags\",\"emb_codes\",\"emb_codes_tags\",\"first_packaging_code_geo\",\"cities\", \"cities_tags\", \n",
    "                \"purchase_places\", \"stores\", \"countries\",\"countries_tags\", \"countries_en\",\"states\",\"states_tags\", \n",
    "                \"states_en\",\"image_url\", \"image_small_url\", \"image_ingredients_url\",\"image_ingredients_small_url\",\n",
    "                \"image_nutrition_url\",\"image_nutrition_small_url\",\"additives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns2drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.replace(\"unknown\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns2keep = data.columns[data.count()/len(data.index)*100 > 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[columns2keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We export to a CSV file the completeness of each column along with the most frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>completeness</th>\n",
       "      <th>most_frequent_1</th>\n",
       "      <th>most_frequent_2</th>\n",
       "      <th>most_frequent_3</th>\n",
       "      <th>most_frequent_4</th>\n",
       "      <th>most_frequent_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>product_name</td>\n",
       "      <td>95.019717</td>\n",
       "      <td>Comt√©</td>\n",
       "      <td>Filet de poulet</td>\n",
       "      <td>Emmental</td>\n",
       "      <td>Miel</td>\n",
       "      <td>Aceite de oliva virgen extra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>generic_name</td>\n",
       "      <td>8.866342</td>\n",
       "      <td>P√¢tes alimentaires au bl√© dur de qualit√© sup√©r...</td>\n",
       "      <td>Beignets fourr√©s √† la pur√©e de framboise</td>\n",
       "      <td>P√¢tes alimentaires de qualit√© sup√©rieure</td>\n",
       "      <td>Beignets fourr√©s √† la pur√©e de pomme</td>\n",
       "      <td>Bi√®re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>quantity</td>\n",
       "      <td>31.055727</td>\n",
       "      <td>500 g</td>\n",
       "      <td>250 g</td>\n",
       "      <td>200 g</td>\n",
       "      <td>100 g</td>\n",
       "      <td>400 g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>packaging</td>\n",
       "      <td>17.836446</td>\n",
       "      <td>sachet,plastique</td>\n",
       "      <td>Sachet,Plastique</td>\n",
       "      <td>Plastique</td>\n",
       "      <td>Kunststoff</td>\n",
       "      <td>Bouteille,Verre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>packaging_tags</td>\n",
       "      <td>17.835778</td>\n",
       "      <td>sachet,plastique</td>\n",
       "      <td>plastique</td>\n",
       "      <td>carton</td>\n",
       "      <td>bouteille,verre</td>\n",
       "      <td>sachet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>categories</td>\n",
       "      <td>37.001845</td>\n",
       "      <td>Boissons</td>\n",
       "      <td>Viandes, Volailles, Poulets, Filets de poulet</td>\n",
       "      <td>Boissons, Boissons avec sucre ajout√©</td>\n",
       "      <td>Mati√®res grasses</td>\n",
       "      <td>Snacks, Snacks sucr√©s, Chocolats, Chocolats noirs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>categories_tags</td>\n",
       "      <td>37.001463</td>\n",
       "      <td>en:plant-based-foods-and-beverages,en:plant-ba...</td>\n",
       "      <td>en:beverages,en:non-alcoholic-beverages,en:uns...</td>\n",
       "      <td>en:snacks,en:sweet-snacks,en:biscuits-and-cake...</td>\n",
       "      <td>en:beverages,en:non-alcoholic-beverages</td>\n",
       "      <td>en:snacks,en:sweet-snacks,en:chocolates,en:dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>categories_en</td>\n",
       "      <td>37.001463</td>\n",
       "      <td>Plant-based foods and beverages,Plant-based fo...</td>\n",
       "      <td>Beverages,Non-Alcoholic beverages,Unsweetened ...</td>\n",
       "      <td>Snacks,Sweet snacks,Biscuits and cakes,Biscuits</td>\n",
       "      <td>Beverages,Non-Alcoholic beverages</td>\n",
       "      <td>Snacks,Sweet snacks,Chocolates,Dark chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>labels</td>\n",
       "      <td>19.676954</td>\n",
       "      <td>en:gluten-free</td>\n",
       "      <td>Bio</td>\n",
       "      <td>en:organic</td>\n",
       "      <td>Point Vert</td>\n",
       "      <td>Bio, Bio europ√©en, AB Agriculture Biologique  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>labels_en</td>\n",
       "      <td>19.679245</td>\n",
       "      <td>Organic</td>\n",
       "      <td>Gluten-free</td>\n",
       "      <td>Organic,EU Organic,fr:ab-agriculture-biologique</td>\n",
       "      <td>Green Dot</td>\n",
       "      <td>Vegetarian,Vegan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 completeness  \\\n",
       "product_name        95.019717   \n",
       "generic_name         8.866342   \n",
       "quantity            31.055727   \n",
       "packaging           17.836446   \n",
       "packaging_tags      17.835778   \n",
       "categories          37.001845   \n",
       "categories_tags     37.001463   \n",
       "categories_en       37.001463   \n",
       "labels              19.676954   \n",
       "labels_en           19.679245   \n",
       "\n",
       "                                                   most_frequent_1  \\\n",
       "product_name                                                 Comt√©   \n",
       "generic_name     P√¢tes alimentaires au bl√© dur de qualit√© sup√©r...   \n",
       "quantity                                                     500 g   \n",
       "packaging                                         sachet,plastique   \n",
       "packaging_tags                                    sachet,plastique   \n",
       "categories                                                Boissons   \n",
       "categories_tags  en:plant-based-foods-and-beverages,en:plant-ba...   \n",
       "categories_en    Plant-based foods and beverages,Plant-based fo...   \n",
       "labels                                              en:gluten-free   \n",
       "labels_en                                                  Organic   \n",
       "\n",
       "                                                   most_frequent_2  \\\n",
       "product_name                                       Filet de poulet   \n",
       "generic_name              Beignets fourr√©s √† la pur√©e de framboise   \n",
       "quantity                                                     250 g   \n",
       "packaging                                         Sachet,Plastique   \n",
       "packaging_tags                                           plastique   \n",
       "categories           Viandes, Volailles, Poulets, Filets de poulet   \n",
       "categories_tags  en:beverages,en:non-alcoholic-beverages,en:uns...   \n",
       "categories_en    Beverages,Non-Alcoholic beverages,Unsweetened ...   \n",
       "labels                                                       Bio     \n",
       "labels_en                                              Gluten-free   \n",
       "\n",
       "                                                   most_frequent_3  \\\n",
       "product_name                                              Emmental   \n",
       "generic_name              P√¢tes alimentaires de qualit√© sup√©rieure   \n",
       "quantity                                                     200 g   \n",
       "packaging                                                Plastique   \n",
       "packaging_tags                                              carton   \n",
       "categories                    Boissons, Boissons avec sucre ajout√©   \n",
       "categories_tags  en:snacks,en:sweet-snacks,en:biscuits-and-cake...   \n",
       "categories_en      Snacks,Sweet snacks,Biscuits and cakes,Biscuits   \n",
       "labels                                                  en:organic   \n",
       "labels_en          Organic,EU Organic,fr:ab-agriculture-biologique   \n",
       "\n",
       "                                         most_frequent_4  \\\n",
       "product_name                                        Miel   \n",
       "generic_name        Beignets fourr√©s √† la pur√©e de pomme   \n",
       "quantity                                           100 g   \n",
       "packaging                                     Kunststoff   \n",
       "packaging_tags                           bouteille,verre   \n",
       "categories                              Mati√®res grasses   \n",
       "categories_tags  en:beverages,en:non-alcoholic-beverages   \n",
       "categories_en          Beverages,Non-Alcoholic beverages   \n",
       "labels                                        Point Vert   \n",
       "labels_en                                      Green Dot   \n",
       "\n",
       "                                                   most_frequent_5  \n",
       "product_name                          Aceite de oliva virgen extra  \n",
       "generic_name                                                 Bi√®re  \n",
       "quantity                                                     400 g  \n",
       "packaging                                          Bouteille,Verre  \n",
       "packaging_tags                                              sachet  \n",
       "categories       Snacks, Snacks sucr√©s, Chocolats, Chocolats noirs  \n",
       "categories_tags  en:snacks,en:sweet-snacks,en:chocolates,en:dar...  \n",
       "categories_en       Snacks,Sweet snacks,Chocolates,Dark chocolates  \n",
       "labels           Bio, Bio europ√©en, AB Agriculture Biologique  ...  \n",
       "labels_en                                         Vegetarian,Vegan  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nRows = len(data.index)\n",
    "odf = pd.DataFrame(columns = [\"most_frequent_1\",\"most_frequent_2\",\"most_frequent_3\",\"most_frequent_4\",\"most_frequent_5\"])\n",
    "\n",
    "for col in data.columns:\n",
    "    freq_list = data[col].value_counts().head(5).index.tolist()\n",
    "    while (len(freq_list) < 5) :\n",
    "        freq_list.append(\"\")\n",
    "    odf.loc[col] = freq_list\n",
    "\n",
    "odf.insert(0,\"completeness\",(data.count()/len(data.index)*100).values)\n",
    "odf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odf.to_csv('odf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the CSV file, no values among the most frequent ones caracterize missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column preprocessing helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to use them to clean the wanted column_to_clean, please look how I clean \"categories\" field in FOOD DIET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = False, langdetec = False, tokenize = False, stemming = False, lemmatizing = False, onlyEngStemmer = False, applyNounFilter = False):\n",
    "    \"\"\"function that appeals all cleaning methods depending of the booleans\"\"\"\n",
    "    cleaned_data = pd.DataFrame()    \n",
    "    cleaned_data[column_to_clean] = data[column_to_clean].copy()\n",
    "    \n",
    "    \n",
    "    if langdetec:\n",
    "        cleaned_data[\"languages\"] = data[data[column_to_clean].notnull()][column_to_clean].apply(language_detection)\n",
    "        #I found all the languages disponibles for stemming, and i map the corresponding name to the ISO_code found by langdetect.detect\n",
    "        cleaned_data[\"languages\"] = cleaned_data[\"languages\"].map({\"ar\": \"arabic\", \"da\": \"danish\", \"nl\" : \"dutch\", \"en\": \"english\", \"fi\": \"finnish\", \"fr\": \"french\", \"de\": \"german\", \\\n",
    "                                  \"hu\": \"hungarian\", \"it\": \"italian\", \"no\": \"norwegian\", \"ro\": \"romanian\", \"ru\" : \"russian\", \"es\": \"spanish\", \\\n",
    "                                  \"sv\" :\"swedish\"})\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_langdetect.pkl\")\n",
    "       \n",
    "    #tokenize column_to_clean : --> stemming + lemmatization need list of tokens\n",
    "    if tokenize:\n",
    "        cleaned_data[column_to_clean] = tokenize_data(cleaned_data[column_to_clean], stop_words, tokenizer, applyNounFilter)\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_token.pkl\")\n",
    "    else: \n",
    "        cleaned_data = pd.read_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_token.pkl\") \n",
    "    \n",
    "    #stemm column_to_clean column according to the language used\n",
    "    if stemming:\n",
    "        #for stemming, langdetection as to be made previously\n",
    "        if (not(langdetec or onlyEngStemmer)):\n",
    "            lang_data = pd.read_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_langdetect.pkl\")\n",
    "            cleaned_data[\"languages\"] = lang_data[\"languages\"].copy()\n",
    "        \n",
    "        cleaned_data = stemming_data(cleaned_data, column_to_clean, onlyEngStemmer)\n",
    "        \n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_stem.pkl\")  \n",
    "            \n",
    "    elif lemmatizing: \n",
    "        #lemmatizing only works well for english words\n",
    "        cleaned_data = lemmatizing_data(cleaned_data, column_to_clean, wordnet_lemmatizer)\n",
    "        if saving: \n",
    "            cleaned_data.to_pickle(\"processed_pickle/\"+str(column_to_clean)+\"/out_lem\")    \n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_nountag(tokenlist_cell):\n",
    "    \"\"\"keep only NN : nouns, singular or mass and NNS : noun,plural and NNP : proper noun\"\"\"\n",
    "    \n",
    "    postag_cell = pos_tag(tokenlist_cell)\n",
    "    postag_cell_filtered = [tag for tag in postag_cell if ((tag[1]=='NNS') or (tag[1] == 'NN') or (tag[1] == \"NNP\"))]\n",
    "    tokenlist_cell = [tag[0] for tag in postag_cell_filtered]\n",
    "    \n",
    "    return tokenlist_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_data(data_column, stop_words, tokenizer, applyNounFilter = False):\n",
    "    \"\"\"Clean data, i.e. \n",
    "        - lower each words in the cells of data_column\n",
    "        - tokenize cells of data_column, i.e. from float type create list of string (token)\n",
    "        - remove stopwords for the list of tokens for each cells of data_column\n",
    "        - keep only tokens with tag = NN (noun), or NNS (noun, plural)\n",
    "    \"\"\"  \n",
    "    data_column = data_column[data_column.notnull()].str.lower() \\\n",
    "        .apply(str) \\\n",
    "        .apply(tokenizer.tokenize) \\\n",
    "        .apply(lambda cell : [item for item in cell if item not in stop_words]) \n",
    "    if applyNounFilter:\n",
    "        data_column = data_column[data_column.notnull()].apply(filter_nountag)\n",
    "    \n",
    "    return data_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_detection(category_cell): \n",
    "    \"\"\"take a cell containing a the string from unprocessed dataframe and detect the language\"\"\"\n",
    "    tmp_cell = str()\n",
    "    tmp_cell = category_cell\n",
    "    \n",
    "    #supress numbers in string\n",
    "    tmp_cell = tmp_cell.replace('\\d+', '')\n",
    "    #supress punctuations\n",
    "    tmp_cell = re.sub(r'[^\\w\\s]','', tmp_cell)\n",
    "    #remove spaces in string\n",
    "    tmp_cell = ''.join(tmp_cell.split())\n",
    "    \n",
    "    #check if the string contain only letters --> f**king üç© \n",
    "    if tmp_cell.isalpha():\n",
    "        language = detect(category_cell)\n",
    "        return detect(category_cell)\n",
    "    else: \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemming_data(df_lang, serie_tokenlist, onlyEngStemmer = False):\n",
    "    \"\"\"Take as argument : \n",
    "        - df_lang = dataframe containing a serie of tokenlist, and a serie \"languages\" of corresponding language\n",
    "        - serie_tokenlist = string name of the serie of tokenlist to stem\n",
    "    \"\"\"\n",
    "    for i in [1, 2]:\n",
    "        for index, row in df_lang[df_lang[serie_tokenlist].notnull()].iterrows():\n",
    "            \n",
    "            if onlyEngStemmer: \n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "            else: \n",
    "                stemmer = SnowballStemmer(row[\"languages\"])\n",
    "                df_lang.iloc[index][serie_tokenlist] = [stemmer.stem(token) for token in row[serie_tokenlist]]\n",
    " \n",
    "        \n",
    "    return df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatizing_data(df, serie_tokenlist, wordnet_lemmatizer):\n",
    "    \"\"\"lemmatize serie containing token_list\n",
    "        - serie_tokenlist = string name of the serie of tokenlist to lemmatize\n",
    "        - df = dataframe containing \"serie_tokenlist\" column\n",
    "        \"\"\"\n",
    "    for index, row in df[df[serie_tokenlist].notnull()].iterrows():\n",
    "        df.iloc[index][serie_tokenlist] =  [wordnet_lemmatizer.lemmatize(token, pos=\"n\") for token in row[serie_tokenlist]]\n",
    "        #if u lemmatize verbs --> pos = \"-v\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOOD DIET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step = cleaning wanted column field by \n",
    "* Tokenization + Stemming / Lemmatazing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning \"categories\" field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> If column field has to be cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below aims to clean the \"categories\" field and save outputs of each steps of the cleaning in pickle format. \n",
    "* create a repository in processed_pickle which has the name of the column to clean, i.e. processed_pickle/categories/ is the repository in which pickles will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving = True\n",
    "# note : Language detection is only needed for stemming, \n",
    "# and it is a very long run (~1h)\n",
    "\n",
    "#StopWords and Tokenizer Object initialization\n",
    "stop_words = set(stopwords.words(\"french\")).union(set(stopwords.words(\"english\"))) #will remove only english and french stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_clean = \"categories\"\n",
    "#language detection\n",
    "langdetect_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, langdetec = True)\n",
    "#tokenization\n",
    "tok_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, tokenize = True, applyNounFilter = True)\n",
    "#stemming --> good since it consider language\n",
    "stem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, stemming = True)\n",
    "#lemmatize --> not very good since it does not consider language (only good for english)\n",
    "lem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, lemmatizing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> If column field already be cleaned and save in pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column = \"categories\"\n",
    "langdetect_data = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_langdetect.pkl\")\n",
    "categories_tok = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_token.pkl\") \n",
    "categories_stem = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_stem.pkl\") \n",
    "categories_lem = pd.read_pickle(\"processed_pickle/\"+str(column)+\"/out_lem.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliment\n",
      "boisson\n",
      "produit\n",
      "snack\n",
      "fruit\n",
      "lait\n",
      "fromag\n",
      "plat\n",
      "c√©r√©al\n",
      "biscuit\n",
      "bas\n",
      "plant\n",
      "food\n",
      "sauc\n",
      "viand\n",
      "pomm\n",
      "l√©gum\n",
      "dessert\n",
      "v√©g√©tal\n",
      "p√¢t\n",
      "confitur\n",
      "beverag\n",
      "poisson\n",
      "chocolat\n",
      "product\n",
      "yaourt\n",
      "d√©jeun\n",
      "lebensmittel\n",
      "nectar\n",
      "jus\n",
      "conserv\n",
      "pet\n",
      "cereal\n",
      "charcut\n",
      "getrank\n",
      "mat\n",
      "pflanzlich\n",
      "snacks\n",
      "chip\n",
      "fr\n",
      "pain\n",
      "orig\n",
      "grain\n",
      "huil\n",
      "tartin\n",
      "epic\n",
      "grass\n",
      "glac\n",
      "franc\n",
      "poulet\n",
      "vin\n",
      "confis\n",
      "coqu\n",
      "miel\n",
      "ajout\n",
      "milk\n",
      "crem\n",
      "soup\n",
      "surgel\n",
      "l√©gumin\n",
      "bonbon\n",
      "volaill\n",
      "san\n",
      "bi\n",
      "pr√©par\n",
      "compot\n",
      "filet\n",
      "marmelad\n",
      "tart\n",
      "foi\n",
      "jambons\n",
      "salad\n",
      "rillet\n",
      "frut\n",
      "sirop\n",
      "oliv\n",
      "sardin\n",
      "thon\n",
      "alimentair\n",
      "cuit\n",
      "jambon\n",
      "roug\n",
      "chocolats\n",
      "sorbet\n",
      "juic\n",
      "√©levag\n",
      "steak\n",
      "beb\n",
      "cond\n",
      "veget\n",
      "terrin\n",
      "pickl\n",
      "spread\n",
      "flocon\n",
      "√©dulcor\n",
      "tomat\n",
      "barr\n",
      "beurr\n",
      "sauces\n",
      "saumon\n",
      "frais\n",
      "meal\n",
      "cream\n",
      "vert\n",
      "sus\n",
      "meat\n",
      "vegetal\n",
      "beverages\n",
      "chocol\n",
      "p√¢tiss\n",
      "viennois\n",
      "riz\n",
      "milch\n",
      "yogurt\n",
      "orang\n",
      "sech\n",
      "champignon\n",
      "sandwich\n",
      "chaud\n",
      "brioch\n",
      "dulc\n",
      "caf\n",
      "fra√Æch\n",
      "charcuteri\n",
      "dairi\n",
      "galet\n",
      "noiset\n",
      "milchprodukt\n",
      "quich\n",
      "bread\n",
      "noir\n",
      "tea\n",
      "m√©lang\n",
      "gazeux\n",
      "onde\n",
      "cr√™p\n",
      "g√¢teau\n",
      "chee\n",
      "lech\n",
      "natur\n",
      "viandes\n",
      "vinaigres\n",
      "multifruit\n",
      "pur\n",
      "moutard\n",
      "emmental\n",
      "enti\n",
      "biscuits\n",
      "fleur\n",
      "aliments\n",
      "verdur\n",
      "amand\n",
      "infus\n",
      "groceri\n",
      "ques\n",
      "haricot\n",
      "drink\n",
      "fermentiert\n",
      "cro√ªt\n",
      "farc\n",
      "bar\n",
      "pan\n",
      "bot\n",
      "pizz\n",
      "cake\n",
      "chips\n",
      "milks\n",
      "comt\n",
      "sucr\n",
      "kas\n",
      "madelein\n",
      "bean\n",
      "escalop\n",
      "confiseri\n",
      "getreid\n",
      "fat\n",
      "potato\n",
      "bouillon\n",
      "dind\n",
      "jam\n",
      "oil\n",
      "congel\n",
      "brotaufstrich\n",
      "pasta\n",
      "aromat\n",
      "breakfast\n",
      "semoul\n",
      "farin\n",
      "ice\n",
      "charcuteries\n",
      "gras\n",
      "chair\n",
      "candi\n",
      "lentill\n",
      "sel\n",
      "carott\n",
      "reconstitu\n",
      "nouill\n",
      "foods\n",
      "mouss\n",
      "vierg\n",
      "hortaliz\n",
      "past\n",
      "com\n",
      "epicer\n",
      "nut\n",
      "rice\n",
      "seed\n",
      "untabl\n",
      "butter\n",
      "base\n",
      "compl\n",
      "boudin\n",
      "sal\n",
      "concentr\n",
      "prep\n",
      "frit\n",
      "oils\n",
      "epiceri\n",
      "corn\n",
      "th√©\n",
      "animal\n",
      "lact\n",
      "yogur\n",
      "virg\n",
      "con\n",
      "yogurts\n",
      "–∏–∑\n",
      "gratin\n",
      "legum\n",
      "gallet\n",
      "canard\n",
      "frucht\n",
      "dur\n",
      "aid\n",
      "tournesol\n",
      "substitut\n",
      "cook\n",
      "blanc\n",
      "campagn\n",
      "deriv\n",
      "cru\n",
      "–∏\n",
      "desayun\n",
      "bebidas\n",
      "farines\n",
      "oignon\n",
      "e\n",
      "min√©ral\n",
      "pastel\n",
      "√©pic\n",
      "mayon\n",
      "almond\n",
      "cacahuet\n",
      "casc\n",
      "wine\n",
      "carn\n",
      "legumin\n",
      "butt\n",
      "vanill\n",
      "poul\n",
      "doset\n",
      "fri\n",
      "fresc\n",
      "tomato\n",
      "porc\n",
      "condimentair\n",
      "beignet\n",
      "camembert\n",
      "margarin\n",
      "lasagn\n",
      "cuiss\n",
      "condiment\n",
      "blanch\n",
      "raisin\n",
      "raviol\n",
      "sod\n",
      "confectioneri\n",
      "citron\n",
      "r√©frig\n",
      "salti\n",
      "mois\n",
      "complet\n",
      "bl√©\n",
      "c√©r√©ales\n",
      "quenel\n",
      "culinair\n",
      "honey\n",
      "muesl\n",
      "—Å—ã—Ä\n",
      "feuill\n",
      "bloc\n",
      "fish\n",
      "crevet\n",
      "drinks\n",
      "peanut\n",
      "caramel\n",
      "syrup\n",
      "sourc\n",
      "frambois\n",
      "col\n",
      "hach\n",
      "wheat\n",
      "fleisch\n",
      "cidr\n",
      "croiss\n",
      "flake\n",
      "produits\n",
      "turron\n",
      "vach\n",
      "ber\n",
      "taboul\n",
      "blond\n",
      "—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω\n",
      "tapenad\n",
      "cibi\n",
      "appet\n",
      "ketchup\n",
      "pies\n",
      "cheddar\n",
      "coff\n",
      "sels\n",
      "comest\n",
      "mustard\n",
      "d√©riv\n",
      "water\n",
      "gum\n",
      "italien\n",
      "herb\n",
      "egg\n",
      "p√¢qu\n",
      "naturel\n",
      "products\n",
      "extra\n",
      "mayonnaises\n",
      "flak\n",
      "salmon\n",
      "truit\n",
      "barbecu\n",
      "chipolat\n",
      "ma√Ø\n",
      "appl\n",
      "coco\n",
      "pizzas\n",
      "fisch\n",
      "deshidrat\n",
      "lardon\n",
      "conf\n",
      "balsamiques\n",
      "joghurt\n",
      "di\n",
      "condiments\n",
      "kuch\n",
      "√©crem\n",
      "muffin\n",
      "alcool\n",
      "und\n",
      "fruhstuck\n",
      "gaufr\n",
      "par\n",
      "dair\n",
      "mit\n",
      "vinaigr\n",
      "confiseries\n",
      "tortill\n",
      "industriel\n",
      "zum\n",
      "pistach\n",
      "–Ω–∞–ø–∏—Ç–∫\n",
      "sop\n",
      "nect\n",
      "asperg\n",
      "desserts\n",
      "biscott\n",
      "aguas\n",
      "plats\n",
      "poudr\n",
      "postr\n",
      "assaison\n",
      "gaufret\n",
      "brass\n",
      "√©pinard\n",
      "pesc\n",
      "≈ìuf\n",
      "goud\n",
      "poir\n",
      "patat\n",
      "cooki\n",
      "boissons\n",
      "boir\n",
      "fum\n",
      "win\n",
      "cib\n",
      "alkohol\n",
      "saint\n",
      "moulag\n",
      "produkt\n",
      "pl\n",
      "fruits\n",
      "poivr\n",
      "p√™ch\n",
      "semill\n",
      "bacon\n",
      "moutardes\n",
      "noix\n",
      "hareng\n",
      "√©dulcorants\n",
      "dairies\n",
      "mar\n",
      "seafood\n",
      "chicken\n",
      "beef\n",
      "myrtill\n",
      "tranch\n",
      "vinaigret\n",
      "cornichon\n",
      "brot\n",
      "minerales\n",
      "breb\n",
      "virgen\n",
      "figu\n",
      "pudding\n",
      "cer\n",
      "bovin\n"
     ]
    }
   ],
   "source": [
    "categories_dict = categories_stem[categories_stem[\"categories\"].notnull()][\"categories\"].explode().value_counts()\n",
    "for word in categories_dict[categories_dict>500].index:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for categories field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regimes = ['vegetarian', 'vegan', 'glutenfree', 'lactosefree', 'ketogenic', 'organic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialization needed to create a serie containing unauthorized items  for each of the diets\n",
    "unauthorized_categories = pd.Series(index = regimes).rename('unauthorized_categories')\n",
    "\n",
    "unauthorized_listnames_categories = ['categories_' + str(x) + '_NOlist' for x in regimes]\n",
    "unauthorized_listnames_categories\n",
    "\n",
    "#Initialization needed to create a serie containing authorized items for each of the diets\n",
    "authorized_categories = pd.Series(index = regimes).rename('authorized_labelsen')\n",
    "\n",
    "authorized_listnames_categories = ['categories_' + str(x) + '_YESlist' for x in regimes]\n",
    "authorized_listnames_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vegetarian_NOlist = regimes(0)\n",
    "categories_Vegetarian_NOlist = ['foi', 'sauc', 'steak', 'viand', 'viandes', 'meat', 'beef', 'poisson', 'charcut', 'charcuteries', 'charcuteri', 'poulet', 'poul', 'volaill', 'jambons', 'jambon', 'jam', 'rillet', 'sardin', 'thon', 'steak', 'saumon', 'salmon', 'escalop', 'dind', 'jam', \\\n",
    "                             'boudin', 'animal', 'cannard', 'porc', 'lasagn', 'cuiss', 'fish', 'crevet', 'fleisch', 'vach', 'chipolat', 'fisch', 'lardon', 'bacon', 'hareng', 'seafood', 'chicken', 'breb', 'bovin']\n",
    "categories_Vegetarian_YESlist = []\n",
    "\n",
    "#Lactosefree = regime(3)\n",
    "categories_Lactosefree_NOlist = ['fromag', 'lait', 'yaourt', 'bi', '√©crem', 'cream', 'crem', 'glac', 'milk', 'milks', 'dairi', 'dair', 'dairies', 'beurr', 'milch', 'milchprodukt', 'chee', \\\n",
    "                                'cake', 'cr√™p', 'bread', 'g√¢teau' , 'lait', 'butter', 'butt', 'cooki', 'emmental', 'comt', 'camembert', 'margarin', 'kas', 'cheddar', 'yogurt', 'yogur', 'yogurts']\n",
    "categories_Lactosefree_YESlist = []\n",
    "\n",
    "\n",
    "#Vegan_NOlist = regimes(1)\n",
    "categories_Vegan_NOlist = categories_Lactosefree_NOlist + \\\n",
    "                          categories_Vegetarian_NOlist + \\\n",
    "                         ['mayon', 'mayonnaises', 'mayonnaises', 'moutardes', 'mustard', 'egg', '≈ìuf']\n",
    "categories_Vegan_YESlist = []\n",
    "\n",
    "#Glutenfree = regime(2)\n",
    "categories_Glutenfree_NOlist = ['p√¢t', 'pasta', 'p√¢tiss', 'past', 'nouill', 'viennois', 'biscuit', 'biscuits', 'sauc', 'sauces', 'charcut', 'charcuteri', 'charcuteries', 'bread', 'cr√™p', 'g√¢teau', 'farin', 'farines', 'pizz', 'pizzas', 'cake', 'madelein', \\\n",
    "                                'brotaufstrich', 'brot', 'beignet', 'lasagn', 'raviol', 'bl√©', 'wheat', 'bi', 'pies', 'kuch', 'muffin', 'gaufr', 'gaufret', 'biscott', 'cooki', 'cook', 'brot']\n",
    "categories_Glutenfree_YESlist = []\n",
    "\n",
    "\n",
    "#Ketogenic = regimes(4)\n",
    "categories_Ketogenic_NOlist = ['sucr', 'pomm', 'confitur', 'confiseries', 'confiseri', 'confis' ,'orang', 'nouill', 'semoul', 'sorbet', 'legumin', 'l√©gumin', 'multifruit', 'potato', 'lait', 'bean', 'haricot', 'lentill', 'lentill', 'riz', 'pasta', \\\n",
    "                               'pasta', 'milk', 'milks', 'sirop', 'candi', 'jus', 'juic', 'tart', 'marmelad', 'snack', 'snacks', 'biscuit', 'p√¢t', 'confitur', 'chocolat', 'chocolats', 'chocol', \\\n",
    "                               'nectar', 'nect', 'bread', 'chip', 'chips', 'pain', 'miel', 'bonbon', 'compot', 'p√¢tiss']\n",
    "categories_Ketogenic_YESlist = categories_Lactosefree_NOlist + \\\n",
    "                               categories_Vegetarian_NOlist + \\\n",
    "                               ['v√©g√©tal', 'grass', 'huil', 'grain', 'nut', 'tomato', 'tomat', 'oils', 'oil', 'moutardes', 'mustard', 'fat', 'mayon', 'mayonnaises', 'mayonnaises', \\\n",
    "                                'seed', 'gras']\n",
    "\n",
    "\n",
    "\n",
    "#Organic = regimes(5)\n",
    "categories_Organic_NOlist = []\n",
    "categories_Organic_YESlist = []\n",
    "\n",
    "\n",
    "categories_YESlists = [categories_Vegetarian_YESlist, categories_Vegan_YESlist, \\\n",
    "                      categories_Glutenfree_YESlist, categories_Lactosefree_YESlist, \\\n",
    "                      categories_Ketogenic_YESlist, categories_Organic_YESlist]\n",
    "\n",
    "categories_NOlists = [categories_Vegetarian_NOlist, categories_Vegan_NOlist, \\\n",
    "                      categories_Glutenfree_NOlist, categories_Lactosefree_NOlist, \\\n",
    "                      categories_Ketogenic_NOlist, categories_Organic_NOlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning labels_en field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving = True\n",
    "# note : Language detection is only needed for stemming, \n",
    "# and it is a very long run (~1h)\n",
    "\n",
    "#StopWords and Tokenizer Object initialization\n",
    "stop_words = set(stopwords.words(\"french\")).union(set(stopwords.words(\"english\"))) #will remove only english and french stopwords\n",
    "#tokenizer_withdash = RegexpTokenizer('(?:(?!\\d)\\w)+|\\S+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a pattern to try to include periods and commas occurring inside words, numbers. Hope this helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_withdash = RegexpTokenizer('(?:(?!\\d)\\w)+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_clean = \"labels_en\"\n",
    "\n",
    "#tokenization\n",
    "tok_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving=True, tokenize=True)\n",
    "#stemming --> good since it consider language\n",
    "stem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving=True, stemming=True, onlyEngStemmer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_tok = pd.read_pickle(\"processed_pickle/labels_en/out_token.pkl\")\n",
    "label_stem = pd.read_pickle(\"processed_pickle/labels_en/out_stem.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for labels_en field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Organic                                                                            18439\n",
       "Gluten-free                                                                        13356\n",
       "Organic,EU Organic,fr:ab-agriculture-biologique                                     6842\n",
       "Green Dot                                                                           6579\n",
       "Vegetarian,Vegan                                                                    5124\n",
       "Made in France                                                                      5055\n",
       "No preservatives                                                                    3889\n",
       "No colorings                                                                        2885\n",
       "No added sugar                                                                      2334\n",
       "Organic,EU Organic,FR-BIO-01                                                        2117\n",
       "No colorings,No preservatives                                                       1828\n",
       "Halal                                                                               1620\n",
       "Palm oil free                                                                       1586\n",
       "Organic,EU Organic,FR-BIO-01,fr:ab-agriculture-biologique                           1573\n",
       "No lactose                                                                          1520\n",
       "PDO                                                                                 1501\n",
       "Made in Germany                                                                     1444\n",
       "Gluten-free,No lactose                                                              1355\n",
       "Made in Italy                                                                       1274\n",
       "Sustainable fishery,Sustainable Seafood MSC                                         1203\n",
       "PGI                                                                                 1104\n",
       "Vegetarian,Gluten-free,Vegan                                                        1025\n",
       "French meat,French pork                                                             1016\n",
       "Vegetarian                                                                           881\n",
       "Green Dot,Made in France                                                             850\n",
       "Made in Belgium                                                                      815\n",
       "Superior quality                                                                     768\n",
       "Organic,EU Organic                                                                   767\n",
       "Made in Spain                                                                        742\n",
       "Green Dot,fr:eco-emballages                                                          734\n",
       "Organic,EU Organic,EU/non-EU Agriculture,FR-BIO-01                                   696\n",
       "Organic,EU Organic,EU/non-EU Agriculture,FR-BIO-01,fr:ab-agriculture-biologique      657\n",
       "Organic,Gluten-free                                                                  633\n",
       "fr:viande-francaise,fr:Volaille Fran√ßaise                                            625\n",
       "fr:D√©conseill√© aux femmes enceintes                                                  613\n",
       "No additives                                                                         613\n",
       "Distributor labels,fr:S√©lection Intermarch√©                                          613\n",
       "Green Dot,fr:Triman                                                                  592\n",
       "Nutriscore,Nutriscore Grade A                                                        577\n",
       "Sustainable farming,UTZ Certified,UTZ Certified Cacao                                573\n",
       "Nutriscore                                                                           568\n",
       "fr:viande-francaise,fr:viande-porcine-francaise                                      555\n",
       "Organic,Vegetarian,Vegan                                                             534\n",
       "Organic,EU Organic,NL-BIO-01                                                         531\n",
       "Produced in Bretagne                                                                 502\n",
       "Organic,EU Organic,FR-BIO-10                                                         490\n",
       "Lait-francais-french-milk                                                            472\n",
       "Organic,EU Organic,DE-√ñKO-001                                                        460\n",
       "French meat,fr:Viande Bovine Fran√ßaise                                               447\n",
       "fr:Entrepreneurs + Engag√©s                                                           435\n",
       "Name: labels_en, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"labels_en\"].notnull()][\"labels_en\"].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organic\n",
      "fr\n",
      "free\n",
      "agriculture\n",
      "gluten\n",
      "green\n",
      "dot\n",
      "made\n",
      "bio\n",
      "france\n",
      "ab\n",
      "biologique\n",
      "vegetarian\n",
      "vegan\n",
      "preservatives\n",
      "01\n",
      "colorings\n",
      "sugar\n",
      "non\n",
      "nutriscore\n",
      "viande\n",
      "french\n",
      "added\n",
      "low\n",
      "oil\n",
      "palm\n",
      "sustainable\n",
      "francaise\n",
      "lactose\n",
      "carbon\n",
      "footprint\n",
      "eco\n",
      "fsc\n",
      "√∂ko\n",
      "flavors\n",
      "artificial\n",
      "certified\n",
      "grade\n",
      "emballages\n",
      "halal\n",
      "fat\n",
      "milk\n",
      "fran√ßaise\n",
      "pork\n",
      "salt\n",
      "germany\n",
      "utz\n",
      "high\n",
      "quality\n",
      "pure\n",
      "natural\n",
      "pdo\n",
      "meat\n",
      "pgi\n",
      "labels\n",
      "italy\n",
      "society\n",
      "gmos\n",
      "fair\n",
      "distributor\n",
      "trade\n",
      "lait\n",
      "transformed\n",
      "006\n",
      "german\n",
      "agricultural\n",
      "additives\n",
      "femmes\n",
      "enceintes\n",
      "d√©conseill√©\n",
      "001\n",
      "kosher\n",
      "msc\n",
      "100\n",
      "triman\n",
      "francais\n",
      "10\n",
      "fishery\n",
      "seafood\n",
      "union\n",
      "superior\n",
      "nl\n",
      "butter\n",
      "bretagne\n",
      "produced\n",
      "bovine\n",
      "reduced\n",
      "farming\n",
      "s√©lection\n",
      "intermarch√©\n",
      "porcine\n",
      "medal\n",
      "sans\n",
      "ru\n",
      "label\n",
      "fran√ßais\n",
      "without\n",
      "volaille\n",
      "european\n",
      "origine\n",
      "mix\n",
      "max\n",
      "havelaar\n",
      "belgium\n",
      "spain\n",
      "source\n",
      "concours\n",
      "fibres\n",
      "cacao\n",
      "gold\n",
      "oko\n",
      "colors\n",
      "saveurs\n",
      "007\n",
      "eg\n",
      "verordnung\n",
      "agricole\n",
      "ecocert\n",
      "g√©n√©ral\n",
      "3\n",
      "b\n",
      "class\n"
     ]
    }
   ],
   "source": [
    "labelsen_dict = label_stem[label_stem[\"labels_en\"].notnull()][\"labels_en\"].explode().value_counts()\n",
    "for word in labelen_dict[labelsen_dict > 1000].index:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labelsen_Vegetarian_NOlist',\n",
       " 'labelsen_Vegan_NOlist',\n",
       " 'labelsen_Glutenfree_NOlist',\n",
       " 'labelsen_Lactosefree_NOlist',\n",
       " 'labelsen_Ketogenic_NOlist',\n",
       " 'labelsen_Organic_NOlist']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization needed to create a serie containing authorized items for each of the diets\n",
    "authorized_labelsen = pd.Series(index = regimes).rename('authorized_labelsen')\n",
    "\n",
    "authorized_listnames_labelsen = ['labelsen_' + str(x) + '_YESlist' for x in regimes]\n",
    "authorized_listnames_labelsen\n",
    "\n",
    "#Initialization needed to create a serie containing unauthorized items  for each of the diets\n",
    "unauthorized_labelsen = pd.Series(index = regimes).rename('unauthorized_labelsen')\n",
    "\n",
    "unauthorized_listnames_labelsen = ['labelsen_' + str(x) + '_NOlist' for x in regimes]\n",
    "unauthorized_listnames_labelsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organic = regimes(5)\n",
    "labelsen_Organic_YESlist = ['organic', 'bio', 'biologique']\n",
    "#Ketogenic = regimes(4)\n",
    "labelsen_Ketogenic_YESlist = []\n",
    "\n",
    "#Lactosefree = regime(3) #since lactose only appears as NO lactose\n",
    "labelsen_Lactosefree_YESlist = ['lactose']\n",
    "\n",
    "#Glutenfree = regime(2)\n",
    "labelsen_Glutenfree_YESlist = ['gluten']\n",
    "\n",
    "#Vegan_NOlist = regimes(1)\n",
    "labelsen_Vegan_YESlist = ['vegan']\n",
    "\n",
    "#Vegetarian_NOlist = regimes(0)\n",
    "labelsen_Vegetarian_YESlist = ['vegetarian']\n",
    "\n",
    "labelsen_YESlists = [labelsen_Vegetarian_YESlist, labelsen_Vegan_YESlist, labelsen_Glutenfree_YESlist, \\\n",
    "                    labelsen_Lactosefree_YESlist, labelsen_Ketogenic_YESlist, labelsen_Organic_YESlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vegetarian', 'vegan', 'glutenfree', 'lactosefree', 'ketogenic', 'organic']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_diet(data_stem, column, regimes, YESlists = None, NOlists = None): \n",
    "    \"\"\"Complete the series \"diet\" of data_stem by taking informations in data_stem[column]\"\"\"\n",
    "    for  index, row in data_stem[data_stem[column].notnull()].iterrows():\n",
    "        for token in row[column]:\n",
    "            for counter, regime in enumerate(regimes):                \n",
    "                \n",
    "                if not(YESlists == None):\n",
    "                    if ((token in YESlists[counter]) & (regime not in data_stem.loc[index, \"diet\"])):\n",
    "                        data_stem.loc[index, \"diet\"] = data_stem.loc[index, \"diet\"] + [regime]\n",
    "                        \n",
    "                if not(NOlists == None): \n",
    "                    #remove the regime if it is in diet_series[index]\n",
    "                    if ((token in NOlists[counter]) & (regime in data_stem.loc[index, \"diet\"])):\n",
    "                        data_stem.loc[index, \"diet\"].remove(regime)\n",
    "                        \n",
    "    return data_stem                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize dataframe\n",
    "cleaned_fields = pd.DataFrame()\n",
    "cleaned_fields[\"categories\"] = categories_stem[\"categories\"].copy()\n",
    "cleaned_fields[\"labels_en\"] = label_stem[\"labels_en\"].copy()\n",
    "cleaned_fields[\"diet\"]= [[]]*len(cleaned_fields)\n",
    "#cleaned_fields[\"diet\"] = cleaned_fields[\"diet\"].apply(lambda x: x + [\"flexitarism\"])\n",
    "cleaned_fields[\"diet\"] =  [[\"flexitarism\"]]*len(cleaned_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_fields = find_diet(cleaned_fields, \"labels_en\", regimes, YESlists=labelsen_YESlists)\n",
    "cleaned_fields[cleaned_fields[\"labels_en\"].notnull()].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_fields.to_pickle(\"processed_pickle/_diet/labelsen_analysed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>labels_en</th>\n",
       "      <th>diet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism, Hello]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[fr, xsf]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017853</td>\n",
       "      <td>[tests]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[organic, vegetarian, vegan]</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017855</td>\n",
       "      <td>[beignet]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017856</td>\n",
       "      <td>[viand, steak, steak, steak, steak, hach√©s]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017858 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          categories  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                          [fr, xsf]   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "...                                              ...   \n",
       "1017853                                      [tests]   \n",
       "1017854                                          NaN   \n",
       "1017855                                    [beignet]   \n",
       "1017856  [viand, steak, steak, steak, steak, hach√©s]   \n",
       "1017857                                          NaN   \n",
       "\n",
       "                            labels_en                  diet  \n",
       "0                                 NaN  [flexitarism, Hello]  \n",
       "1                                 NaN         [flexitarism]  \n",
       "2                                 NaN         [flexitarism]  \n",
       "3                                 NaN         [flexitarism]  \n",
       "4                                 NaN         [flexitarism]  \n",
       "...                               ...                   ...  \n",
       "1017853                           NaN         [flexitarism]  \n",
       "1017854  [organic, vegetarian, vegan]         [flexitarism]  \n",
       "1017855                           NaN         [flexitarism]  \n",
       "1017856                           NaN         [flexitarism]  \n",
       "1017857                           NaN         [flexitarism]  \n",
       "\n",
       "[1017858 rows x 3 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_fields.loc[0,\"diet\"] = cleaned_fields.loc[0,\"diet\"] + [\"Hello\"]\n",
    "cleaned_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_fields.loc[0,\"diet\"].remove(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>labels_en</th>\n",
       "      <th>diet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[fr, xsf]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017853</td>\n",
       "      <td>[tests]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[organic, vegetarian, vegan]</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017855</td>\n",
       "      <td>[beignet]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017856</td>\n",
       "      <td>[viand, steak, steak, steak, steak, hach√©s]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flexitarism]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017858 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          categories  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                          [fr, xsf]   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "...                                              ...   \n",
       "1017853                                      [tests]   \n",
       "1017854                                          NaN   \n",
       "1017855                                    [beignet]   \n",
       "1017856  [viand, steak, steak, steak, steak, hach√©s]   \n",
       "1017857                                          NaN   \n",
       "\n",
       "                            labels_en           diet  \n",
       "0                                 NaN  [flexitarism]  \n",
       "1                                 NaN  [flexitarism]  \n",
       "2                                 NaN  [flexitarism]  \n",
       "3                                 NaN  [flexitarism]  \n",
       "4                                 NaN  [flexitarism]  \n",
       "...                               ...            ...  \n",
       "1017853                           NaN  [flexitarism]  \n",
       "1017854  [organic, vegetarian, vegan]  [flexitarism]  \n",
       "1017855                           NaN  [flexitarism]  \n",
       "1017856                           NaN  [flexitarism]  \n",
       "1017857                           NaN  [flexitarism]  \n",
       "\n",
       "[1017858 rows x 3 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017853</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017854</td>\n",
       "      <td>[organic, vegetarian, vegan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017855</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017856</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017857</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017858 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            labels_en\n",
       "0                                 NaN\n",
       "1                                 NaN\n",
       "2                                 NaN\n",
       "3                                 NaN\n",
       "4                                 NaN\n",
       "...                               ...\n",
       "1017853                           NaN\n",
       "1017854  [organic, vegetarian, vegan]\n",
       "1017855                           NaN\n",
       "1017856                           NaN\n",
       "1017857                           NaN\n",
       "\n",
       "[1017858 rows x 1 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOOD CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first step, we aim to categorize products according to their PNNS Category.\n",
    "\n",
    "https://www.cerin.org/rapports/groupes-groupes-daliments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_data = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = initial_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 category from PNNS norm\n",
    "pnns_category = [\"Dairies\",\"Composite\",\"Fish Meat Eggs\",\"Beverages\",\"Fat Sauces\",\"Fruits Vegetables\",\"Starchy\",\"Snacks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pnns_groups_1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sugary snacks              58626\n",
       "Milk and dairy products    43945\n",
       "Fish Meat Eggs             43040\n",
       "Cereals and potatoes       34562\n",
       "Beverages                  29319\n",
       "Fat and sauces             28375\n",
       "Composite foods            25712\n",
       "Fruits and vegetables      24710\n",
       "Salty snacks               19978\n",
       "sugary-snacks               3498\n",
       "fruits-and-vegetables       2924\n",
       "cereals-and-potatoes          44\n",
       "salty-snacks                   5\n",
       "Name: pnns_groups_1, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data[\"pnns_groups_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionnary to normalize category names into PNNS category\n",
    "pnns1_category = {\n",
    " 'Sugary snacks' : 'Snacks', \n",
    " 'Milk and dairy products' : 'Dairies',\n",
    " 'Composite foods' : 'Composite', \n",
    " 'Cereals and potatoes' : 'Starchy', \n",
    " 'Fish Meat Eggs' : 'Fish Meat Eggs',\n",
    " 'Beverages' : 'Beverages',\n",
    " 'Fat and sauces' : 'Fat Sauces',\n",
    " 'Fruits and vegetables' : 'Fruits Vegetables',\n",
    " 'Salty snacks' : 'Snacks',\n",
    " 'fruits-and-vegetables' : 'Fruits Vegetables',\n",
    " 'sugary-snacks' : 'Snacks',\n",
    " 'cereals-and-potatoes' : 'Starchy',\n",
    " 'salty-snacks' : 'Snacks'\n",
    "}\n",
    "data.loc[:,'pnns_groups_1'] = data['pnns_groups_1'].map(pnns1_category)\n",
    "data = data.rename(columns={\"pnns_groups_1\": \"pnns_category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Snacks               82107\n",
       "Dairies              43945\n",
       "Fish Meat Eggs       43040\n",
       "Starchy              34606\n",
       "Beverages            29319\n",
       "Fat Sauces           28375\n",
       "Fruits Vegetables    27634\n",
       "Composite            25712\n",
       "Name: pnns_category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New pnns categories\n",
    "data[\"pnns_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pnns_groups_2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alcoholic beverages       10460\n",
       "Pizza pies and quiches      402\n",
       "Name: pnns_groups_2, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values of pnns_groups_2 which haven't been classified in pnns_category\n",
    "data.loc[data.pnns_category.isna()][\"pnns_groups_2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionnary to extend pnns_categories from pnns_groups_2\n",
    "pnns2_category = {\n",
    " 'Pizza pies and quiches' : 'Composite', \n",
    " 'Alcoholic beverages' : 'Beverages',\n",
    "}\n",
    "data.loc[data.pnns_category.isna(),'pnns_category'] = data.loc[data.pnns_category.isna()]['pnns_groups_2'].map(pnns2_category)\n",
    "data = data.drop('pnns_groups_2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Snacks               82107\n",
       "Dairies              43945\n",
       "Fish Meat Eggs       43040\n",
       "Beverages            39779\n",
       "Starchy              34606\n",
       "Fat Sauces           28375\n",
       "Fruits Vegetables    27634\n",
       "Composite            26114\n",
       "Name: pnns_category, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"pnns_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From main_category_en column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beverages                          9485\n",
       "Groceries                          6052\n",
       "Plant-based foods and beverages    5362\n",
       "Snacks                             3535\n",
       "Dietary supplements                2032\n",
       "Dairies                            1578\n",
       "Desserts                           1300\n",
       "Sweeteners                         1248\n",
       "Baby foods                         1242\n",
       "Cooking helpers                     743\n",
       "Cr√™pes and galettes                 689\n",
       "Food additives                      595\n",
       "Non food products                   521\n",
       "Canned foods                        480\n",
       "Frozen foods                        452\n",
       "Spreads                             398\n",
       "Cocoa and chocolate powders         332\n",
       "Licensed products                   306\n",
       "Syrups                              288\n",
       "Chips and fries                     267\n",
       "es:bolleria-industrial              237\n",
       "fr:bloc-de-foie-gras-de-canard      229\n",
       "Pizza dough                         204\n",
       "Breakfasts                          198\n",
       "Banana-crisps                       197\n",
       "Fish eggs                           172\n",
       "Terrines                            168\n",
       "fr:escalopes                        165\n",
       "Salads                              164\n",
       "fr:pilons-de-poulet                 136\n",
       "Name: main_category_en, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.pnns_category.isna()][\"main_category_en\"].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After inspecting the food type for the 30 most frequent values in main_category_en, we create the following dictionary\n",
    "maincategoryen_category = {\n",
    "'Beverages' : 'Beverages',\n",
    "'Snacks' : 'Snacks',\n",
    "'Dairies' : 'Dairies',\n",
    "'Desserts' : 'Dairies',\n",
    "'Cr√™pes and galettes' : 'Snacks',\n",
    "'Cocoa and chocolate powders': 'Snacks',\n",
    "'Syrups':'Fat Sauces',\n",
    "'Chips and fries': 'Snacks',\n",
    "'es:bolleria-industrial': 'Snacks',\n",
    "'fr:bloc-de-foie-gras-de-canard': 'Fish Meat Eggs' ,\n",
    "'Pizza dough' : 'Starchy',\n",
    "'Breakfast' : 'Starchy',\n",
    "'Banana-crisps' : \"Snacks\",\n",
    "'Fish eggs' : 'Fish Meat Eggs',\n",
    "'Terrines' : 'Fish Meat Eggs',\n",
    "'fr:escalopes' : 'Fish Meat Eggs',\n",
    "'Salads' : 'Fruits Vegetables',\n",
    "'fr:pilons-de-poulet' : 'Fish Meat Eggs'\n",
    "}\n",
    "\n",
    "data.loc[data.pnns_category.isna(),'pnns_category'] =  data.loc[data.pnns_category.isna(),'main_category_en'].map(maincategoryen_category)\n",
    "data = data.drop('main_category_en',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3290845377633065"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"pnns_category\"].value_counts())/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After these steps, we managed to categorize 33% of our date among PNNS groups.**\n",
    "\n",
    "For the unclassified data, the other columns don't bring any strong evidence for the categorization of the product. For instance, a lot of Dietary Supplements or Non Food Products are unclassified. Also, the table beneath shows that below 6% of the unclassified data have values in columns representing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_name                               0.931412\n",
       "energy_100g                                0.767232\n",
       "proteins_100g                              0.764975\n",
       "carbohydrates_100g                         0.763191\n",
       "fat_100g                                   0.762912\n",
       "sugars_100g                                0.749857\n",
       "salt_100g                                  0.745506\n",
       "sodium_100g                                0.745460\n",
       "energy-kcal_100g                           0.741572\n",
       "saturated-fat_100g                         0.730360\n",
       "ingredients_text                           0.333944\n",
       "additives_n                                0.333791\n",
       "ingredients_from_palm_oil_n                0.333791\n",
       "ingredients_that_may_be_from_palm_oil_n    0.333791\n",
       "fiber_100g                                 0.253628\n",
       "serving_quantity                           0.249664\n",
       "serving_size                               0.249524\n",
       "nova_group                                 0.193878\n",
       "additives_en                               0.186772\n",
       "additives_tags                             0.186772\n",
       "quantity                                   0.175353\n",
       "cholesterol_100g                           0.172855\n",
       "trans-fat_100g                             0.172052\n",
       "iron_100g                                  0.169706\n",
       "vitamin-c_100g                             0.169349\n",
       "calcium_100g                               0.168746\n",
       "vitamin-a_100g                             0.166091\n",
       "labels                                     0.119601\n",
       "labels_en                                  0.119595\n",
       "categories                                 0.061012\n",
       "categories_tags                            0.061006\n",
       "categories_en                              0.061006\n",
       "main_category                              0.060927\n",
       "packaging                                  0.047274\n",
       "packaging_tags                             0.047265\n",
       "nutrition-score-uk_100g                    0.036315\n",
       "nutrition-score-fr_100g                    0.036315\n",
       "nutrition_grade_fr                         0.036315\n",
       "traces_tags                                0.030537\n",
       "traces_en                                  0.030537\n",
       "energy-kj_100g                             0.025540\n",
       "allergens                                  0.018101\n",
       "generic_name                               0.015380\n",
       "traces                                     0.015047\n",
       "pnns_category                              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.pnns_category.isnull()].count().sort_values(ascending=False)/len(data.loc[data.pnns_category.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_data = data.loc[~data.pnns_category.isnull()]\n",
    "unclassified_data = data.loc[data.pnns_category.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6275750842649371"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_data.loc[~classified_data.ingredients_text.isnull(),\"ingredients_text\"].count()/len(classified_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33% of the remaining data has a list of ingredients, and 62% of our classified data also has a list of ingredients. We will use Jaccard similarities on the words in ingredient_text to associate unclassified products to categories if the similarity is big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ingredients_text column (IN THE WORKINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_data.loc[:,\"ingredients_text\"] = classified_data.loc[:,\"ingredients_text\"].str.lower() \\\n",
    "                                            .replace('%','') \\\n",
    "                                            .replace('-','') \\\n",
    "                                            .replace(':','') \\\n",
    "                                            .replace(',','') \\\n",
    "                                            .replace(\"  \",'') \\\n",
    "                                            .replace(\"(\",'') \\\n",
    "                                            .replace(\")\",'') \\\n",
    "                                            .replace(\";\",'')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "stopWords = stopwords.words('french')\n",
    "\n",
    "classified_data.loc[:,\"ingredients_text\"] = classified_data.loc[:,\"ingredients_text\"].str.lower()\n",
    "\n",
    "def createWordSet(df):\n",
    "    \n",
    "    count = Counter([ word for sentence in df.loc[~df[\"ingredients_text\"].isnull(),\"ingredients_text\"].tolist() for word in sentence.split(\" \")])\n",
    "    return [word[0] for word in count.most_common(100)]\n",
    "\n",
    "res = classified_data.groupby(\"pnns_category\").apply(createWordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res[\"Fat Sauces\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PACKAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ebauche of dictionnary to class type of packaging Plastique/Recylable(nonplastik)/nonrecyclable\n",
    "pack_mapping = dict.fromkeys(['plastic', 'plastique,frais','plastique', 'barquette,plastique', 'sachet,plastique',\n",
    "                              'frais,plastique','carton,plastique','carton,plastique,surgele','flacon,plastique'\n",
    "                 'sachet-plastique','carton,boite,plastique', 'boite,plastique', 'plastique,carton','sachet-plastique',\n",
    "                'sachet,plastique,surgele','barquette,plastique,frais','pot,plastique','plastique,sachet',\n",
    "                             'plastico','boite,carton,plastique','frais,plastique,carton','frais,carton,plastique',\n",
    "                              'carton,sachet,plastique','kunststoff','barquette,film,plastique,sous-atmosphere-protectrice',\n",
    "                             'carton,surgele,plastique','plastique,barquette','sachet,plastique,frais',\n",
    "                             'barquette,plastique,sous-atmosphere-protectrice','frais,barquette,plastique,opercule,film-plastique,sous-atmosphere-protectrice',\n",
    "                             'sachet,plastique,carton','film,plastique','sachet,plastique,sous-atmosphere-protectrice',\n",
    "                             'bolsa-de-plastico,ultracongelado','barquette-plastique','plastique,sous-vide','plastic,bag',\n",
    "                             'barquette,film,plastique','plastik','plastica','pot-plastique','plastic-bag','boite-plastique',\n",
    "                             'frais,barquette,plastique','05-pp','pp','sachet,plastique,etui,carton','bolsa-de-plastico',\n",
    "                             'boite,carton,sachet,plastique','pot,plastique,frais','botella,plastico','sachet,plastique,doypack',\n",
    "                             'sachet,plastique,sachet','plastique,boite','plastique,surgele','sac-plastique'],'Plastic')\n",
    "\n",
    "pack_mapping.update(dict.fromkeys(['bouteille,verre','carton','boite,carton','carton,surgele','karton','carton,boite','karton,kunststoff',\n",
    "                                  'sachet,papier','papier','bouteille,plastique','conserve','bocal,verre','glas',\n",
    "                                  'bouteille','conserve,metal','bocal,verre,couvercle,metal','bocal,verre,metal',\n",
    "                                  'verre','verre,bouteille','pot,verre','bocal','verre,bocal',\n",
    "                                  'flacon,plastique','bouteille-plastique','bottle','canned','pot-en-verre','bolsa,plastico',\n",
    "                                   'bouteille-verre','bocal-en-verre','verre,bocal,metal','plastique,bouteille','bouteille-en-verre','canette','pot,verre,couvercle,metal',\n",
    "                                  'glas,mehrwegpfand','carton,aluminium','boite,metal','botella-de-plastico','boite-carton',\n",
    "                                  'plastic-bottle','sous-vide','plastic,bottle','can','pot-en-plastique','glass','caja,carton','bote-de-vidrio',\n",
    "                                  'boite,conserve,metal','pet','becher','lata','bocal-verre','aluminium','caja-de-carton','bouteille,verre,capsule,metal',\n",
    "                                  'bokaal,glas','konserve','paper','glass-bottle','bocal,verre,couvercle,metal,conserve','glass-jar',\n",
    "                                  'glasflasche','flacon-verre','glas,flasche','kunststoff,karton','papier,aluminium','boite-de-conserve',\n",
    "                                  'pot-verre','canette,metal','carton,brique','flacon,verre','caja,carton,lata,en-conserva','bocal-verre,couvercle-metal','glass,bottle',\n",
    "                                  'conserve,boite,metal','carton,sachet','canette,aluminium','brique,carton','boite-en-carton','conserve,conserve'],'Recyclable'))\n",
    "\n",
    "pack_mapping.update(dict.fromkeys(['frais','surgele','surgele,carton,plastique','tetra-pak','tetra-brik','tetrapak',\n",
    "                                  'brique','tetrapack','surgele,carton','barquette,film,plastique,frais,sous-atmosphere-protectrice','sous-atmosphere-protectrice',\n",
    "                                  'carton,plastique,frais','frais,pot,plastique'],'Non-Recyclable'))\n",
    "\n",
    "\n",
    "data['emballage'] = data['packaging_tags'].replace(pack_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTRISCORE\n",
    "https://quoidansmonassiette.fr/comment-est-calcule-le-nutri-score-logo-nutritionnel/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import just enough field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/envs/ada/lib/python3.7/site-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "# replace N√†N of 'fruits-vegetables-nuts_100g' by 100 if it is 'Fruit juices'\n",
    "data[(data['main_category_en'] == 'Fruit juices')]['fruits-vegetables-nuts_100g'].fillna(100, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_enough_field =[\n",
    "    'Category_Food',\n",
    "    'energy_100g','sugars_100g','saturated-fat_100g','sodium_100g',\n",
    "    'fruits-vegetables-nuts_100g','fiber_100g','proteins_100g',\n",
    "    'nutrition-score-uk_100g','nutrition_grade_fr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide dataframe into 3 groups to simplify the calculation of nutriscrore\n",
    "data_beverages = data[data['Category_Food']=='Beverages'][selected_enough_field]\n",
    "data_fatsauces = data[data['Category_Food']=='Fat Sauces'][selected_enough_field]\n",
    "data_without_beverage_fat = data[(data['Category_Food']!='Fat Sauces') & (data['Category_Food']!='Beverages') ][selected_enough_field]\n",
    "                                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of negative points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ENERGY\n",
    "# energy bins\n",
    "energy_bins_without_beverage_fat = [data_without_beverage_fat['energy_100g'].min() - 1, 335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350, data_without_beverage_fat['energy_100g'].max()]\n",
    "energy_bins_fatsauces = [data_fatsauces['energy_100g'].min() - 1, 335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350, data_fatsauces['energy_100g'].max()]\n",
    "energy_bins_beverages = [data_beverages['energy_100g'].min() - 1, 0, 30, 60, 90, 120, 150, 180, 210, 240, 270, data_beverages['energy_100g'].max()]\n",
    "# energy point\n",
    "data_without_beverage_fat['energy_points'] = pd.cut(data_without_beverage_fat['energy_100g'], energy_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_fatsauces['energy_points'] = pd.cut(data_fatsauces['energy_100g'], energy_bins_fatsauces, labels=range(11)).astype(float)\n",
    "data_beverages['energy_points'] = pd.cut(data_beverages['energy_100g'], energy_bins_beverages, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SUGAR\n",
    "# sugar bins\n",
    "sugar_bins_without_beverage_fat = [data_without_beverage_fat['sugars_100g'].min() - 1, 0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5, 12, 13.5, data_without_beverage_fat['sugars_100g'].max()]\n",
    "sugar_bins_fatsauces = [data_fatsauces['sugars_100g'].min() - 1, 0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5, 12, 13.5, data_fatsauces['sugars_100g'].max()]\n",
    "sugar_bins_beverages = [data_beverages['sugars_100g'].min() - 1, 4.5, 9, 13.5, 18, 22.5, 27, 31, 36, 40, 45, data_beverages['sugars_100g'].max()]\n",
    "# sugar point (CHANGE SUGAR_BINS)\n",
    "data_without_beverage_fat['sugar_points'] = pd.cut(data_without_beverage_fat['sugars_100g'], sugar_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_fatsauces['sugar_points'] = pd.cut(data_fatsauces['sugars_100g'], sugar_bins_fatsauces, labels=range(11)).astype(float)\n",
    "data_beverages['sugar_points'] = pd.cut(data_beverages['sugars_100g'], sugar_bins_beverages, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SATURATED FAT\n",
    "# s-fat bins\n",
    "fat_bins_without_beverage_fat = [data_without_beverage_fat['saturated-fat_100g'].min() - 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, data_without_beverage_fat['saturated-fat_100g'].max()]\n",
    "fat_bins_beverages = [data_beverages['saturated-fat_100g'].min() - 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, data_beverages['saturated-fat_100g'].max()]\n",
    "fat_bins_fatsauces = [data_fatsauces['saturated-fat_100g'].min() - 1, 10, 16, 22, 28, 34, 40, 46, 52, 58, 64, data_fatsauces['saturated-fat_100g'].max()]\n",
    "# s-FAT point (CHANGE FAT_BINS)\n",
    "data_without_beverage_fat['saturated-fat_points'] = pd.cut(data_without_beverage_fat['saturated-fat_100g'], fat_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_beverages['saturated-fat_points'] = pd.cut(data_beverages['saturated-fat_100g'], fat_bins_beverages, labels=range(11)).astype(float)\n",
    "data_fatsauces['saturated-fat_points'] = pd.cut(data_fatsauces['saturated-fat_100g'], fat_bins_fatsauces, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SODIUM\n",
    "# sodium bins\n",
    "sodium_bins = [data['sodium_100g'].min() - 1, 90, 180, 270, 360, 450, 540, 630, 720, 810, 900, data['sodium_100g'].max()]\n",
    "# sodium points\n",
    "data_without_beverage_fat['sodium_points'] = pd.cut(data_without_beverage_fat['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "data_beverages['sodium_points'] = pd.cut(data_beverages['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "data_fatsauces['sodium_points'] = pd.cut(data_fatsauces['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation positive points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FRUITS\n",
    "# fruits bins\n",
    "fruits_bins_without_beverage_fat = [data_without_beverage_fat['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_without_beverage_fat['fruits-vegetables-nuts_100g'].max()]\n",
    "fruits_bins_fatsauces = [data_fatsauces['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_fatsauces['fruits-vegetables-nuts_100g'].max()]\n",
    "fruits_bins_beverages = [data_beverages['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_beverages['fruits-vegetables-nuts_100g'].max()]\n",
    "# fruits points\n",
    "data_without_beverage_fat['fruits_points'] = pd.cut(data_without_beverage_fat['fruits-vegetables-nuts_100g'], fruits_bins_without_beverage_fat, labels=[0,1,2,5]).astype(float)\n",
    "data_beverages['fruits_points'] = pd.cut(data_beverages['fruits-vegetables-nuts_100g'], fruits_bins_beverages, labels=[0,2,4,10]).astype(float)\n",
    "data_fatsauces['fruits_points'] = pd.cut(data_fatsauces['fruits-vegetables-nuts_100g'], fruits_bins_fatsauces, labels=[0,1,2,5]).astype(float)\n",
    "\n",
    "\n",
    "# FIBRES\n",
    "# fibers bins\n",
    "fibers_bins = [data['fiber_100g'].min() - 1, 0.7, 1.4, 2.1, 2.8, 3.5, data['fiber_100g'].max()]\n",
    "# fibers points\n",
    "data_without_beverage_fat['fiber_points'] = pd.cut(data_without_beverage_fat['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "data_beverages['fiber_points'] = pd.cut(data_beverages['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "data_fatsauces['fiber_points'] = pd.cut(data_fatsauces['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "\n",
    "# PROTEINS\n",
    "# proteins bins\n",
    "proteins_bins = [data['proteins_100g'].min() - 1, 1.6, 3.2, 4.8, 6.4, 8.0, data['proteins_100g'].max()]\n",
    "# proteins points\n",
    "data_without_beverage_fat['proteins_points'] = pd.cut(data_without_beverage_fat['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n",
    "data_beverages['proteins_points'] = pd.cut(data_beverages['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n",
    "data_fatsauces['proteins_points'] = pd.cut(data_fatsauces['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rejoin last separated dataframe\n",
    "frames = [data_without_beverage_fat, data_beverages, data_fatsauces]\n",
    "nutridata = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concession made to put 0 inplace of Nan of 'fruits_points','fiber_points','proteins_points' \n",
    "# this will not be a problem we do not substract Positive point\n",
    "nutridata['fruits_points'].fillna(0, inplace=True)\n",
    "nutridata['fiber_points'].fillna(0, inplace=True)\n",
    "nutridata['proteins_points'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculation of P x N\n",
    "nutridata['points_N']= nutridata['energy_points'] + nutridata['saturated-fat_points'] + nutridata['sugar_points'] + nutridata['sodium_points']\n",
    "nutridata['points_P'] = nutridata['fruits_points'] + nutridata['fiber_points'] + nutridata['proteins_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_score(row):\n",
    "    N = row['points_N']\n",
    "    P = row['points_P']\n",
    "    fruit = row['fruits_points']\n",
    "    fiber = row['fiber_points']\n",
    "    if N < 11 or fruit == 5:\n",
    "        return N - P\n",
    "    else:\n",
    "        return N - (fiber + fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutridata['nutri-score_calculated'] = nutridata.apply(compute_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#petite visualization\n",
    "nutridata['nutri-score_calculated'].hist(bins=int(nutridata['nutri-score_calculated'].max() - nutridata['nutri-score_calculated'].min() + 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re divided dataframe to allow a simpler transformation of score into letter\n",
    "nutridata_beverages = nutridata[nutridata['Category_Food']=='Beverages']\n",
    "nutridata_not_beverages = nutridata[nutridata['Category_Food']!='Beverages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matt/anaconda3/envs/ada/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/Matt/anaconda3/envs/ada/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# NUTRILETTER\n",
    "\n",
    "#letter bins\n",
    "letter_bins_aliments = [nutridata['nutri-score_calculated'].min() - 1, 0, 2.9, 10.9, 18.9, nutridata['nutri-score_calculated'].max()]\n",
    "letter_bins_beverages = [nutridata['nutri-score_calculated'].min() - 1, 2.5, 5.5, 9.5, nutridata['nutri-score_calculated'].max()]\n",
    "\n",
    "#letter\n",
    "nutridata_not_beverages['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_not_beverages['nutri-score_calculated'], letter_bins_aliments, labels=['a','b','c','d','e'])\n",
    "nutridata_beverages['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_beverages['nutri-score_calculated'], letter_bins_beverages, labels=['b','c','d','e'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutriframes = [nutridata_not_beverages, nutridata_beverages]\n",
    "nutridata = pd.concat(nutriframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d    247561\n",
       "a    160302\n",
       "c    156517\n",
       "e    118151\n",
       "b     87801\n",
       "Name: nutri-score_letter_CALCULATED, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutridata['nutri-score_letter_CALCULATED'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
