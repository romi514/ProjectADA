{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.util import hash_pandas_object\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Uncomment next line when u first use nltk and press download when all in selected on the windows of nltk downloads\n",
    "#nltk.download()\n",
    "\n",
    "import string\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "#To detect language for stemming\n",
    "# https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "#from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/en.openfoodfacts.org.products.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns not needed, explore missing values, and delete rows with zero or one value present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns2keep = ['energy_100g','sugars_100g','saturated-fat_100g','sodium_100g',\n",
    "    'fruits-vegetables-nuts_100g','fiber_100g','proteins_100g','nutrition-score-uk_100g',\n",
    "    'nutrition_grade_fr','main_category_en','packaging','categories','labels_en',\n",
    "    'pnns_groups_1','pnns_groups_2','product_name','ingredients_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = raw_data[columns2keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.replace(\"unknown\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.loc[(~data.isnull()).sum(axis=1)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOOD CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first step, we aim to categorize products according to their Food Category. The PNNS (\"Programme national nutrition santé\" in French) aims at categorizing food in several major classes: Dairies, Composite, Fish Meat Eggs, Beverages, Fat Sauces, Fruits Vegetables, Starchy, and Snacks.\n",
    "\n",
    "https://www.cerin.org/rapports/groupes-groupes-daliments/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pnns_groups_1 column\n",
    "\n",
    "We map the given pnns categories to our 8 major ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pnns_groups_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionnary to normalize category names into PNNS category\n",
    "pnns1_category = {\n",
    " 'Sugary snacks' : 'Snacks', \n",
    " 'Milk and dairy products' : 'Dairies',\n",
    " 'Cereals and potatoes' : 'Starchy', \n",
    " 'Fish Meat Eggs' : 'Fish Meat Eggs',\n",
    " 'Beverages' : 'Beverages',\n",
    " 'Fat and sauces' : 'Fat Sauces',\n",
    " 'Fruits and vegetables' : 'Fruits Vegetables',\n",
    " 'Salty snacks' : 'Snacks',\n",
    " 'fruits-and-vegetables' : 'Fruits Vegetables',\n",
    " 'sugary-snacks' : 'Snacks',\n",
    " 'cereals-and-potatoes' : 'Starchy',\n",
    " 'salty-snacks' : 'Snacks'\n",
    "}\n",
    "data.loc[:,'food_category'] = data['pnns_groups_1'].map(pnns1_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New pnns categories\n",
    "data[\"food_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pnns_groups_2 column\n",
    "\n",
    "We notice that several products haven't been categorized but have a non missing value in pnns_groups_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of pnns_groups_2 which haven't been classified in food_category\n",
    "data.loc[data.food_category.isnull()][\"pnns_groups_2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionnary to extend food_categories from pnns_groups_2\n",
    "#pnns2_category = {\n",
    "# 'Pizza pies and quiches' : 'Composite', \n",
    "# 'Alcoholic beverages' : 'Beverages',\n",
    "#}\n",
    "#data.loc[data.food_category.isnull(),'food_category'] = data.loc[data.food_category.isnull()]['pnns_groups_2'].map(pnns2_category)\n",
    "\n",
    "data.loc[(data.food_category.isnull())&(data.pnns_groups_2 == \"Alcoholic beverages\"),\"food_category\"] = \"Beverages\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"food_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From main_category_en column\n",
    "\n",
    "We complete our categorization with the help of the 'main_category_en' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.food_category.isnull()][\"main_category_en\"].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After inspecting the food type for the 30 most frequent values in main_category_en, we create the following dictionary\n",
    "maincategoryen_category = {\n",
    "'Beverages' : 'Beverages',\n",
    "'Snacks' : 'Snacks',\n",
    "'Dairies' : 'Dairies',\n",
    "'Desserts' : 'Dairies',\n",
    "'Crêpes and galettes' : 'Snacks',\n",
    "'Cocoa and chocolate powders': 'Snacks',\n",
    "'Syrups':'Fat Sauces',\n",
    "'Chips and fries': 'Snacks',\n",
    "'es:bolleria-industrial': 'Snacks',\n",
    "'fr:bloc-de-foie-gras-de-canard': 'Fish Meat Eggs' ,\n",
    "'Pizza dough' : 'Starchy',\n",
    "'Breakfast' : 'Starchy',\n",
    "'Banana-crisps' : \"Snacks\",\n",
    "'Fish eggs' : 'Fish Meat Eggs',\n",
    "'Terrines' : 'Fish Meat Eggs',\n",
    "'fr:escalopes' : 'Fish Meat Eggs',\n",
    "'Salads' : 'Fruits Vegetables',\n",
    "'fr:pilons-de-poulet' : 'Fish Meat Eggs'\n",
    "}\n",
    "\n",
    "data.loc[data.food_category.isnull(),'food_category'] =  data.loc[data.food_category.isnull(),'main_category_en'].map(maincategoryen_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data[\"food_category\"].value_counts())/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After these steps, we managed to categorize 37% of our date among PNNS groups.**\n",
    "\n",
    "For the unclassified data, the other columns don't bring any strong evidence for the categorization of the product. For instance, a lot of Dietary Supplements or Non Food Products are unclassified. Also, the table beneath shows that no column with above 10% completeness for uncategorized data gives us direct information on food categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness = raw_data.iloc[data[data.food_category.isnull()].index].count().sort_values(ascending=False)/len(data.loc[data.food_category.isnull()])\n",
    "completeness.loc[completeness>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_data = data.loc[~data.food_category.isnull()]\n",
    "unclassified_data = data.loc[data.food_category.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_data.loc[~classified_data.ingredients_text.isnull(),\"ingredients_text\"].count()/len(classified_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40% of the remaining data has a list of ingredients, and 62% of our classified data also has a list of ingredients. We will use word similarities on the words in ingredient_text to associate unclassified products to categories if the similarity is big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ingredients_text column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean the list of ingredients by removing punctuation and numbers\n",
    "\n",
    "def clean_text(df,col):\n",
    "    # We clean the list of ingredients by removing punctuation and numbers\n",
    "def clean_text(df,col):\n",
    "    \n",
    "    caracters2delete = ['%','-',':',',',\"  \",'(',')',';','/','_','*','\\d+','.','\\'','[',']','•','+']\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].str.lower()\n",
    "    for c in caracters2delete:\n",
    "        df.loc[:,col] = df.loc[:,col].str.replace(c,' ')\n",
    "\n",
    "    \n",
    "\n",
    "clean_text(classified_data,\"ingredients_text\")\n",
    "clean_text(unclassified_data,\"ingredients_text\")\n",
    "\n",
    "# We then transform the text to a list of nonStop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# The dominant language in the ingredients in french\n",
    "stopWords = stopwords.words('french') + [\"kg\",\"g\",\"v\",\"a\",\"b\",\"k\",\"e\",\"mg\",\"and\",\"of\",\"ml\",\"cl\"]\n",
    "\n",
    "classified_data.loc[~classified_data.ingredients_text.isnull(),'ingredients_text'] = \\\n",
    "    classified_data.loc[~classified_data.ingredients_text.isnull(),'ingredients_text'] \\\n",
    "    .apply(lambda x: [word for word in x.split() if word not in stopWords])\n",
    "    \n",
    "unclassified_data.loc[~unclassified_data.ingredients_text.isnull(),'ingredients_text'] = \\\n",
    "    unclassified_data.loc[~unclassified_data.ingredients_text.isnull(),'ingredients_text'] \\\n",
    "    .apply(lambda x: [word for word in x.split() if word not in stopWords])\n",
    "    \n",
    "\n",
    "    caracters2delete = ['%','-',':',',',\"  \",'(',')',';','/','_','*','\\d+','.','\\'','[',']']\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].str.lower()\n",
    "    for c in caracters2delete:\n",
    "        df.loc[:,col] = df.loc[:,col].str.replace(c,' ')\n",
    "\n",
    "    \n",
    "\n",
    "clean_text(classified_data,\"ingredients_text\")\n",
    "clean_text(unclassified_data,\"ingredients_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the set of words for each category by keeping the nFrequent most frequent words that aren't stopWords\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nFrequent = 200\n",
    "maxWords = 10\n",
    "\n",
    "def createWordSet(df):   \n",
    "    count = Counter([ word for sentence in df.loc[~df[\"ingredients_text\"].isnull(),\"ingredients_text\"].tolist() for word in sentence[:maxWords]])\n",
    "    count[\"\"] = 0\n",
    "    n = sum(count.values())\n",
    "    return dict([(word[0],word[1]/n) for word in count.most_common(nFrequent)])\n",
    "\n",
    "category_words = classified_data.groupby(\"food_category\").apply(createWordSet)\n",
    "#category_words[\"Beverages\"]\n",
    "\n",
    "\n",
    "ingredientDict = dict();\n",
    "for col in category_words.index.values:\n",
    "    for word in set(category_words[col]):\n",
    "        ingredientDict[word] = ingredientDict.get(word,0)+1\n",
    "\n",
    "ingredientDictList = list(ingredientDict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example shown above is the word dictionary for 'Beverages'. For each category we obtain a dictionary of the 100 most frequent words with normalized counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = category_words.index.values\n",
    "n = len(columns)\n",
    "similarity_matrix = np.zeros((n,n))\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1) # gives the keys\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "for idx1,name1 in enumerate(columns):\n",
    "    for idx2,name2 in enumerate(columns):\n",
    "        similarity_matrix[idx1,idx2] = jaccard_similarity(category_words[name1],category_words[name2])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(similarity_matrix, annot=True, cbar_kws={'label': 'jaccard similarity score'})\n",
    "plt.title(\"Jaccard similarity scores among categories\", fontsize=15, fontweight=\"bold\")\n",
    "plt.xticks(np.arange(n)+0.5,columns,rotation='vertical')\n",
    "plt.yticks(np.arange(n)+0.5,np.flip(columns,axis=0),rotation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dictionaries are rather distinct. The 'Composite' category shares words with other categories since it groups products from different food categories. We decide to drop 'Composite' for further categorization as it is too subjective to associate multiple categories to products with only the list of ingredients. We obtain more accurate results by associating only one category to products if the similarity is above an experimentally-set threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the similarity between a list of ingredients and a category, we compute a score. Each word in the list adds to the category score its importance in the category dictionary (normalized count in the dictionary) which is also weighted by its position in the ingredients list. The idea the ingredients of a product are listed in order of importance. We only consider the 10 first words in the ingredient list. We also divide the word score by the number of dictionaries it which it occurs. The ingredient is less representative of a food category if it also represents other categories.\n",
    "\n",
    "We then categorize a product if its maximum similarity is above a 0.3 threshold. This threshold was set experimentally by manually looking up a product's information with its computed category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using similarities to category dictionaries\n",
    "\n",
    "def similarity_dic(ingredients):\n",
    "    dic = {}\n",
    "    n = 10        \n",
    "    for cat in category_words.index.values:\n",
    "        score = 0\n",
    "        for idx,word in enumerate(ingredients[:n]):\n",
    "            score += category_words[cat].get(word,0)*(n-idx)/ingredientDict.get(word,1)\n",
    "        dic[cat] = score\n",
    "    total = sum(dic.values())\n",
    "    if (total != 0) :\n",
    "        dic = {k: v / total for k, v in dic.items()}\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0,1,0.01)\n",
    "accuracies = []\n",
    "percKept = []\n",
    "\n",
    "test_similarities = classified_data.loc[~classified_data.ingredients_text.isnull(),\"ingredients_text\"] \\\n",
    "            .apply(similarity_dic).apply(pd.Series)\n",
    "\n",
    "test_similarities = test_similarities[(test_similarities.T != 0).any()]\n",
    "\n",
    "for t in thresholds:\n",
    "    results = test_similarities.loc[test_similarities.max(axis=1)>t].idxmax(axis=1)\n",
    "    accuracies.append(sum(classified_data.loc[results.index,\"food_category\"] == results)/len(results))\n",
    "    percKept.append(len(results)/len(test_similarities))\n",
    "\n",
    "plt.plot(percKept,accuracies)\n",
    "\n",
    "plt.title(\"Category Prediction Accuracy vs Percentage of Data with Prediction\")\n",
    "plt.xlabel(\"Percentage of data categorized\")\n",
    "plt.ylabel(\"Prediction accuracy\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_vector(l,dic_list):\n",
    "    \n",
    "    vec = np.zeros(len(dic_list))\n",
    "    counter = 1\n",
    "    for elem in l[::-1]:\n",
    "        if elem in dic_list:\n",
    "            vec[dic_list.index(elem)]+=counter\n",
    "            counter+= 1\n",
    "\n",
    "    if counter != 1:\n",
    "        vec = vec/np.linalg.norm(vec)\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_X = classified_data.loc[~classified_data.ingredients_text.isnull(),'ingredients_text'].apply(lambda x : embedding_vector(x,ingredientDictList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_Y = classified_data.loc[~classified_data.ingredients_text.isnull(),\"food_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_conv = {\"Fruits Vegetables\":0,\"Starchy\":1,\"Fat Sauces\":2,\"Snacks\":3,\"Dairies\":4,\"Beverages\":5,\"Fish Meat Eggs\":6}\n",
    "training_Y = training_Y.replace(cat_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_X = np.stack(training_X.values)\n",
    "training_Y = np.array(training_Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonZeroRowsIndexes = np.where(training_X.any(axis=1))[0]\n",
    "print(\"Number of kept rows : {}, {}% of training data\".format(len(nonZeroRowsIndexes),100*len(nonZeroRowsIndexes)/training_X.shape[0]))\n",
    "training_X = np.take(training_X,nonZeroRowsIndexes,axis=0)\n",
    "training_Y = np.take(training_Y,nonZeroRowsIndexes,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_X, training_Y, test_size=0.33, random_state=2)\n",
    "print(\"Number of training samples : {}\".format(X_train.shape[0]))\n",
    "print(\"Number of testing samples : {}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0,n_estimators=10,criterion=\"entropy\")\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,clf.predict(X_test),normalize='true')\n",
    "n = len(cat_conv)\n",
    "columns = list(cat_conv)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, cbar_kws={'label': 'accuracy score'})\n",
    "plt.title(\"Normalized Confusion Matrix\", fontsize=15, fontweight=\"bold\")\n",
    "plt.xticks(np.arange(n)+0.5,columns,rotation='vertical')\n",
    "plt.yticks(np.arange(n)+0.5,np.flip(columns,axis=0),rotation='horizontal')\n",
    "plt.ylabel(\"Target\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0,1.01,0.01)\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(training_X, training_Y, test_size=0.33, random_state=i)\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0,n_estimators=10,criterion=\"entropy\")\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    y_preds = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)\n",
    "    y_probs_max = np.max(y_probs,axis=1)\n",
    "\n",
    "    n = len(y_test)\n",
    "\n",
    "    accuracies = []\n",
    "    percKept = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_test_kept = np.take(y_test,np.where(y_probs_max>=t)[0])\n",
    "        y_preds_kept = np.take(y_preds,np.where(y_probs_max>=t)[0])\n",
    "    \n",
    "        percKept.append(len(y_test_kept)/n)\n",
    "        accuracies.append(accuracy_score(y_test_kept,y_preds_kept))\n",
    "\n",
    "    plt.plot(percKept,accuracies)\n",
    "\n",
    "plt.title(\"Category Prediction Accuracy vs Percentage of Data with Prediction for Different Train/Test Splits\")\n",
    "plt.xlabel(\"Percentage of data categorized\")\n",
    "plt.ylabel(\"Prediction accuracy\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wantedAccuracy = 0.98\n",
    "bestThreshold = thresholds[np.where(np.asarray(accuracies)>wantedAccuracy)[0][0]]\n",
    "expectedCatPercentage = percKept[np.where(np.asarray(accuracies)>wantedAccuracy)[0][0]]\n",
    "\n",
    "print(\"By choosing a threshold of {0:.2f} we categorized {1:.1f}% of the testing set with {2:}% accuracy.\" \\\n",
    "      .format(bestThreshold,expectedCatPercentage*100,wantedAccuracy*100))\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0,n_estimators=10,criterion=\"entropy\")\n",
    "clf.fit(training_X,training_Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unclassifiedWithIngred = unclassified_data.loc[~unclassified_data.ingredients_text.isnull(),'ingredients_text'].apply(lambda x : embedding_vector(x,ingredientDictList))\n",
    "unclassifiedWithIngred = np.stack(unclassifiedWithIngred.values)\n",
    "\n",
    "nonZeroRowsIndexes = np.where(unclassifiedWithIngred.any(axis=1))[0]\n",
    "print(\"Number of kept rows : {}, {}% of unclassified data with ingredients_text\".format(len(nonZeroRowsIndexes),100*len(nonZeroRowsIndexes)/unclassifiedWithIngred.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataToPredict = np.take(unclassifiedWithIngred,nonZeroRowsIndexes,axis=0)\n",
    "preds = clf.predict(dataToPredict)\n",
    "probs = clf.predict_proba(dataToPredict)\n",
    "probs_max = np.max(probs,axis=1)\n",
    "\n",
    "y_preds_kept = np.take(preds,np.where(probs_max>=bestThreshold)[0])\n",
    "    \n",
    "print(\"We classifed {}% of the unclassified data with ingredients_text\".format(100*len(y_preds_kept)/unclassifiedWithIngred.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACKAGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this field remains incomplete, it is still useful for us to analyze the packaging methods of different food categories. The processing pipeline for this field was performed as follows :\n",
    "- clean 'Packaging' column of the dataframe by removing stop words and unecessary entries\n",
    "- retain only certain material types of packaging (plastic, glass, cardboard, metal ...)\n",
    "- plot the proportion of each material within a certain category of food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We build our list of stopwords to remove when tockenizing the packaging column\n",
    "stop_words_c = set(stopwords.words(\"french\")).union(set(stopwords.words(\"english\")))\n",
    "\n",
    "# 'can' was removed since it was in the stop words list but still useful for Beverages containers\n",
    "stop_words_c.remove('can')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean the packaging column\n",
    "def clean_data_packaging(data_column):\n",
    "    \"\"\"Clean data packaging, i.e. \n",
    "        - lower each words in the cells of data_column\n",
    "        - tokenize cells of data_column, i.e. from float type create list of string (token)\n",
    "        - remove stopwords for the list of tokens for each cells of data_column\n",
    "    \"\"\"  \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_column = data_column[data_column.notnull()].str.lower() \\\n",
    "    .apply(str) \\\n",
    "    .apply(tokenizer.tokenize) \\\n",
    "    .apply(lambda cell : [item for item in cell if item not in stop_words_c]) \\\n",
    "    .apply(str)\n",
    "    \n",
    "    return data_column\n",
    "\n",
    "\n",
    "packaging = clean_data_packaging(data.packaging)\n",
    "packaging = packaging.apply(literal_eval)\n",
    "filled_pck = data.packaging.count()/len(data.packaging)\n",
    "\n",
    "# We visualize some results\n",
    "print(f'Packaging field is full at {filled_pck} %')\n",
    "packaging.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20% of the data has packaging information**\n",
    "We now categorize the packaging information to defined packaging classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaccum items were added since it surely contains plastic\n",
    "plastique_words = ['plastic', 'plastique', 'plastik', 'plastico', 'plastica', 'sachets', \n",
    "                   'plàstic', 'plástico', 'plastisco' ,'bucket', 'barquette', 'sachet', 'tube', 'film', 'vacio', 'vide', 'vacío']\n",
    "\n",
    "verre_words = ['verre', 'glass', 'glas', 'vidrio', 'pot']\n",
    "carton_words = ['paper', 'karton', 'carton', 'papier', 'boite', 'cartón', 'papel', 'cardboard', \n",
    "                'card', 'eggbox', 'box']\n",
    "\n",
    "metal_words = ['metal', 'métal', 'fer', 'acier', 'aluminium', 'can', 'canette', 'conserve','tin']\n",
    "\n",
    "# Fresh products needs an adapted container which is useful for further exploration\n",
    "prod_frais_words = ['frais', 'fresh', 'frai']\n",
    "\n",
    "# The recycable materials depend on the country and regulations. However, these materials are commonly recycable.\n",
    "# (soiled papers, soiled metalic containers and soiled plastic containers tend to be avoided for medical reasons)\n",
    "recyclable_words = ['bouteille', 'flacon', 'brique', 'brick', 'bottle', 'bte', 'verre', \n",
    "                    'glass', 'glas', 'vidrio', 'metal', 'métal', 'fer', 'acier', 'aluminium', 'flasche', 'canned', \n",
    "                    'can', 'canette', 'conserve', 'tin', 'pet', 'pot', 'botella']\n",
    "\n",
    "def count_entries(self, word_list):\n",
    "    \"\"\" Return true if word_list and self have a word in common,\n",
    "        else o/w.\n",
    "    \"\"\"\n",
    "    if len(set(self).intersection(set(word_list))) > 0:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def to_materials(elem):\n",
    "    \"\"\" Return a list of bool that informs about the materials used \n",
    "        for the packaging.\n",
    "    \"\"\"\n",
    "    return [count_entries(elem, plastique_words), \n",
    "               count_entries(elem, verre_words), \n",
    "               count_entries(elem, carton_words),\n",
    "               count_entries(elem, metal_words),\n",
    "               count_entries(elem, prod_frais_words),\n",
    "               count_entries(elem, recyclable_words)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We a create a boolean list for each value representing the membership to packaging methods and fresh/recycable attribute\n",
    "materials_df = pd.DataFrame({'Packaging': packaging, 'materials':pd.Series(np.zeros(len(packaging)), packaging.index)})\n",
    "materials_df['materials'] = packaging.apply(to_materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a dataframe with the information\n",
    "names_list = ['Plastic', 'Glass', 'Carton', 'Metal', 'Fresh', 'Recyclable']\n",
    "materials_df = materials_df.materials.apply(pd.Series)\n",
    "materials_df = materials_df.rename(columns = lambda x : names_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We merge with the food categories\n",
    "eco_impact_plot_df = materials_df.merge(data.loc[materials_df.index.tolist(), [\"food_category\"]], left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove Fresh and Recycable categories and plot normalized bar plots for the packaging for each food category\n",
    "packaging_plot_df = eco_impact_plot_df.drop('Fresh', axis =1)\n",
    "packaging_plot_df = packaging_plot_df.drop('Recyclable', axis =1)\n",
    "counts_pack = packaging_plot_df.groupby('food_category').sum()\n",
    "\n",
    "plt.figure(figsize =(10,10))\n",
    "ax=plt.gca()\n",
    "counts_pack.div(counts_pack.sum(axis=1)/100, axis = 0).plot.bar(ax=ax, stacked=True)\n",
    "plt.title('Material Use Percentage within each Category',fontsize=20);\n",
    "plt.xlabel(\"Food Category\");\n",
    "plt.ylabel(\"Packaging Type Percentages\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the sample size is limited, a few useful informations are depicted on the above plot. First we can see that a lot of plastic is used among every category. Moreover, glass is mainly used as a liquid container which is consistent with what we could expect. The packaging information along with the use of cooling containers and recyclability of products will be used in the analysis of different diets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIET from labels_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regimes = ['isVegetarian', 'isVegetalian', 'isGlutenfree', 'isLactosefree', 'isKetogenic', 'isOrganic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_caract(df,col):\n",
    "    \n",
    "    caracters2delete = ['%',':','(',')',';','/','_','*','\\d+','.','\\'','[',']']\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].str.lower()\n",
    "    for c in caracters2delete:\n",
    "        df.loc[:,col] = df.loc[:,col].str.replace(c,' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"split_labels_en\"] = data[\"labels_en\"]\n",
    "clean_caract(data,\"split_labels_en\")\n",
    "data.loc[:,\"split_labels_en\"] = data.loc[:,\"split_labels_en\"].str.split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_vegetarian_list = ['vegetarian']\n",
    "not_vegetarian_list = ['meat']\n",
    "data[\"isVGT\"] = data.loc[~data[\"split_labels_en\"].isnull(),\"split_labels_en\"] \\\n",
    ".apply(lambda l : bool(~any([cat_word in not_vegetarian_list for cat_word in l]) &  \\\n",
    "                       any([cat_word in is_vegetarian_list for cat_word in l])))\n",
    "\n",
    "is_vegetalian_list = ['vegan']\n",
    "not_vegetalian_list = ['meat']\n",
    "data[\"isVGL\"] = data.loc[~data[\"split_labels_en\"].isnull(),\"split_labels_en\"] \\\n",
    ".apply(lambda l : bool(~any([cat_word in not_vegetalian_list for cat_word in l]) &  \\\n",
    "                       any([cat_word in is_vegetalian_list for cat_word in l])))\n",
    "\n",
    "is_glutenfree_list = ['gluten-free']\n",
    "not_glutenfree_list = ['mais']\n",
    "data[\"isGF\"] = data.loc[~data[\"split_labels_en\"].isnull(),\"split_labels_en\"] \\\n",
    ".apply(lambda l : bool(~any([cat_word in not_glutenfree_list for cat_word in l]) &  \\\n",
    "                       any([cat_word in is_glutenfree_list for cat_word in l])))\n",
    "\n",
    "is_lactosefree_list = ['no lactose']\n",
    "not_lactosefree_list = ['lait']\n",
    "data[\"isLF\"] = data.loc[~data[\"split_labels_en\"].isnull(),\"split_labels_en\"] \\\n",
    ".apply(lambda l : bool(~any([cat_word in not_lactosefree_list for cat_word in l]) &  \\\n",
    "                       any([cat_word in is_lactosefree_list for cat_word in l])))\n",
    "\n",
    "is_organic_list = ['organic','eu organic','fr ab-agriculture-biologique']\n",
    "not_organic_list = ['non organic']\n",
    "data[\"isOG\"] = data.loc[~data[\"split_labels_en\"].isnull(),\"split_labels_en\"] \\\n",
    ".apply(lambda l : bool(~any([cat_word in not_organic_list for cat_word in l]) &  \\\n",
    "                       any([cat_word in is_organic_list for cat_word in l])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data[\"isVGT\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data[\"isVGL\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data[\"isGF\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data[\"isLF\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data[\"isOG\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost all vegetalian are in vegetarian\n",
    "len(data[(data[\"isVGL\"]==True) & (data[\"isVGT\"]==True)])/len(data[(data[\"isVGT\"]==True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "diets_data = data[['food_category', 'isVGT', 'isVGL', 'isGF', 'isLF', 'isOG']]\n",
    "diets_plot = diets_data.groupby('food_category').sum()\n",
    "\n",
    "# Create a pieplot\n",
    "fig, axes = plt.subplots(1,5, figsize=(30,20))\n",
    "for ax, col in zip(axes, diets_data.columns[1:]):\n",
    "    ax.pie(diets_plot[col], wedgeprops=dict(width=.3))\n",
    "    ax.set(ylabel='', title=col, aspect='equal')\n",
    "plt.legend(diets_plot.index, loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diets_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet from ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "carnivore_list = [\n",
    "    'animal','animals',\n",
    "    #viande\n",
    "    'viande','viandes','meat','fleisch','carne',\n",
    "    'bœuf','boeuf','boeufs','beef','steak','vache','cow', 'bovin','bovine',\n",
    "    'charcut', 'charcuteries', 'charcuterie',\n",
    "    'poulet','poulets','poule','poules','chicken','volaille','volailles',\n",
    "    'porc','pork','jambons', 'jambon', 'jam','cerdo','lardon','lardons','bacon','schweinefleisch',\n",
    "    'cannard','canard',\n",
    "    'dinde','dindes', 'turkey',\n",
    "    'boudin','foie','rillette','rillettes','couenne',\n",
    "    'poitrine','cuisse','cuisses'\n",
    "    'chipolata','chipolatas',\n",
    "    'brebis', 'mouton'\n",
    "    #poisson\n",
    "    'poisson','fish', 'fisch',\n",
    "    'sardine','sardines','sardina',\n",
    "    'thon','thuna','tuna',\n",
    "    'saumon','salmon', 'salmo'\n",
    "    'hareng', 'seafood',\n",
    "    'maquereaux','cabillaud','morue',\n",
    "    'crevettes', 'shrimp',\n",
    "    'albacore','colin','truite','moules'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words which reference water\n",
    "#carnivore_list = [\"viande\"]\n",
    "\n",
    "# Some 'categories' values contain words refering to water but are liquors\n",
    "# We thus create a list to discard such products\n",
    "soja_list = [\"soja\"]\n",
    "\n",
    "# We clean the 'categories' column and split into words\n",
    "data[\"split_ingredients_text\"] = data[\"ingredients_text\"]\n",
    "clean_text(data,\"ingredients_text\")\n",
    "data.loc[:,\"split_ingredients_text\"] = data.loc[:,\"split_ingredients_text\"].str.split()\n",
    "#\n",
    "#data[\"iscarnivore\"] = data.loc[~data[\"split_ingredients_text\"].isnull(),\"split_ingredients_text\"] \\\n",
    "#.apply(lambda l : bool(~any([cat_word in soja_list for cat_word in l]) &  \\\n",
    "#                       any([cat_word in carnivore_list for cat_word in l])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"iscarnivore\"] = data.loc[~data[\"split_ingredients_text\"].isnull(),\"split_ingredients_text\"] \\\n",
    ".apply(lambda l : bool(any([cat_word in carnivore_list for cat_word in l])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"iscarnivore\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTRISCORE\n",
    "https://quoidansmonassiette.fr/comment-est-calcule-le-nutri-score-logo-nutritionnel/\n",
    "\n",
    "We calculate the nutriscore according to the method described in the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Tag\n",
    "In order to compute the NutriScore (NS) of products, we need to seperate water from other beverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words which reference water\n",
    "water_list = [\"water\",\"waters\",\"eau\",\"agua\"]\n",
    "\n",
    "# Some 'categories' values contain words refering to water but are liquors\n",
    "# We thus create a list to discard such products\n",
    "nonwater_list = [\"alkoholische\",\"alcoholic\",\"spirits\",\"liquors\",\"tonic\",'coconut','sodas','soda']\n",
    "\n",
    "# We clean the 'categories' column and split into words\n",
    "data[\"split_categories\"] = data[\"categories\"]\n",
    "clean_text(data,\"split_categories\")\n",
    "data.loc[:,\"split_categories\"] = data.loc[:,\"split_categories\"].str.split()\n",
    "\n",
    "data[\"iswater\"] = data.loc[~data[\"split_categories\"].isnull(),\"split_categories\"] \\\n",
    ".apply(lambda l : bool(~any([cat_word in nonwater_list for cat_word in l]) &  \\\n",
    "                       any([cat_word in water_list for cat_word in l])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"iswater\"]==True][['product_name','split_categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"iswater\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN of 'fruits-vegetables-nuts_100g' by 100 if it is 'Fruit juices'\n",
    "data[(data['main_category_en'] == 'Fruit juices')]['fruits-vegetables-nuts_100g'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_fields =[\n",
    "    'product_name','food_category',\n",
    "    'energy_100g','sugars_100g','saturated-fat_100g','sodium_100g',\n",
    "    'fruits-vegetables-nuts_100g',\n",
    "    'fiber_100g','proteins_100g',\n",
    "    'nutrition-score-uk_100g',\n",
    "    'nutrition_grade_fr',\n",
    "    'iswater']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide dataframe into 3 groups to simplify the calculation of nutriscrore\n",
    "data_beverages = data[data['food_category']=='Beverages'][selected_fields]\n",
    "data_fatsauces = data[data['food_category']=='Fat Sauces'][selected_fields]\n",
    "data_without_beverage_fat = data[(data['food_category']!='Fat Sauces') & (data['food_category']!='Beverages') ][selected_fields]                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of negative points\n",
    "\n",
    "We attribute Negative points (N) to different nutritional factors that have to be limited: energy, sugar, saturated fat and sodium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ENERGY\n",
    "# energy bins\n",
    "energy_bins_without_beverage_fat = [data_without_beverage_fat['energy_100g'].min() - 1, 335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350, data_without_beverage_fat['energy_100g'].max()]\n",
    "energy_bins_fatsauces = [data_fatsauces['energy_100g'].min() - 1, 335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350, data_fatsauces['energy_100g'].max()]\n",
    "energy_bins_beverages = [data_beverages['energy_100g'].min() - 1, 0, 30, 60, 90, 120, 150, 180, 210, 240, 270, data_beverages['energy_100g'].max()]\n",
    "# energy point\n",
    "data_without_beverage_fat['energy_points'] = pd.cut(data_without_beverage_fat['energy_100g'], energy_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_fatsauces['energy_points'] = pd.cut(data_fatsauces['energy_100g'], energy_bins_fatsauces, labels=range(11)).astype(float)\n",
    "data_beverages['energy_points'] = pd.cut(data_beverages['energy_100g'], energy_bins_beverages, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SUGAR\n",
    "# sugar bins\n",
    "sugar_bins_without_beverage_fat = [data_without_beverage_fat['sugars_100g'].min() - 1, 0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5, 12, 13.5, data_without_beverage_fat['sugars_100g'].max()]\n",
    "sugar_bins_fatsauces = [data_fatsauces['sugars_100g'].min() - 1, 0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5, 12, 13.5, data_fatsauces['sugars_100g'].max()]\n",
    "sugar_bins_beverages = [data_beverages['sugars_100g'].min() - 1, 4.5, 9, 13.5, 18, 22.5, 27, 31, 36, 40, 45, data_beverages['sugars_100g'].max()]\n",
    "# sugar point (CHANGE SUGAR_BINS)\n",
    "data_without_beverage_fat['sugar_points'] = pd.cut(data_without_beverage_fat['sugars_100g'], sugar_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_fatsauces['sugar_points'] = pd.cut(data_fatsauces['sugars_100g'], sugar_bins_fatsauces, labels=range(11)).astype(float)\n",
    "data_beverages['sugar_points'] = pd.cut(data_beverages['sugars_100g'], sugar_bins_beverages, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SATURATED FAT\n",
    "# s-fat bins\n",
    "fat_bins_without_beverage_fat = [data_without_beverage_fat['saturated-fat_100g'].min() - 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, data_without_beverage_fat['saturated-fat_100g'].max()]\n",
    "fat_bins_beverages = [data_beverages['saturated-fat_100g'].min() - 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, data_beverages['saturated-fat_100g'].max()]\n",
    "fat_bins_fatsauces = [data_fatsauces['saturated-fat_100g'].min() - 1, 10, 16, 22, 28, 34, 40, 46, 52, 58, 64, data_fatsauces['saturated-fat_100g'].max()]\n",
    "# s-FAT point (CHANGE FAT_BINS)\n",
    "data_without_beverage_fat['saturated-fat_points'] = pd.cut(data_without_beverage_fat['saturated-fat_100g'], fat_bins_without_beverage_fat, labels=range(11)).astype(float)\n",
    "data_beverages['saturated-fat_points'] = pd.cut(data_beverages['saturated-fat_100g'], fat_bins_beverages, labels=range(11)).astype(float)\n",
    "data_fatsauces['saturated-fat_points'] = pd.cut(data_fatsauces['saturated-fat_100g'], fat_bins_fatsauces, labels=range(11)).astype(float)\n",
    "\n",
    "\n",
    "## SODIUM\n",
    "# sodium bins\n",
    "sodium_bins = [data['sodium_100g'].min() - 1, 90, 180, 270, 360, 450, 540, 630, 720, 810, 900, data['sodium_100g'].max()]\n",
    "# sodium points\n",
    "data_without_beverage_fat['sodium_points'] = pd.cut(data_without_beverage_fat['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "data_beverages['sodium_points'] = pd.cut(data_beverages['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "data_fatsauces['sodium_points'] = pd.cut(data_fatsauces['sodium_100g'], sodium_bins, labels=range(11)).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation positive points\n",
    "\n",
    "We attribute Positive points (P) to different nutritional factors that have to be promoted: fruit vegetable nuts pourcentage, fibers and proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FRUITS\n",
    "# fruits bins\n",
    "fruits_bins_without_beverage_fat = [data_without_beverage_fat['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_without_beverage_fat['fruits-vegetables-nuts_100g'].max()]\n",
    "fruits_bins_fatsauces = [data_fatsauces['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_fatsauces['fruits-vegetables-nuts_100g'].max()]\n",
    "fruits_bins_beverages = [data_beverages['fruits-vegetables-nuts_100g'].min() - 1, 40, 60, 80, data_beverages['fruits-vegetables-nuts_100g'].max()]\n",
    "# fruits points\n",
    "data_without_beverage_fat['fruits_points'] = pd.cut(data_without_beverage_fat['fruits-vegetables-nuts_100g'], fruits_bins_without_beverage_fat, labels=[0,1,2,5]).astype(float)\n",
    "data_beverages['fruits_points'] = pd.cut(data_beverages['fruits-vegetables-nuts_100g'], fruits_bins_beverages, labels=[0,2,4,10]).astype(float)\n",
    "data_fatsauces['fruits_points'] = pd.cut(data_fatsauces['fruits-vegetables-nuts_100g'], fruits_bins_fatsauces, labels=[0,1,2,5]).astype(float)\n",
    "\n",
    "\n",
    "# FIBRES\n",
    "# fibers bins\n",
    "fibers_bins = [data['fiber_100g'].min() - 1, 0.7, 1.4, 2.1, 2.8, 3.5, data['fiber_100g'].max()]\n",
    "# fibers points\n",
    "data_without_beverage_fat['fiber_points'] = pd.cut(data_without_beverage_fat['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "data_beverages['fiber_points'] = pd.cut(data_beverages['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "data_fatsauces['fiber_points'] = pd.cut(data_fatsauces['fiber_100g'], fibers_bins, labels=range(6)).astype(float)\n",
    "\n",
    "# PROTEINS\n",
    "# proteins bins\n",
    "proteins_bins = [data['proteins_100g'].min() - 1, 1.6, 3.2, 4.8, 6.4, 8.0, data['proteins_100g'].max()]\n",
    "# proteins points\n",
    "data_without_beverage_fat['proteins_points'] = pd.cut(data_without_beverage_fat['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n",
    "data_beverages['proteins_points'] = pd.cut(data_beverages['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n",
    "data_fatsauces['proteins_points'] = pd.cut(data_fatsauces['proteins_100g'], proteins_bins, labels=range(6)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rejoin last separated dataframe\n",
    "frames = [data_without_beverage_fat, data_beverages, data_fatsauces]\n",
    "nutridata = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutriscore calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concession made to put 0 inplace of Nan of 'fruits_points','fiber_points','proteins_points' \n",
    "# this will not be a problem we do not substract Positive point\n",
    "nutridata['fruits_points'] = nutridata['fruits_points'].fillna(0)\n",
    "nutridata['fiber_points'] = nutridata['fiber_points'].fillna(0)\n",
    "nutridata['proteins_points'] = nutridata['proteins_points'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculation of P x N\n",
    "nutridata['points_N']= nutridata['energy_points'] + nutridata['saturated-fat_points'] + nutridata['sugar_points'] + nutridata['sodium_points']\n",
    "nutridata['points_P'] = nutridata['fruits_points'] + nutridata['fiber_points'] + nutridata['proteins_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_score(row):\n",
    "    N = row['points_N']\n",
    "    P = row['points_P']\n",
    "    fruit = row['fruits_points']\n",
    "    fiber = row['fiber_points']\n",
    "        \n",
    "    if N < 11 or fruit == 5:\n",
    "        return N - P\n",
    "    else:\n",
    "        return N - (fiber + fruit)\n",
    "    \n",
    "nutridata['nutri-score_calculated'] = nutridata.apply(compute_score, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertion of NutriScore into NutriLetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re divided dataframe to allow a simpler transformation of score into letter\n",
    "nutridata_beverages = nutridata[ (nutridata['food_category']=='Beverages') & (nutridata['iswater']!=True)]\n",
    "nutridata_not_beverages = nutridata[ nutridata['food_category']!='Beverages']\n",
    "nutridata_beverages_water = nutridata[ (nutridata['food_category']=='Beverages') & (nutridata['iswater']==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUTRILETTER\n",
    "\n",
    "#letter bins\n",
    "letter_bins_aliments = [nutridata['nutri-score_calculated'].min() - 1, 0, 2.9, 10.9, 18.9, nutridata['nutri-score_calculated'].max()]\n",
    "letter_bins_beverages = [nutridata['nutri-score_calculated'].min() - 1, 2.5, 5.5, 9.5, nutridata['nutri-score_calculated'].max()]\n",
    "letter_bins_beverages_water = [nutridata['nutri-score_calculated'].min() - 1, nutridata['nutri-score_calculated'].max()]\n",
    "\n",
    "#letter\n",
    "nutridata_not_beverages['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_not_beverages['nutri-score_calculated'], letter_bins_aliments, labels=['a','b','c','d','e'])\n",
    "nutridata_beverages['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_beverages['nutri-score_calculated'], letter_bins_beverages, labels=['b','c','d','e'])\n",
    "nutridata_beverages_water['nutri-score_letter_CALCULATED'] = pd.cut(nutridata_beverages_water['nutri-score_calculated'], letter_bins_beverages_water, labels=['a'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutriframes = [nutridata_not_beverages, nutridata_beverages, nutridata_beverages_water]\n",
    "nutridata = pd.concat(nutriframes)\n",
    "\n",
    "nutridata['nutri-score_letter_CALCULATED'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this process of NS calculation we obtain a NS for 770k product that represent almost 80% our the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of proportion of NutriLetter label for each Food Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lettercounts =nutridata.groupby(['food_category','nutri-score_letter_CALCULATED']).count()['nutri-score_calculated']\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax=plt.gca()\n",
    "\n",
    "lettercounts.unstack().div(lettercounts.unstack().sum(axis=1)/100,axis=0).plot.bar(ax=ax,stacked=True)\n",
    "plt.legend(loc='center left')\n",
    "plt.title(\"Normalized Bar Plot of Nutrition Grade for each Category\",fontsize=20);\n",
    "plt.xlabel(\"Food Category\");\n",
    "plt.ylabel(\"Nutrition Grade Percentage\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparation nutriletter rawdata vs calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_2nutriletter =len(nutridata[(nutridata['nutri-score_letter_CALCULATED'].notnull()) & (nutridata['nutrition_grade_fr'].notnull())])\n",
    "product_2nutriletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_samenutriletter =len(nutridata[(nutridata['nutri-score_letter_CALCULATED'].notnull()) & (nutridata['nutrition_grade_fr'].notnull()) & (nutridata['nutri-score_letter_CALCULATED']==nutridata['nutrition_grade_fr'])])\n",
    "product_samenutriletter             \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutri_df = nutridata[(nutridata['nutri-score_letter_CALCULATED'].notnull()) & (nutridata['nutrition_grade_fr'].notnull()) & (nutridata['nutri-score_letter_CALCULATED']!=nutridata['nutrition_grade_fr'])][['product_name','food_category','nutrition_grade_fr','nutri-score_letter_CALCULATED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutri_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutri_df['NG_fr'] = pd.factorize(nutri_df['nutrition_grade_fr'], sort=True)[0] + 1\n",
    "nutri_df['NG_calc'] = pd.factorize(nutri_df['nutri-score_letter_CALCULATED'], sort=True)[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutri_df['Diff'] = (nutri_df['NG_fr'] - nutri_df['NG_calc']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutri_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutri_df.drop(['product_name', 'nutrition_grade_fr', 'nutri-score_letter_CALCULATED'], axis = 1).groupby('food_category').mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression nutriscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_fields_regression =[\n",
    "    'energy_100g','saturated-fat_100g','sugars_100g','sodium_100g',\n",
    "    #'fruits-vegetables-nuts_100g','fiber_100g',\n",
    "    'proteins_100g',\n",
    "    'nutrition_grade_fr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataregression = data[selected_fields_regression]\n",
    "len(dataregression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataregression = dataregression.loc[(~dataregression.isnull()).sum(axis=1)==len(selected_fields_regression)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataregression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = dataregression[\"nutrition_grade_fr\"]\n",
    "X = dataregression[selected_fields_regression].drop(\"nutrition_grade_fr\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitLogRegWithCesSearch(X_train,y_train,minC=1,maxC=10,CStep=1):\n",
    "\n",
    "    Ces = np.linspace(minC, maxC, int((maxC-minC/CStep)+1))\n",
    "    scores = []\n",
    "    \n",
    "    for c in Ces:\n",
    "    \n",
    "        model = LogisticRegression(C=c,random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "        model.fit(X_train, y_train)\n",
    "        # use the model to make predictions with the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        # how did our model perform?\n",
    "        count_misclassified = (y_test != y_pred).sum()\n",
    "        #print('Misclassified samples: {}'.format(count_misclassified))\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        #print('Accuracy: {:.2f}'.format(accuracy))\n",
    "        scores.append(accuracy)\n",
    "        \n",
    "    plt.figure(1, figsize=(6, 6))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(Ces,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitLogRegWithCesSearch(X_train,y_train,minC=10,maxC=20,CStep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diet Study \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Diet Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, tok_gaps=False, saving=False, langdetec=False, tokenize=False, stemming=False, lemmatizing=False, onlyEngStemmer=False, onlyFrStemmer=False, applyNounFilter=False):\n",
    "    \"\"\"function that appeals all cleaning methods depending of the booleans\"\"\"\n",
    "    cleaned_data = pd.DataFrame()    \n",
    "    cleaned_data[column_to_clean] = data[column_to_clean].copy()\n",
    "    \n",
    "    \n",
    "    if langdetec:\n",
    "        cleaned_data[\"languages\"] = data[data[column_to_clean].notnull()][column_to_clean].apply(language_detection)\n",
    "        #I found all the languages disponibles for stemming, and i map the corresponding name to the ISO_code found by langdetect.detect\n",
    "        cleaned_data[\"languages\"] = cleaned_data[\"languages\"].map({\"ar\": \"arabic\", \"da\": \"danish\", \"nl\" : \"dutch\", \"en\": \"english\", \"fi\": \"finnish\", \"fr\": \"french\", \"de\": \"german\", \\\n",
    "                                  \"hu\": \"hungarian\", \"it\": \"italian\", \"no\": \"norwegian\", \"ro\": \"romanian\", \"ru\" : \"russian\", \"es\": \"spanish\", \\\n",
    "                                  \"sv\" :\"swedish\"})\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"exploration/processed_pickle/\"+str(column_to_clean)+\"/out_langdetect.pkl\")\n",
    "       \n",
    "    #tokenize column_to_clean : --> stemming + lemmatization need list of tokens\n",
    "    if tokenize:\n",
    "        cleaned_data[column_to_clean] = tokenize_data(cleaned_data[column_to_clean], stop_words, tokenizer, applyNounFilter)\n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"exploration/processed_pickle/\"+str(column_to_clean)+\"/out_token.pkl\")\n",
    "    else: \n",
    "        cleaned_data = pd.read_pickle(\"exploration/processed_pickle/\"+str(column_to_clean)+\"/out_token.pkl\") \n",
    "    \n",
    "    #stemm column_to_clean column according to the language used\n",
    "    if stemming:\n",
    "        #for stemming, langdetection as to be made previously\n",
    "        if (not(langdetec or onlyEngStemmer or onlyFrStemmer)):\n",
    "            lang_data = pd.read_pickle(\"exploration/processed_pickle/\"+str(column_to_clean)+\"/out_langdetect.pkl\")\n",
    "            cleaned_data[\"languages\"] = lang_data[\"languages\"].copy()\n",
    "        \n",
    "        cleaned_data = stemming_data(cleaned_data, column_to_clean, onlyEngStemmer, onlyFrStemmer)\n",
    "        \n",
    "        if saving:\n",
    "            cleaned_data.to_pickle(\"exploration/processed_pickle/\"+str(column_to_clean)+\"/out_stem.pkl\")  \n",
    "            \n",
    "    elif lemmatizing: \n",
    "        #lemmatizing only works well for english words\n",
    "        cleaned_data = lemmatizing_data(cleaned_data, column_to_clean, wordnet_lemmatizer)\n",
    "        if saving: \n",
    "            cleaned_data.to_pickle(\"exploration/processed_pickle/\"+str(column_to_clean)+\"/out_lem\")    \n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def filter_nountag(tokenlist_cell):\n",
    "    \"\"\"keep only NN : nouns, singular or mass and NNS : noun,plural and NNP : proper noun\"\"\"\n",
    "    \n",
    "    postag_cell = pos_tag(tokenlist_cell)\n",
    "    postag_cell_filtered = [tag for tag in postag_cell if ((tag[1]=='NNS') or (tag[1] == 'NN') or (tag[1] == \"NNP\"))]\n",
    "    tokenlist_cell = [tag[0] for tag in postag_cell_filtered]\n",
    "    \n",
    "    return tokenlist_cell\n",
    "\n",
    "def tokenize_data(data_column, stop_words, tokenizer, applyNounFilter = False, tok_gaps = False):\n",
    "    \"\"\"Clean data, i.e. \n",
    "        - lower each words in the cells of data_column\n",
    "        - tokenize cells of data_column, i.e. from float type create list of string (token)\n",
    "        - remove stopwords for the list of tokens for each cells of data_column\n",
    "        - keep only tokens with tag = NN (noun), or NNS (noun, plural)\n",
    "    \"\"\"  \n",
    "    if tok_gaps:\n",
    "        data_column = data_column[data_column.notnull()].str.lower() \\\n",
    "            .apply(str) \\\n",
    "            .apply(lambda x :  \"\".join([ch for ch in x if ch not in string.punctuation.replace(\"-\", \"\")])) \\\n",
    "            .apply(tokenizer.tokenize) \\\n",
    "            .apply(lambda cell : [item for item in cell if item not in stop_words])\n",
    "        if applyNounFilter:\n",
    "            data_colun = data_column[data_column.notnull()].apply(filter_nountag)\n",
    "    \n",
    "    else: \n",
    "        data_column = data_column[data_column.notnull()].str.lower() \\\n",
    "            .apply(str) \\\n",
    "            .apply(tokenizer.tokenize) \\\n",
    "            .apply(lambda cell : [item for item in cell if item not in stop_words]) \n",
    "        if applyNounFilter:\n",
    "            data_column = data_column[data_column.notnull()].apply(filter_nountag)\n",
    "    \n",
    "    return data_column\n",
    "\n",
    "def language_detection(category_cell): \n",
    "    \"\"\"take a cell containing a the string from unprocessed dataframe and detect the language\"\"\"\n",
    "    tmp_cell = str()\n",
    "    tmp_cell = category_cell\n",
    "    \n",
    "    #supress numbers in string\n",
    "    tmp_cell = tmp_cell.replace('\\d+', '')\n",
    "    #supress punctuations\n",
    "    tmp_cell = re.sub(r'[^\\w\\s]','', tmp_cell)\n",
    "    #remove spaces in string\n",
    "    tmp_cell = ''.join(tmp_cell.split())\n",
    "    \n",
    "    #check if the string contain only letters --> f**king 🍩 \n",
    "    if tmp_cell.isalpha():\n",
    "        language = detect(category_cell)\n",
    "        return detect(category_cell)\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def stemming_data(df_lang, serie_tokenlist, onlyEngStemmer = False, onlyFrStemmer=False):\n",
    "    \"\"\"Take as argument : \n",
    "        - df_lang = dataframe containing a serie of tokenlist, and a serie \"languages\" of corresponding language\n",
    "        - serie_tokenlist = string name of the serie of tokenlist to stem\n",
    "    \"\"\"\n",
    "    \n",
    "    if onlyEngStemmer: \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    elif onlyFrStemmer:\n",
    "        stemmer = SnowballStemmer(\"french\")\n",
    "    \n",
    "    for i in [1, 1]:\n",
    "        for index, row in tqdm(df_lang[df_lang[serie_tokenlist].notnull()].iterrows()):\n",
    "            if (onlyEngStemmer==False and onlyFrStemmer==False): \n",
    "                stemmer = SnowballStemmer(row[\"languages\"])\n",
    "            \n",
    "            df_lang.loc[index,serie_tokenlist] = [stemmer.stem(token) for token in row[serie_tokenlist]]\n",
    " \n",
    "        \n",
    "    return df_lang\n",
    "\n",
    "def lemmatizing_data(df, serie_tokenlist, wordnet_lemmatizer):\n",
    "    \"\"\"lemmatize serie containing token_list\n",
    "        - serie_tokenlist = string name of the serie of tokenlist to lemmatize\n",
    "        - df = dataframe containing \"serie_tokenlist\" column\n",
    "        \"\"\"\n",
    "    for index, row in df[df[serie_tokenlist].notnull()].iterrows():\n",
    "        df.loc[index,serie_tokenlist] =  [wordnet_lemmatizer.lemmatize(token, pos=\"n\") for token in row[serie_tokenlist]]\n",
    "        #if u lemmatize verbs --> pos = \"-v\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving = False\n",
    "# note : Language detection is only needed for stemming, \n",
    "# and it is a very long run (~1h)\n",
    "regimes = ['isVegetarian', 'isVegetalian', 'isGlutenfree', 'isLactosefree', 'isKetogenic', 'isOrganic']\n",
    "#StopWords and Tokenizer Object initialization\n",
    "stop_words = set(stopwords.words(\"french\")).union(set(stopwords.words(\"english\"))) #will remove only english and french stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning _categories_ field\n",
    "\n",
    "We first clean the 'categories' field and save the result in pickle format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> If column field has to be cleaned\n",
    "\n",
    "The cell below aims to clean the \"categories\" field and save outputs of each steps of the cleaning in pickle format. \n",
    "* create a repository in processed_pickle which has the name of the column to clean, i.e. processed_pickle/categories/ is the repository in which pickles will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column_to_clean = \"categories\"\n",
    "# #language detection\n",
    "# langdetect_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, langdetec = True)\n",
    "# #tokenization\n",
    "# tok_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, tokenize = True, applyNounFilter = True)\n",
    "# #stemming --> good since it consider language\n",
    "# stem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, stemming = True)\n",
    "# #lemmatize --> not very good since it does not consider language (only good for english)\n",
    "# lem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving = True, lemmatizing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> If column field already cleaned and saved in pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column = \"categories\"\n",
    "langdetect_data = pd.read_pickle(\"exploration/processed_pickle/\"+str(column)+\"/out_langdetect.pkl\")\n",
    "categories_tok = pd.read_pickle(\"exploration/processed_pickle/\"+str(column)+\"/out_token.pkl\") \n",
    "categories_stem = pd.read_pickle(\"exploration/processed_pickle/\"+str(column)+\"/out_stem.pkl\") \n",
    "categories_lem = pd.read_pickle(\"exploration/processed_pickle/\"+str(column)+\"/out_lem.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories_dict = categories_stem[categories_stem[\"categories\"].notnull()][\"categories\"].explode().value_counts()\n",
    "for word in categories_dict[categories_dict>500].index:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for _categories_ field\n",
    "\n",
    "We create dictionary for each diets. They have a YES list and NO list. A product is considered part of the diet is it has at least ONE element in its YES list (if not empty) and NO element in its NO list (if not empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Initialization needed to create a serie containing unauthorized items  for each of the diets\n",
    "# unauthorized_categories = pd.Series(index = regimes).rename('unauthorized_categories')\n",
    "# \n",
    "# unauthorized_listnames_categories = ['categories_' + str(x) + '_NOlist' for x in regimes]\n",
    "# unauthorized_listnames_categories\n",
    "# \n",
    "# #Initialization needed to create a serie containing authorized items for each of the diets\n",
    "# authorized_catexgories = pd.Series(index = regimes).rename('authorized_labelsen')\n",
    "# \n",
    "# authorized_listnames_categories = ['categories_' + str(x) + '_YESlist' for x in regimes]\n",
    "# authorized_listnames_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GENERAL LISTS\n",
    "viand_list =  ['foi', 'foie', 'steak', 'viand', 'sausag', 'viandes', 'meat', 'beef', 'poisson', 'charcut', 'charcuteries', 'charcuteri', 'poulet', 'poul', 'volaill', 'jambons', 'jambon', 'jam', 'rillet', 'sardin', 'thon', 'steak', 'saumon', 'salmon', 'escalop', 'dind', 'jam', \\\n",
    "               'terrin', 'boudin', 'animal', 'cannard', 'porc', 'lasagn', 'cuiss', 'fish', 'crevet', 'fleisch', 'vach', 'chipolat', 'fisch', 'lardon', 'bacon', 'hareng', 'seafood', 'chicken', 'breb', 'bovin']\n",
    "\n",
    "lactose_list = ['fromag', 'lait', 'yaourt', 'bi', 'écrem', 'cream', 'crem', 'glac', 'milk', 'milks', 'dairi', 'dair', 'dairies', 'beurr', 'milch', 'milchprodukt', 'chee', \\\n",
    "                                'cake', 'crêp', 'bread', 'gâteau' , 'lait', 'butter', 'butt', 'cooki', 'emmental', 'comt', 'camembert', 'margarin', 'kas', 'cheddar', 'yogurt', 'yogur', 'yogurts']\n",
    "\n",
    "gluten_list = ['pât', 'pasta', 'pâtiss', 'past', 'nouill', 'viennois', 'biscuit', 'biscuits', 'sauc', 'sauces', 'charcut', 'charcuteri', 'charcuteries', 'bread', 'crêp', 'gâteau', 'farin', 'farines', 'pizz', 'pizzas', 'cake', 'madelein', \\\n",
    "               'brotaufstrich', 'brot', 'beignet', 'lasagn', 'raviol', 'blé', 'wheat', 'bi', 'pies', 'kuch', 'muffin', 'gaufr', 'gaufret', 'biscott', 'cooki', 'cook', 'brot']\n",
    "\n",
    "fruit_list =  ['fruits','fruit', 'multifruit', 'pomm', 'confitur','nectar', 'nect', 'jus', 'juic','frut', 'sorbet', 'tomat', 'tomato', 'orang', 'compotes', 'marmelad'] \n",
    "\n",
    "legum_list = ['legum', 'légum',  'plant', 'végétal', 'vegetal', 'pflanzlich', 'salad',  'verdur', 'haricot',  'amand', 'veget',  'carott', 'oignon']\n",
    "\n",
    "fat_list = ['huil', 'oil', 'oils', 'fat', 'gras', 'grass'] \n",
    "\n",
    "egg_list = ['mayon', 'mayonnaises', 'mayonnaises', 'moutardes', 'mustard', 'egg', 'œuf']\n",
    "\n",
    "# Vegetarian_NOlist = regimes(0)\n",
    "notVegetarian_categories = viand_list\n",
    "isVegetarian_categories = lactose_list + \\\n",
    "                        fruit_list + \\\n",
    "                        legum_list + \\\n",
    "                        fat_list + \\\n",
    "                        egg_list \n",
    "\n",
    "# Lactosefree = regime(3)\n",
    "notLactosefree_categories = lactose_list\n",
    "isLactosefree_categories = viand_list + \\\n",
    "                            fruit_list + \\\n",
    "                            legum_list + \\\n",
    "                            fat_list + \\\n",
    "                            ['céréal', 'céréales', 'cereal', 'getreid', 'pât', 'sauces', 'sauc', \\\n",
    "                            'chips', 'chip', 'confiseries', 'confiseri', 'pain', 'huil', 'vin', 'vinaigres', \\\n",
    "                            'vinaigr', 'vinaigret', 'miel',  'bonbon', \\\n",
    "                            'grain', 'sirop', 'pickl', 'flocon',  'oliv', \\\n",
    "                            'riz', 'moutardes', 'moutard', 'champignon', 'farin', 'farines', 'noiset', 'caf', 'bean', 'pasta',\\\n",
    "                            'escalop', 'flake', 'flak', 'bouillon', 'seed', 'wine', 'win', 'potato', 'sod', 'nut', 'nouill', 'lentill', 'candi', \\\n",
    "                            'corn', 'sel', 'sucr', 'thé', 'tournesol', 'semoul', 'vert', 'muesl', 'pur', 'épic', 'vanill', 'minéral', 'minerales']\n",
    "\n",
    "\n",
    "#Vegan_NOlist = regimes(1)\n",
    "notVegetalian_categories = lactose_list + \\\n",
    "                              viand_list + \\\n",
    "                              egg_list\n",
    "\n",
    "isVegetalian_categories = legum_list + \\\n",
    "                           fruit_list \n",
    "\n",
    "#Glutenfree = regime(2)\n",
    "notGlutenfree_categories = gluten_list\n",
    "isGlutenfree_categories = fruit_list + \\\n",
    "                            legum_list + \\\n",
    "                            egg_list + \\\n",
    "                            fat_list\n",
    "\n",
    "\n",
    "#Ketogenic = regimes(4)\n",
    "notKetogenic_categories = fruit_list + \\\n",
    "                              ['sucr', 'confiseries', 'confiseri', 'confis' , 'nouill', 'semoul', 'legumin', 'légumin', 'potato', 'bean', 'haricot', 'lentill', 'lentill', 'riz', 'pasta', \\\n",
    "                               'pasta', 'milk', 'milks', 'sirop', 'candi', 'jus', 'juic', 'tart', 'marmelad', 'snack', 'snacks', 'biscuit', 'pât', 'confitur', 'chocolat', 'chocolats', 'chocol', \\\n",
    "                               'nectar', 'nect', 'bread', 'chip', 'chips', 'pain', 'miel', 'bonbon', 'compot', 'pâtiss']\n",
    "\n",
    "isKetogenic_categories = lactose_list + \\\n",
    "                            viand_list + \\\n",
    "                            fat_list + \\\n",
    "                            egg_list  + \\\n",
    "                            legum_list + \\\n",
    "                            ['grain', 'nut', 'tomato', 'tomat', 'seed']\n",
    "\n",
    "\n",
    "\n",
    "#Organic = regimes(5)\n",
    "isOrganic_categories = []\n",
    "notOrganic_categories = []\n",
    "\n",
    "\n",
    "isRegimes_categories = [isVegetarian_categories, isVegetalian_categories, \\\n",
    "                      isGlutenfree_categories, isLactosefree_categories, \\\n",
    "                      isKetogenic_categories, isOrganic_categories]\n",
    "\n",
    "notRegimes_categories = [notVegetarian_categories, notVegetalian_categories, \\\n",
    "                      isGlutenfree_categories, notLactosefree_categories, \\\n",
    "                      notKetogenic_categories, notOrganic_categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning _labels_en_ field\n",
    "\n",
    "The 'label_en' column is used to complete the diet attribution of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##here we use tokenizer gaps to keep compounds words (don't separate words with dashe, e.g. gluten-free --> \"gluten-free\")\n",
    "#tokenizer_gaps = RegexpTokenizer('\\s+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column_to_clean = \"labels_en\"\n",
    "#\n",
    "##tokenization\n",
    "#tok_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, tok_gaps=False, saving=True, tokenize=True)\n",
    "##stemming --> good since it consider language\n",
    "#stem_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, tok_gaps=False, saving=True, stemming=True, onlyEngStemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column = \"labels_en\"\n",
    "\n",
    "label_tok = pd.read_pickle(\"exploration/processed_pickle/\"+column+\"/out_token.pkl\")\n",
    "label_stem = pd.read_pickle(\"exploration/processed_pickle/\"+column+\"/out_stem.pkl\")\n",
    "\n",
    "##With gap, gluten-free is not separated in 'gluten' 'free'\n",
    "label_tokgap = pd.read_pickle(\"exploration/processed_pickle/\"+column+\"/out_tokengap.pkl\")\n",
    "label_stemgap = pd.read_pickle(\"exploration/processed_pickle/\"+column+\"/out_stemgap.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for _labels_en_ field\n",
    "\n",
    "The same methodology is used for obtaining diet information from the 'labels_en' column and completing our diet categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data[\"labels_en\"].notnull()][\"labels_en\"].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labelsen_dict = label_stem[label_stem[\"labels_en\"].notnull()][\"labels_en\"].explode().value_counts()\n",
    "for word in labelsen_dict[labelsen_dict > 1000].index:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Initialization needed to create a serie containing authorized items for each of the diets\n",
    "#authorized_labelsen = pd.Series(index = regimes).rename('authorized_labelsen')\n",
    "#\n",
    "#authorized_listnames_labelsen = ['labelsen_' + str(x) + '_YESlist' for x in regimes]\n",
    "#authorized_listnames_labelsen\n",
    "#\n",
    "##Initialization needed to create a serie containing unauthorized items  for each of the diets\n",
    "#unauthorized_labelsen = pd.Series(index = regimes).rename('unauthorized_labelsen')\n",
    "#\n",
    "#unauthorized_listnames_labelsen = ['labelsen_' + str(x) + '_NOlist' for x in regimes]\n",
    "#unauthorized_listnames_labelsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organic = regimes(5)\n",
    "isOrganic_labels_en = ['organic', 'bio', 'biologique']\n",
    "#Ketogenic = regimes(4)\n",
    "isKetogenic_labels_en = []\n",
    "\n",
    "#Lactosefree = regime(3) #since lactose only appears as NO lactose\n",
    "isLactosefree_labels_en = ['lactose']\n",
    "\n",
    "#Glutenfree = regime(2)\n",
    "isGlutenfree_labels_en_gap = ['gluten-free']\n",
    "isGlutenfree_labels_en  = ['gluten']\n",
    "\n",
    "#Vegan_NOlist = regimes(1)\n",
    "isVegetalian_labels_en = ['vegan']\n",
    "\n",
    "#Vegetarian_NOlist = regimes(0)\n",
    "isVegetarian_labels_en = ['vegetarian']\n",
    "\n",
    "#isRegimes_labels_en_gap = [labelsen_Vegetarian_YESlist, labelsen_Vegetalian_YESlist, labelsen_Glutenfree_YESlistgap, \\\n",
    "#                    labelsen_Lactosefree_YESlist, labelsen_Ketogenic_YESlist, labelsen_Organic_YESlist]\n",
    "\n",
    "isRegimes_labels_en = [isVegetarian_labels_en, isVegetalian_labels_en, isGlutenfree_labels_en, \\\n",
    "                    isLactosefree_labels_en, isKetogenic_labels_en, isOrganic_labels_en]\n",
    "\n",
    "isRegimes_labels_en_gap = [isVegetarian_labels_en, isVegetalian_labels_en, isGlutenfree_labels_en_gap, \\\n",
    "                    isLactosefree_labels_en, isKetogenic_labels_en, isOrganic_labels_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet Dictionary for _ingredients_text_ field\n",
    "\n",
    "The same methodology is used for obtaining diet information from the 'ingredients_text' column and completing our diet categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"ingredients_text\"].notnull()][\"ingredients_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column = \"ingredients_text\"\n",
    "# #tokenization  \n",
    "# tok_data = clean_data(data, column_to_clean, stop_words, tokenizer, wordnet_lemmatizer, saving=True, tokenize=True)\n",
    "# #stemming --> good since it consider language\n",
    "# stem_data = clean_data(data, column, stop_words, tokenizer, wordnet_lemmatizer, saving=True, stemming=True, onlyFrStemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientstext_tok = pd.read_pickle(\"exploration/processed_pickle/ingredients_text/out_token.pkl\")\n",
    "ingredientstext_stem = pd.read_pickle(\"exploration/processed_pickle/ingredients_text/out_stem.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientstext_tok_dict = ingredientstext_tok[ingredientstext_tok[\"ingredients_text\"].notnull()][\"ingredients_text\"].explode().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredientstext_tok = pd.Series(data = ingredientstext_tok_dict[ingredientstext_tok_dict > 100])\n",
    "ingredientstext_tok1000 = pd.Series(data = ingredientstext_tok.index)\n",
    "ingredientstext_tok1000.to_csv(\"ingredients_text_tokens100.csv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ingredientstext_dict = ingredientstext_stem[ingredientstext_stem[\"ingredients_text\"].notnull()][\"ingredients_text\"].explode().value_counts()\n",
    "for word in ingredientstext_dict[ingredientstext_dict > 1000].index:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organic = regimes(5)\n",
    "#isOrganic_ingredients_text = ['organic']\n",
    "notOrganic_ingredients_text = ['TODO']\n",
    "\n",
    "#Ketogenic = regimes(4)\n",
    "#isKetogenic_ingredients_text = ['oil']\n",
    "notKetogenic_ingredients_text = ['sucr','sucres','sugar','pât','ric', 'potato', 'barley', 'malted', 'orge', 'ble', 'cornstarch', 'avoin',\\\n",
    "                                 'froment', 'seigl',  'cereal', 'pâtiss', 'patat', 'houblon', 'sarrasin',\\\n",
    "                                 'épeautr', 'bread', 'sucros', 'seigle', 'wheat', 'basmat', 'corn', 'blé', 'pâte', 'riz', 'rice', 'zucker',\\\n",
    "                                 'céréales', 'potatoes', 'pasta', 'pain', 'biscuit', 'basmati', 'macaroni', 'patata', 'patate', 'riso',\\\n",
    "                                 'cereals', 'bière', 'cereales', 'noodles', 'nouilles', 'reis', 'orzo', 'weizenstärke', 'penne', 'ravioli',\\\n",
    "                                 'beer', 'weizen', 'frites', 'gâteau', 'pinda', 'patatas', 'potatis', 'lacto', 'pains',\\\n",
    "                                 'blé', 'avoine', 'orge','riz', 'quinoa', 'soya', 'chiches', 'lentille', 'gâteaux','haricots','miel','soda', ]\n",
    "\n",
    "#Lactosefree = regime(3) \n",
    "#isLactosefree_ingredients_text = []\n",
    "notLactosefree_ingredients_text = ['lait', 'milk', 'whey', 'pasteurized', 'lactos', 'lactiqu', 'fromag', 'cheddar', 'lactat', 'lactose',\\\n",
    "                                   'mozzarel', 'mozzarella', 'yogurt', 'emmental', 'lactylat', 'lactosérum', 'yaourt', 'milch', 'fromage',\\\n",
    "                                   'dairy', 'leit', 'cheese', 'pasteurisé', 'leche', 'lactic', 'latte', 'melk', 'leite', 'pasteurizada',\\\n",
    "                                   'pasteurisés', 'lactoserum', 'laitière', 'lactis', 'lactosa', 'laitiers', 'yogourt', 'pasteurised',\\\n",
    "                                   'pasteurisierte', 'feta', 'lactase', 'joghurt', 'gouda', 'laitier', 'mleko', 'cheeses', 'yoghurt',\\\n",
    "                                   'fromages', 'láctico', 'laitières', 'milcheiweiß', 'lalt', 'lattici', 'milchzucker', 'raclette',\\\n",
    "                                   'pasteurisiert', 'laktose', 'milchschokolade', 'pastorizzato', 'lacticos', 'pasteurizat', 'brie',\\\n",
    "                                   'roquefort', 'lactilato']\n",
    "\n",
    "#Glutenfree = regime(2)\n",
    "#isGlutenfree_ingredients_text  = [] \n",
    "notGlutenfree_ingredients_text = ['wheat', 'blé', 'ble', 'soj', 'soy', 'soybean', 'soja', 'soję', 'soya', 'soybeans', 'soia', 'sojalecithine',\\\n",
    "                                  'sojalecithin', 'lecithin', 'pât', 'orge', 'froment', 'seigl', 'épeautr', 'seigle', 'épeautre', 'epeautre',\\\n",
    "                                 'kamut','seigle','pates','pâtes','pate','pâte','raviolis','ravioli','gnocchi','boulghour','chapelure','pain',\\\n",
    "                                 'biscottes','viennoiseries','beignets','crepes','crêpes', 'gaufres','pâtisseries','biscuits','pané','quiches',\\\n",
    "                                 'cannelloni','hosties','biere','bière','panache','panaché']\n",
    "\n",
    "\n",
    "#Vegetarian = regimes(0)\n",
    "#isVegetarian_ingredients_text = []\n",
    "notVegetarian_ingredients_text = ['porc', 'pork', 'por', 'viand', 'viande', 'meat', 'poulet', 'chicken', 'poisson',  'bœuf', 'boeuf', 'beef',\\\n",
    "                                  'jambon', 'vache', 'vach', 'canard', 'foi', 'foie', 'dind', 'dinde', 'cow', 'mollusqu', 'mollusque',\\\n",
    "                                  'mollusques', 'saumon', 'boyau', 'turkey', 'poul', 'bovin', 'volaill', 'volaille', 'poitrin', 'animal',\\\n",
    "                                  'crevet', 'couen', 'couenne', 'bacon', 'poissons', 'thon', 'shrimp', 'chipotl', 'vac', 'veau', 'salmo',\\\n",
    "                                  'fish', 'thicken', 'carn', 'crustacés', 'chèvre', 'bovine', 'os', 'brebis', 'vaca', 'crevettes', 'carne',\\\n",
    "                                  'lard', 'viandes', 'fleisch', 'boeufs', 'steak', 'charcut', 'charcuteries', 'charcuterie', 'poulets', \\\n",
    "                                  'poule', 'poules', 'volailles', 'jambons',  'cerdo', 'lardon', 'lardons', 'schweinefleisch', 'cannard',\\\n",
    "                                  'ancho', 'cabillaud', 'nuggets', 'homard', 'magret', 'harengus', 'porcini', 'colin', 'dindes', 'speck',\\\n",
    "                                  'escargots', 'boudin', 'rillette', 'rillettes', 'saucisse', 'épaule', 'serrano', 'truite', 'salami',\\\n",
    "                                  'poitrine', 'cuisse', 'cuisses', 'agneau', 'merguez', 'crevette', 'merlu', 'hareng', 'chipolata',\\\n",
    "                                  'chipolatas', 'mouton', 'halal', 'kidney', 'chorizo', 'crabe', 'lapin', 'sardines', 'porcine', 'anchois',\\\n",
    "                                  'taurine', 'ham', 'sheep', 'saucisses', 'fillets', 'crab', 'goat', 'crustacé', 'anchovies', 'pollo',\\\n",
    "                                  'couennes', 'morue', 'moules', 'saucisson', 'fisch', 'sardine', 'sardina', 'thuna', 'tuna', 'salmon',\\\n",
    "                                  'seafood', 'maquereaux', 'albacore', 'rib', 'shea', 'crumbs', 'gelatine']\n",
    "\n",
    "\n",
    "#Vegan = regimes(1)\n",
    "#isVegetalian_ingredients_text = []\n",
    "notVegetalian_ingredients_text = notVegetarian_ingredients_text + notLactosefree_ingredients_text + \\\n",
    "                                 ['egg', 'eggs', 'œuf', 'œufs', 'oeuf', 'oeufs', 'ceufs', 'honey', 'abeille', 'miel', 'gélatin', 'chevr',\\\n",
    "                                  'chèvre', 'mayon', 'yolks', 'yolk', 'milkfat', 'torula', 'animale', 'animal', 'ceuf', 'beeswax', 'animaux',\\\n",
    "                                  'eufs', 'euf', 'auf', 'miels', 'miele']\n",
    "\n",
    "#isRegimes_ingredients_text = [isVegetarian_ingredients_text, isVegetalian_ingredients_text, isGlutenfree_ingredients_text, \\\n",
    "#                                isLactosefree_ingredients_text, isKetogenic_ingredients_text, isOrganic_ingredients_text]\n",
    "\n",
    "notRegimes_ingredients_text = [notVegetarian_ingredients_text, notVegetalian_ingredients_text, notGlutenfree_ingredients_text, \\\n",
    "                                notLactosefree_ingredients_text, notKetogenic_ingredients_text, notOrganic_ingredients_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet series creation\n",
    "\n",
    "We create the series representing possible the diet regimes of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isDiet(input_df, split_col, regimes, isRegimes_lists = None, notRegimes_lists = None): \n",
    "    \"\"\"Create isDiet columns (isVegetarian, isVegetalian,...) containing bool cell for each product, True or False. It takes into account notRegime_lists and isRegime_lists\"\"\"\n",
    "    data = input_df.copy()\n",
    "    for index, isRegime in enumerate(regimes): \n",
    "        \n",
    "        if ((isRegimes_lists!=None) & (notRegimes_lists==None)):\n",
    "            tmp_data = data.loc[data[split_col].notnull(), split_col].copy()\n",
    "            tmp_diet = tmp_data.apply(lambda token_list : bool(any([token in isRegimes_lists[index] for token in token_list])))\n",
    "            \n",
    "            #If isRegime already exist, just complete the lines still NaN, or Bool that are False\n",
    "            if isRegime in data.columns:\n",
    "                index1 = data[(data[isRegime]==False) | (data[isRegime].isnull())].index\n",
    "                index2 = tmp_diet.index\n",
    "                data.loc[index1.intersection(index2), isRegime] = tmp_diet.loc[index1.intersection(index2)]\n",
    "            else: \n",
    "                data[isRegime] = tmp_diet\n",
    "            \n",
    "        elif (isRegimes_lists!=None) & (notRegimes_lists!=None): \n",
    "            tmp_data = data.loc[data[split_col].notnull(), split_col].copy()\n",
    "            \n",
    "            tmp_diet_not = tmp_data.apply(lambda token_list : bool ((any([token in notRegimes_lists[index] for token in token_list])))).apply(lambda boolean: not(boolean))\n",
    "            tmp_diet_is =  tmp_data.apply(lambda token_list : bool ((any([token in isRegimes_lists[index] for token in token_list]))))\n",
    "            \n",
    "            tmp_diet = pd.DataFrame()\n",
    "            tmp_diet[\"not\"] = tmp_diet_not\n",
    "            tmp_diet[\"is\"] = tmp_diet_is\n",
    "            \n",
    "            tmp_diet = tmp_diet.apply(lambda row : isAndnot(row), axis = 1)\n",
    "            \n",
    "            if isRegime in data.columns:\n",
    "                index1 = data[(data[isRegime]==False) | (data[isRegime].isnull())].index\n",
    "                index2 = tmp_diet.index\n",
    "                data.loc[index1.intersection(index2), isRegime] = tmp_diet.loc[index1.intersection(index2)]\n",
    "            else: \n",
    "                data[isRegime] = tmp_diet\n",
    "            \n",
    "        elif ((isRegimes_lists==None) & (notRegimes_lists!=None)):\n",
    "            tmp_data = data.loc[data[split_col].notnull(), split_col]\n",
    "            tmp_diet = tmp_data.apply(lambda token_list : bool ((any([token in notRegimes_lists[index] for token in token_list])))).apply(lambda boolean: not(boolean))\n",
    "            \n",
    "            if isRegime in data.columns:\n",
    "                index1 = data[(data[isRegime]==False) | (data[isRegime].isnull())].index\n",
    "                index2 = tmp_diet.index\n",
    "                data.loc[index1.intersection(index2), isRegime] = tmp_diet.loc[index1.intersection(index2)]\n",
    "            else: \n",
    "                data[isRegime] = tmp_diet\n",
    "                \n",
    "    return data\n",
    "\n",
    "def isAndnot(row): \n",
    "    return row[\"is\"] and row[\"not\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test if ___isDiet()___ works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test if isDiet booleans check if word in/not in list work as needed. \n",
    "testdf_categories = pd.DataFrame()\n",
    "testserie_categories = pd.Series([[str('mayon'), str('pasta'), str('milk'), str('fruits')], \\\n",
    "                        [str('pasta'), str('fruits')], \\\n",
    "                        [str('steak'), str('milk'), str('fruits')], \\\n",
    "                        [str('steak'), str('pasta'), str('fruits')],\\\n",
    "                        [str('steak'), str('pasta'), str('milk')], \\\n",
    "                        [str('')], \\\n",
    "                        [str('mayon'), str('milk'), str('fruits')], \\\n",
    "                        [str('milk'), str('fruits')], \\\n",
    "                        [str('steak'), str('fruits')], \\\n",
    "                        [str('steak'), str('milk')], \\\n",
    "                        [str('steak'), str('pasta'), str('milk'), str('fruits')]])\n",
    "testseries1_categories = pd.Series([['vege'], ['vegan + vege + lactose-free'], ['gluten-free'],['lactose-free'], ['ketogenic'], [''], ['vege + gluten-free'], \\\n",
    "                         ['vege + gluten-free'], ['gluten-free + lactose-free'], ['gluten-free + ketogenic'], ['All False']])\n",
    "testdf_categories[\"categories_tok\"] = testserie_categories\n",
    "testdf_categories[\"expected_output\"] = testseries1_categories\n",
    "testdf_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_categories = isDiet(testdf_categories, \"categories_tok\", regimes[:5], isRegimes_lists = isRegimes_categories[:5], notRegimes_lists = notRegimes_categories[:5])\n",
    "testdf_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_categories = isDiet(testdf_categories, \"categories_tok\", regimes[:5], notRegimes_lists = notRegimes_categories[:5])\n",
    "testdf_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Seems to works well when creating the new fields. Let's see if it also works when the field has already been created et when it only needs to complete None and False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_serie = pd.Series([[str('mayon'), str('pasta'), str('milk'), str('fruits')], \\\n",
    "                        [str('pasta'), str('fruits')], \\\n",
    "                        [str('steak'), str('milk'), str('fruits')], \\\n",
    "                        [str('steak'), str('pasta'), str('fruits')],\\\n",
    "                        [str('steak'), str('pasta'), str('milk')], \\\n",
    "                        [str('')], \\\n",
    "                        [str('mayon'), str('milk'), str('fruits')], \\\n",
    "                        [str('milk'), str('fruits')], \\\n",
    "                        [str('steak'), str('fruits')], \\\n",
    "                        [str('steak'), str('milk')], \\\n",
    "                        [str('steak'), str('pasta'), str('milk'), str('fruits')]])\n",
    "test_serie1 = pd.Series([[''], \\\n",
    "                         [''], \\\n",
    "                         [''], \\\n",
    "                         [''], \\\n",
    "                         [''], \\\n",
    "                         ['gluten'], \\\n",
    "                         [''], \\\n",
    "                         [''], \\\n",
    "                         [''], \\\n",
    "                         [''], \\\n",
    "                         ['']])\n",
    "test_serie2 = pd.Series([['vege'], ['vegan + vege + lactose-free'], ['gluten-free'],['lactose-free'], ['ketogenic'], ['gluten-free'], ['vege + gluten-free'], \\\n",
    "                         ['vege + gluten-free'], ['gluten-free + lactose-free'], ['gluten-free + ketogenic'], ['All False']])\n",
    "test_df[\"categories_tok\"] = test_serie\n",
    "test_df[\"labelsen_tok\"] = test_serie1\n",
    "test_df[\"expected_output\"] = test_serie2\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = isDiet(test_df, \"categories_tok\", regimes[:5], isRegimes_lists = isRegimes_categories[:5], notRegimes_lists = notRegimes_categories[:5])\n",
    "test_df = isDiet(test_df, \"labelsen_tok\", regimes[:5], isRegimes_lists = isRegimes_labels_en[:5])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Seems to work well in that case too! (see line 5). Lets try on the real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with _labels_en_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we use the tokenizer with gaps = True (keep gluten-free as 'gluten-free')\n",
    "data[\"labels_en_tokgap\"] = label_stemgap[\"labels_en\"]\n",
    "dietdf_labels_en_gap = isDiet(data, \"labels_en_tokgap\", regimes, isRegimes_lists = isRegimes_labels_en_gap)\n",
    "\n",
    "#Here we use the normal tokenizer (gluten-free becomes 'gluten' 'free')\n",
    "data[\"labels_en_tok\"] = label_stem[\"labels_en\"]\n",
    "dietdf_labels_en = isDiet(data, \"labels_en_tok\", regimes, isRegimes_lists = isRegimes_labels_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Using labels_en field with gap tokenizer, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    if len(dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts()[1]/len(dietdf_labels_en_gap)*100\n",
    "        nb_true = dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts()[0]\n",
    "    elif len(dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts().index[0] == False):\n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts().index[0] == True):\n",
    "            percentage = 0; \n",
    "            nb_true = dietdf_labels_en_gap[dietdf_labels_en_gap[\"labels_en_tokgap\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime))                                                \n",
    "\n",
    "    \n",
    "print(\"\\n\\n Using labels_en field with normal tokenizer, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    if len(dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts()[1]/len(dietdf_labels_en)*100\n",
    "        nb_true = dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts()[0]\n",
    "    elif len(dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts().index[0] == False):\n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts().index[0] == True):\n",
    "            percentage = 0; \n",
    "            nb_true = dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Regarding the previous output, we will use preferentially the normal tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, isRegime in enumerate(regimes):\n",
    "    if (len(dietdf_labels_en[dietdf_labels_en[\"labels_en_tok\"].notnull()][isRegime].value_counts()) == 2):\n",
    "        print(\"\\n\"+isRegime+\" --> 3 True samples:\")\n",
    "        samples_true = dietdf_labels_en[dietdf_labels_en[isRegime]==True][\"labels_en\"].sample(n=3, random_state=1)\n",
    "        print(samples_true)\n",
    "    \n",
    "    print(\"\\n\"+isRegime+\" --> 3 False samples:\")\n",
    "    samples_false = dietdf_labels_en[dietdf_labels_en[isRegime]==False][\"labels_en\"].sample(n=3, random_state=1)\n",
    "    print(samples_false)\n",
    "    \n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with _categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we use both : isRegimes_lists and notRegimes_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"categories_tok\"] = categories_stem[\"categories\"]\n",
    "dietdf_categories = isDiet(data, \"categories_tok\", regimes, isRegimes_lists=isRegimes_categories, notRegimes_lists=notRegimes_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][\"isOrganic\"].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n Using categories field isRegimes_lists and notRegimes_lists, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    if len(dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts()[1]/len(dietdf_categories)*100\n",
    "        nb_true = dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts()[0]\n",
    "    elif len(dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == False):\n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == True):\n",
    "            percentage = 0; \n",
    "            nb_true = dietdf_categories[dietdf_categories[\"categories_tok\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, isRegime in enumerate(regimes):\n",
    "    print(\"\\n\"+isRegime+\" --> 3 True samples:\")\n",
    "    length_true = len(dietdf_categories[dietdf_categories[isRegime]==True][\"categories\"])\n",
    "    samples_true = dietdf_categories[dietdf_categories[isRegime]==True][\"categories\"].sample(n=min(3,length_true), random_state=1)\n",
    "    print(samples_true)\n",
    "    \n",
    "    print(\"\\n\"+isRegime+\" --> 3 False samples:\")\n",
    "    length_false = len(dietdf_categories[dietdf_categories[isRegime]==False][\"categories\"])\n",
    "    samples_false = dietdf_categories[dietdf_categories[isRegime]==False][\"categories\"].sample(n=min(3,length_false), random_state=1)\n",
    "    print(samples_false)\n",
    "    \n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Secondly we only use notRegimes_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"categories_tok\"] = categories_stem[\"categories\"]\n",
    "dietdf_categories_1 = isDiet(data, \"categories_tok\", regimes, notRegimes_lists=notRegimes_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n Using categories field notRegimes_lists, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    if len(dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()[1]/len(dietdf_categories_1)*100\n",
    "        nb_true = dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()[0]\n",
    "    elif len(dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == False): \n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == True): \n",
    "            percentage = 0; \n",
    "            nb_true = dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, isRegime in enumerate(regimes):\n",
    "    #if (len(dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()) == 2):\n",
    "    print(\"\\n\"+isRegime+\" --> 3 True samples:\")\n",
    "    length_true = len(dietdf_categories_1[dietdf_categories_1[isRegime]==True][\"categories\"])\n",
    "    samples_true = dietdf_categories_1[dietdf_categories_1[isRegime]==True][\"categories\"].sample(n=min(3,length_true), random_state=1)\n",
    "    print(samples_true)\n",
    "    \n",
    "    print(\"\\n\"+isRegime+\" --> 3 False samples:\")\n",
    "    length_false = len(dietdf_categories_1[dietdf_categories_1[isRegime]==False][\"categories\"])\n",
    "    samples_false = dietdf_categories_1[dietdf_categories_1[isRegime]==False][\"categories\"].sample(n=min(3,length_false), random_state=1)\n",
    "    print(samples_false)\n",
    "    \n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Regarding the results, we will only use notRegimes_categories for classification, more over, we might not classify organic diets, since it does not correspond to a specific type of diet but more to a label of quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isRegimes_labels_en = isRegimes_labels_en[:5]\n",
    "notRegimes_categories = notRegimes_categories[:5]\n",
    "notRegimes_ingredients_text = notRegimes_ingredients_text[:5]\n",
    "regimes = regimes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with _ingredients_text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"ingredients_text_tok\"] = ingredientstext_stem[\"ingredients_text\"]\n",
    "dietdf_ingredients_text = isDiet(data, \"ingredients_text_tok\", regimes, notRegimes_lists=notRegimes_ingredients_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n Using ingredients_text field notRegimes_lists, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    if len(dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts()[1]/len(dietdf_ingredients_text)*100\n",
    "        nb_true = dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts()[0]\n",
    "    elif len(dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts().index[0] == False): \n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts().index[0] == True): \n",
    "            percentage = 0; \n",
    "            nb_true = dietdf_ingredients_text[dietdf_ingredients_text[\"ingredients_text\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, isRegime in enumerate(regimes):\n",
    "    #if (len(dietdf_categories_1[dietdf_categories_1[\"categories_tok\"].notnull()][isRegime].value_counts()) == 2):\n",
    "    print(\"\\n\"+isRegime+\" --> 3 True samples:\")\n",
    "    length_true = len(dietdf_ingredients_text[dietdf_ingredients_text[isRegime]==True][\"ingredients_text\"])\n",
    "    samples_true = dietdf_ingredients_text[dietdf_ingredients_text[isRegime]==True][\"ingredients_text\"].sample(n=min(3,length_true), random_state=1)\n",
    "    print(samples_true)\n",
    "    \n",
    "    print(\"\\n\"+isRegime+\" --> 3 False samples:\")\n",
    "    length_false = len(dietdf_ingredients_text[dietdf_ingredients_text[isRegime]==False][\"ingredients_text\"])\n",
    "    samples_false = dietdf_ingredients_text[dietdf_ingredients_text[isRegime]==False][\"ingredients_text\"].sample(n=min(3,length_false), random_state=1)\n",
    "    print(samples_false)\n",
    "    \n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the results. \n",
    "\n",
    "Keeping in mind that _labels_en_ founding is prioritary (diet are explicit in that field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dietdf = data.copy()\n",
    "dietdf[\"labels_en_tok\"] = label_stem[\"labels_en\"]\n",
    "dietdf[\"categories_tok\"] = categories_stem[\"categories\"]\n",
    "dietdf = isDiet(dietdf, 'labels_en_tok', regimes, isRegimes_lists = isRegimes_labels_en)\n",
    "\n",
    "print(\"Using labels fields, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    \n",
    "    if len(dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[1]/len(dietdf)*100\n",
    "        nb_true = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[0]\n",
    "        \n",
    "    elif len(dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == False): \n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == True): \n",
    "            percentage = 0; \n",
    "            nb_true = dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "        \n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime))                                                \n",
    "\n",
    "\n",
    "\n",
    "dietdf = isDiet(dietdf, 'categories_tok', regimes, notRegimes_lists = notRegimes_categories)\n",
    "\n",
    "print(\"\\n Using labels_en + categories fields, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    \n",
    "    if len(dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[1]/len(dietdf)*100\n",
    "        nb_true = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[0]\n",
    "        \n",
    "    elif len(dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == False): \n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().index[0] == True): \n",
    "            percentage = 0; \n",
    "            nb_true = dietdf[dietdf[\"categories_tok\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "        \n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime))                                                \n",
    "\n",
    "    \n",
    "dietdf = isDiet(dietdf, 'ingredients_text_tok', regimes, notRegimes_lists=notRegimes_ingredients_text)\n",
    "\n",
    "print(\"\\n Using labels_en + categories + ingredients_text fields, we managed to classified : \\n\")\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    \n",
    "    if len(dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()) == 2:\n",
    "        percentage = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[1]/len(dietdf)*100\n",
    "        nb_true = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[1]\n",
    "        nb_false = dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()[0]\n",
    "        \n",
    "    elif len(dietdf[dietdf[isRegime].notnull()][isRegime].value_counts()) == 1:\n",
    "        if (dietdf[dietdf[\"ingredients_text_tok\"].notnull()][isRegime].value_counts().index[0] == False): \n",
    "            percentage = 0; \n",
    "            nb_true = 0; \n",
    "            nb_false = dietdf[dietdf[\"ingredients_text_tok\"].notnull()][isRegime].value_counts().loc[False]\n",
    "        elif (dietdf[dietdf[\"ingredients_text_tok\"].notnull()][isRegime].value_counts().index[0] == True): \n",
    "            percentage = 0; \n",
    "            nb_true = dietdf[dietdf[\"ingredients_text_tok\"].notnull()][isRegime].value_counts().loc[True]; \n",
    "            nb_false = 0\n",
    "        \n",
    "    print(\"%s : %d True and %d False --> %d %%  product of total data set %s\" % (isRegime, nb_true, nb_false, percentage, isRegime))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_nanlab = dietdf[dietdf[\"labels_en\"].isnull()].index\n",
    "index_notnanlab = dietdf[dietdf[\"labels_en\"].notnull()].index\n",
    "index_nancat = dietdf[dietdf[\"categories\"].isnull()].index\n",
    "index_notnancat = dietdf[dietdf[\"categories\"].notnull()].index\n",
    "index_naning = dietdf[dietdf[\"ingredients_text\"].isnull()].index\n",
    "index_notnaning = dietdf[dietdf[\"ingredients_text\"].notnull()].index\n",
    "\n",
    "for index, isRegime in enumerate(regimes):\n",
    "    index_true =  dietdf[dietdf[isRegime]==True].index\n",
    "    index_false =  dietdf[dietdf[isRegime]==False].index\n",
    "\n",
    "    \n",
    "    print(\"\\n\\n================\"+isRegime+\"================\")\n",
    "    print(\"\\n--> 3 True samples found by categories:\")\n",
    "    len_true_cat = len(dietdf.loc[index_true.intersection(index_nanlab).intersection(index_naning),[\"categories\"]])\n",
    "    samples_true_cat = dietdf.loc[index_true.intersection(index_nanlab).intersection(index_naning),[\"categories\"]].sample(n=min(3, len_true_cat), random_state=1)\n",
    "    print(samples_true_cat)\n",
    "    \n",
    "    print(\"\\n--> 3 True samples found by labels_en:\")\n",
    "    len_true_lab = len(dietdf.loc[index_true.intersection(index_nancat).intersection(index_naning),\"labels_en\"])\n",
    "    samples_true_lab = dietdf.loc[index_true.intersection(index_nancat).intersection(index_naning),\"labels_en\"].sample(n=min(3,len_true_lab), random_state=1)\n",
    "    print(samples_true_lab)\n",
    "    \n",
    "    print(\"\\n--> 3 True samples found by ingredients_text:\")\n",
    "    len_true_ing = len(dietdf.loc[index_true.intersection(index_nancat).intersection(index_nanlab),\"ingredients_text\"])\n",
    "    samples_true_ing = dietdf.loc[index_true.intersection(index_nancat).intersection(index_nanlab),\"ingredients_text\"].sample(n=min(3,len_true_ing), random_state=1)\n",
    "    print(samples_true_ing)\n",
    "    \n",
    "    print(\"\\n--> 3 False samples:\")\n",
    "    len_false = len(dietdf.loc[index_false.intersection(index_notnancat).intersection(index_notnanlab).intersection(index_notnaning),[\"categories\", 'labels_en', 'ingredients_text']])\n",
    "    samples_false = dietdf.loc[index_false.intersection(index_notnancat).intersection(index_notnanlab).intersection(index_notnaning),[\"categories\", 'labels_en', 'ingredients_text']].sample(n=min(3,len_false), random_state=1)\n",
    "    print(samples_false)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO.. Can't figure out for now why Viandes, Charcuteries, Saucissons, Saucisses s... is categorized as Gluten-Free since we have the stemmed token \"charcut\" in the notGlutenfree_categories list, the isDiet might not work exactly as expected.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Estimation\n",
    "\n",
    "Since we know the classification by _labels_en_ to be __True__, we want to analyse how well we classify using _categories_ by comparing the number of differents outputs in the common product classified by both fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dietdf_categories= isDiet(dietdf_categories, 'categories_tok', regimes, isRegimes_lists =isRegimes_categories, notRegimes_lists=notRegimes_categories)\n",
    "dietdf_labels_en = isDiet(dietdf_labels_en, 'labels_en_tok', regimes, isRegimes_lists = isRegimes_labels_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, isRegime in enumerate(regimes):\n",
    "    index_cat = dietdf_categories[dietdf_categories[isRegime].notnull()].index\n",
    "    index_lab = dietdf_labels_en[dietdf_labels_en[isRegime].notnull()].index\n",
    "    \n",
    "    index_inter = index_cat.intersection(index_lab)\n",
    "    \n",
    "    common_cat = dietdf_categories.loc[index_inter, isRegime].astype(int)\n",
    "    common_lab = dietdf_labels_en.loc[index_inter, isRegime].astype(int)\n",
    "    \n",
    "    commonPlus = common_lab + common_cat \n",
    "    commonMinus = common_lab - common_cat\n",
    "    out_serie =  commonPlus.value_counts(normalize = True).sort_index()\n",
    "    out_serie1 =  commonMinus.value_counts(normalize = True).sort_index()\n",
    "    #If we want to compare both series, when common_lab is True (i.e = 1), it is the correct classification, so if common_cat has False (i.e = 0) on that cell, then\n",
    "    #1 + 0 will return 1, \n",
    "    #0 + 0 will return 0, \n",
    "    #1 + 1 will return 2 \n",
    "    #So correctly classified correspond to 0 and 2, uncorreclty classified correspond to 1\n",
    "    print(\"\\n\\n\" + isRegime + \"\\n\")\n",
    "    for index, value in out_serie.items():\n",
    "        print(\"    Index : {}, Proportion : {}\".format(index, round(value,4)))\n",
    "        #0 : False correclty classified\n",
    "        #1 : Uncorrect classification\n",
    "        #2 : True correcly classified\n",
    "    print(\"\\n\")\n",
    "    for index, value in out_serie1.items():\n",
    "        print(\"    Index : {}, Proportion : {}\".format(index, round(value,4)))\n",
    "        #-1 : True uncorreclty classified in categories (is False in labels)\n",
    "        #0 : correct classification\n",
    "        #1 : False uncorrecly classified in categories (is True in labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Regarding the previous output, \n",
    "- Only a fews of isRegime are __False__ when they should be __True__ (-1) but a lots are __True__ when they should be __False__(are False using labels_en)(1).\n",
    "- Only a fews of isRegime are __True__ correctly classified"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
